{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "\n",
    "from lib import plotting\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-12 14:03:03,961] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84, 1] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholders for our input\n",
    "        # Our input are 4 RGB frames of shape 160, 160 each\n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three convolutional layers\n",
    "        conv1 = tf.contrib.layers.conv2d(\n",
    "            X, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "        conv2 = tf.contrib.layers.conv2d(\n",
    "            conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        conv3 = tf.contrib.layers.conv2d(\n",
    "            conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "\n",
    "        # Fully connected layers\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calcualte the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.train.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-d6e4720ffac5>:59: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-12 14:03:09,356] From <ipython-input-5-d6e4720ffac5>:59: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02933731  0.02895204  0.          0.        ]\n",
      " [ 0.02933731  0.02895204  0.          0.        ]]\n",
      "99.7109\n"
     ]
    }
   ],
   "source": [
    "# For Testing....\n",
    "\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "e = Estimator(scope=\"test\")\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    observation = env.reset()\n",
    "    \n",
    "    observation_p = sp.process(sess, observation)\n",
    "    observation = np.stack([observation_p] * 4, axis=2)\n",
    "    observations = np.array([observation] * 2)\n",
    "    \n",
    "    # Test Prediction\n",
    "    print(e.predict(sess, observations))\n",
    "\n",
    "    # Test training step\n",
    "    y = np.array([10.0, 10.0])\n",
    "    a = np.array([1, 3])\n",
    "    print(e.update(sess, observations, a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_model_parameters(sess, estimator1, estimator2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "\n",
    "    Args:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "    update_ops = []\n",
    "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "        op = e2_v.assign(e1_v)\n",
    "        update_ops.append(op)\n",
    "\n",
    "    sess.run(update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Gamma discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.train.get_global_step())\n",
    "\n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        print(\"\\r@ Step {}/{}\".format(\n",
    "                    i + 1, replay_memory_init_size), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        prob = policy(sess, np.stack([state] * 4, axis=2), epsilon_start)\n",
    "        action = np.random.choice(range(len(VALID_ACTIONS)), p=prob)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = state_processor.process(sess, next_state)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    # Record videos\n",
    "    env= Monitor(env,\n",
    "                 directory=monitor_path,\n",
    "                 resume=True,\n",
    "                 video_callable=lambda count: count % record_video_every == 0)\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "#         state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # Add epsilon to Tensorboard\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(simple_value=epsilon, tag=\"epsilon\")\n",
    "            q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "\n",
    "            # TODO: Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                copy_model_parameters(sess, q_estimator, target_estimator)\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step in the environment\n",
    "            prob = policy(sess, np.stack([state]*4, axis=2), epsilon)\n",
    "            action = np.random.choice(range(len(VALID_ACTIONS)), p=prob)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = state_processor.process(sess, next_state)\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # TODO: Sample a minibatch from the replay memory\n",
    "            # TODO: Calculate q values and targets\n",
    "            # TODO Perform gradient descent update\n",
    "\n",
    "            sample_transitions = random.sample(replay_memory, batch_size)\n",
    "\n",
    "            Q_ns = target_estimator.predict(sess,\n",
    "                                            np.array([np.stack([s.next_state]*4, axis=2)\n",
    "                                                      for s in sample_transitions]))\n",
    "            rewards = np.array([s.reward for s in sample_transitions])\n",
    "            dones = np.array([s.done for s in sample_transitions])\n",
    "            targets = rewards + discount_factor*np.max(Q_ns, axis=1)\n",
    "            targets[dones] = rewards[dones]\n",
    "            loss = q_estimator.update(sess,\n",
    "                               np.array([np.stack([s.state]*4, axis=2) for s in sample_transitions]),\n",
    "                               np.array([s.action for s in sample_transitions]),\n",
    "                               targets\n",
    "                              )\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name=\"episode_reward\", tag=\"episode_reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name=\"episode_length\", tag=\"episode_length\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "        q_estimator.summary_writer.flush()\n",
    "\n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    env.close()\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-12 14:03:12,781] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model checkpoint /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/checkpoints/model...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/checkpoints/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-12 14:03:13,355] Restoring parameters from /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/checkpoints/model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n",
      "@ Step 50000/50000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-12 14:12:29,139] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 482 (1411403) @ Episode 1/2500, loss: 0.0103228325024247175\n",
      "Episode Reward: 3.0\n",
      "Step 352 (1411755) @ Episode 2/2500, loss: 0.0083486642688512845\n",
      "Episode Reward: 3.0\n",
      "Step 429 (1412184) @ Episode 3/2500, loss: 0.0101576531305909162\n",
      "Episode Reward: 4.0\n",
      "Step 569 (1412753) @ Episode 4/2500, loss: 0.0190174505114555365\n",
      "Episode Reward: 4.0\n",
      "Step 495 (1413248) @ Episode 5/2500, loss: 0.0130521524697542197\n",
      "Episode Reward: 4.0\n",
      "Step 495 (1413743) @ Episode 6/2500, loss: 0.0175666287541389473\n",
      "Episode Reward: 3.0\n",
      "Step 426 (1414169) @ Episode 7/2500, loss: 0.0121017359197139745\n",
      "Episode Reward: 4.0\n",
      "Step 336 (1414505) @ Episode 8/2500, loss: 0.0151020158082246784\n",
      "Episode Reward: 2.0\n",
      "Step 227 (1414732) @ Episode 9/2500, loss: 0.0043418207205832005\n",
      "Episode Reward: 0.0\n",
      "Step 242 (1414974) @ Episode 10/2500, loss: 0.0042044222354888922\n",
      "Episode Reward: 1.0\n",
      "Step 471 (1415445) @ Episode 11/2500, loss: 0.0097195189446210865\n",
      "Episode Reward: 4.0\n",
      "Step 546 (1415991) @ Episode 12/2500, loss: 0.2744808495044708445\n",
      "Episode Reward: 2.0\n",
      "Step 323 (1416314) @ Episode 13/2500, loss: 0.0035581854172050953\n",
      "Episode Reward: 1.0\n",
      "Step 421 (1416735) @ Episode 14/2500, loss: 0.0033262437209486965\n",
      "Episode Reward: 2.0\n",
      "Step 379 (1417114) @ Episode 15/2500, loss: 0.0063016572967171675\n",
      "Episode Reward: 2.0\n",
      "Step 585 (1417699) @ Episode 16/2500, loss: 0.0268257409334182746\n",
      "Episode Reward: 5.0\n",
      "Step 372 (1418071) @ Episode 17/2500, loss: 0.0252379346638917925\n",
      "Episode Reward: 1.0\n",
      "Step 431 (1418502) @ Episode 18/2500, loss: 0.0112648736685514456\n",
      "Episode Reward: 3.0\n",
      "Step 448 (1418950) @ Episode 19/2500, loss: 0.0041849086992442614\n",
      "Episode Reward: 4.0\n",
      "Step 461 (1419411) @ Episode 20/2500, loss: 0.0022892290726304054\n",
      "Episode Reward: 3.0\n",
      "Step 219 (1419630) @ Episode 21/2500, loss: 0.0040869712829589844\n",
      "Episode Reward: 1.0\n",
      "Step 355 (1419985) @ Episode 22/2500, loss: 0.2734838128089905567\n",
      "Episode Reward: 2.0\n",
      "Step 373 (1420358) @ Episode 23/2500, loss: 0.0096579650416970255\n",
      "Episode Reward: 2.0\n",
      "Step 337 (1420695) @ Episode 24/2500, loss: 0.0073144156485795975\n",
      "Episode Reward: 2.0\n",
      "Step 603 (1421298) @ Episode 25/2500, loss: 0.0071208183653652675\n",
      "Episode Reward: 4.0\n",
      "Step 587 (1421885) @ Episode 26/2500, loss: 0.3776133656501776573\n",
      "Episode Reward: 6.0\n",
      "Step 511 (1422396) @ Episode 27/2500, loss: 0.0025165304541587835\n",
      "Episode Reward: 5.0\n",
      "Step 394 (1422790) @ Episode 28/2500, loss: 0.2418369799852371254\n",
      "Episode Reward: 3.0\n",
      "Step 367 (1423157) @ Episode 29/2500, loss: 0.0053910273127257825\n",
      "Episode Reward: 3.0\n",
      "Step 365 (1423522) @ Episode 30/2500, loss: 0.0049687484279274943\n",
      "Episode Reward: 3.0\n",
      "Step 334 (1423856) @ Episode 31/2500, loss: 0.0056628547608852396\n",
      "Episode Reward: 2.0\n",
      "Step 403 (1424259) @ Episode 32/2500, loss: 0.0310624837875366295\n",
      "Episode Reward: 3.0\n",
      "Step 341 (1424600) @ Episode 33/2500, loss: 0.0102405482903122914\n",
      "Episode Reward: 3.0\n",
      "Step 364 (1424964) @ Episode 34/2500, loss: 0.0830854922533035395\n",
      "Episode Reward: 2.0\n",
      "Step 422 (1425386) @ Episode 35/2500, loss: 0.0055689690634608275\n",
      "Episode Reward: 4.0\n",
      "Step 455 (1425841) @ Episode 36/2500, loss: 0.0038579138927161694\n",
      "Episode Reward: 5.0\n",
      "Step 450 (1426291) @ Episode 37/2500, loss: 0.0074486006051301965\n",
      "Episode Reward: 4.0\n",
      "Step 530 (1426821) @ Episode 38/2500, loss: 0.0054752603173255925\n",
      "Episode Reward: 7.0\n",
      "Step 310 (1427131) @ Episode 39/2500, loss: 0.0145735386759042746\n",
      "Episode Reward: 1.0\n",
      "Step 366 (1427497) @ Episode 40/2500, loss: 0.0051271845586597924\n",
      "Episode Reward: 2.0\n",
      "Step 428 (1427925) @ Episode 41/2500, loss: 0.0063769901171326644\n",
      "Episode Reward: 2.0\n",
      "Step 613 (1428538) @ Episode 42/2500, loss: 0.0106646344065666247\n",
      "Episode Reward: 7.0\n",
      "Step 572 (1429110) @ Episode 43/2500, loss: 0.0070731500163674355\n",
      "Episode Reward: 5.0\n",
      "Step 732 (1429842) @ Episode 44/2500, loss: 0.0118026221171021465\n",
      "Episode Reward: 6.0\n",
      "Step 334 (1430176) @ Episode 45/2500, loss: 0.0042534442618489265\n",
      "Episode Reward: 3.0\n",
      "Step 229 (1430405) @ Episode 46/2500, loss: 0.0076227132230997086\n",
      "Episode Reward: 1.0\n",
      "Step 359 (1430764) @ Episode 47/2500, loss: 0.0031684497371315956\n",
      "Episode Reward: 3.0\n",
      "Step 198 (1430962) @ Episode 48/2500, loss: 0.0069135418161749845\n",
      "Episode Reward: 0.0\n",
      "Step 770 (1431732) @ Episode 49/2500, loss: 0.2553221583366394045\n",
      "Episode Reward: 2.0\n",
      "Step 394 (1432126) @ Episode 50/2500, loss: 0.0067113433033227925\n",
      "Episode Reward: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-12 14:50:41,943] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video000050.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 375 (1432501) @ Episode 51/2500, loss: 0.0037570721469819546\n",
      "Episode Reward: 3.0\n",
      "Step 383 (1432884) @ Episode 52/2500, loss: 0.0075995679944753656\n",
      "Episode Reward: 3.0\n",
      "Step 452 (1433336) @ Episode 53/2500, loss: 0.0120565481483936315\n",
      "Episode Reward: 5.0\n",
      "Step 376 (1433712) @ Episode 54/2500, loss: 0.0065718628466129395\n",
      "Episode Reward: 3.0\n",
      "Step 342 (1434054) @ Episode 55/2500, loss: 0.0044920328073203565\n",
      "Episode Reward: 3.0\n",
      "Step 278 (1434332) @ Episode 56/2500, loss: 0.0054495329968631275\n",
      "Episode Reward: 2.0\n",
      "Step 488 (1434820) @ Episode 57/2500, loss: 0.0032777022570371628\n",
      "Episode Reward: 5.0\n",
      "Step 314 (1435134) @ Episode 58/2500, loss: 0.2480518072843551635\n",
      "Episode Reward: 3.0\n",
      "Step 221 (1435355) @ Episode 59/2500, loss: 0.0042437664233148123\n",
      "Episode Reward: 1.0\n",
      "Step 449 (1435804) @ Episode 60/2500, loss: 0.0037320833653211594\n",
      "Episode Reward: 5.0\n",
      "Step 539 (1436343) @ Episode 61/2500, loss: 0.1221672147512435958\n",
      "Episode Reward: 3.0\n",
      "Step 641 (1436984) @ Episode 62/2500, loss: 0.0061902301385998736\n",
      "Episode Reward: 6.0\n",
      "Step 304 (1437288) @ Episode 63/2500, loss: 0.0091482978314161357\n",
      "Episode Reward: 2.0\n",
      "Step 473 (1437761) @ Episode 64/2500, loss: 0.0052875922992825516\n",
      "Episode Reward: 5.0\n",
      "Step 385 (1438146) @ Episode 65/2500, loss: 0.0056247916072607047\n",
      "Episode Reward: 2.0\n",
      "Step 427 (1438573) @ Episode 66/2500, loss: 0.3320438265800476153\n",
      "Episode Reward: 3.0\n",
      "Step 336 (1438909) @ Episode 67/2500, loss: 0.0166559144854545645\n",
      "Episode Reward: 3.0\n",
      "Step 315 (1439224) @ Episode 68/2500, loss: 0.0039165699854493144\n",
      "Episode Reward: 2.0\n",
      "Step 390 (1439614) @ Episode 69/2500, loss: 0.0062360209412872795\n",
      "Episode Reward: 3.0\n",
      "Step 542 (1440156) @ Episode 70/2500, loss: 0.0162274446338415155\n",
      "Episode Reward: 4.0\n",
      "Step 553 (1440709) @ Episode 71/2500, loss: 0.0043750880286097535\n",
      "Episode Reward: 5.0\n",
      "Step 251 (1440960) @ Episode 72/2500, loss: 0.0050586853176355366\n",
      "Episode Reward: 1.0\n",
      "Step 488 (1441448) @ Episode 73/2500, loss: 0.0148521503433585175\n",
      "Episode Reward: 3.0\n",
      "Step 353 (1441801) @ Episode 74/2500, loss: 0.0050606811419129376\n",
      "Episode Reward: 4.0\n",
      "Step 409 (1442210) @ Episode 75/2500, loss: 0.0309116430580616547\n",
      "Episode Reward: 4.0\n",
      "Step 270 (1442480) @ Episode 76/2500, loss: 0.0226146969944238666\n",
      "Episode Reward: 2.0\n",
      "Step 443 (1442923) @ Episode 77/2500, loss: 0.0079247076064348224\n",
      "Episode Reward: 4.0\n",
      "Step 507 (1443430) @ Episode 78/2500, loss: 0.1391031742095947346\n",
      "Episode Reward: 2.0\n",
      "Step 352 (1443782) @ Episode 79/2500, loss: 0.0107902977615594864\n",
      "Episode Reward: 3.0\n",
      "Step 483 (1444265) @ Episode 80/2500, loss: 0.0134799368679523477\n",
      "Episode Reward: 5.0\n",
      "Step 513 (1444778) @ Episode 81/2500, loss: 0.0156720168888568885\n",
      "Episode Reward: 6.0\n",
      "Step 390 (1445168) @ Episode 82/2500, loss: 0.0134901609271764765\n",
      "Episode Reward: 3.0\n",
      "Step 396 (1445564) @ Episode 83/2500, loss: 0.2424944043159485425\n",
      "Episode Reward: 3.0\n",
      "Step 703 (1446267) @ Episode 84/2500, loss: 0.0113083943724632265\n",
      "Episode Reward: 4.0\n",
      "Step 389 (1446656) @ Episode 85/2500, loss: 0.0026191496290266514\n",
      "Episode Reward: 2.0\n",
      "Step 351 (1447007) @ Episode 86/2500, loss: 0.0145166292786598285\n",
      "Episode Reward: 2.0\n",
      "Step 269 (1447276) @ Episode 87/2500, loss: 0.0070540998131036762\n",
      "Episode Reward: 2.0\n",
      "Step 292 (1447568) @ Episode 88/2500, loss: 0.0076381666585803033\n",
      "Episode Reward: 2.0\n",
      "Step 437 (1448005) @ Episode 89/2500, loss: 0.0072968294844031335\n",
      "Episode Reward: 5.0\n",
      "Step 322 (1448327) @ Episode 90/2500, loss: 0.0122381728142499924\n",
      "Episode Reward: 2.0\n",
      "Step 456 (1448783) @ Episode 91/2500, loss: 0.0052976962178945545\n",
      "Episode Reward: 3.0\n",
      "Step 593 (1449376) @ Episode 92/2500, loss: 0.0042447452433407316\n",
      "Episode Reward: 5.0\n",
      "Step 365 (1449741) @ Episode 93/2500, loss: 0.0088867004960775386\n",
      "Episode Reward: 4.0\n",
      "Step 277 (1450018) @ Episode 94/2500, loss: 0.2017595022916793855\n",
      "Episode Reward: 2.0\n",
      "Step 472 (1450490) @ Episode 95/2500, loss: 0.0495490171015262673\n",
      "Episode Reward: 4.0\n",
      "Step 360 (1450850) @ Episode 96/2500, loss: 0.0738610252737999175\n",
      "Episode Reward: 2.0\n",
      "Step 352 (1451202) @ Episode 97/2500, loss: 0.0016224497230723515\n",
      "Episode Reward: 3.0\n",
      "Step 391 (1451593) @ Episode 98/2500, loss: 0.0202066805213689824\n",
      "Episode Reward: 4.0\n",
      "Step 591 (1452184) @ Episode 99/2500, loss: 0.0054039601236581875\n",
      "Episode Reward: 5.0\n",
      "Step 486 (1452670) @ Episode 100/2500, loss: 0.0169411022216081625\n",
      "Episode Reward: 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-12 15:27:52,310] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video000100.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 367 (1453037) @ Episode 101/2500, loss: 0.2611456215381622395\n",
      "Episode Reward: 3.0\n",
      "Step 352 (1453389) @ Episode 102/2500, loss: 0.0070146834477782254\n",
      "Episode Reward: 3.0\n",
      "Step 405 (1453794) @ Episode 103/2500, loss: 0.0072993570938706465\n",
      "Episode Reward: 4.0\n",
      "Step 323 (1454117) @ Episode 104/2500, loss: 0.0254225321114063267\n",
      "Episode Reward: 2.0\n",
      "Step 261 (1454378) @ Episode 105/2500, loss: 0.0050727976486086845\n",
      "Episode Reward: 1.0\n",
      "Step 376 (1454754) @ Episode 106/2500, loss: 0.0041808560490608215\n",
      "Episode Reward: 4.0\n",
      "Step 317 (1455071) @ Episode 107/2500, loss: 0.0102879013866186144\n",
      "Episode Reward: 2.0\n",
      "Step 355 (1455426) @ Episode 108/2500, loss: 0.0164159592241048875\n",
      "Episode Reward: 3.0\n",
      "Step 320 (1455746) @ Episode 109/2500, loss: 0.1023698523640632686\n",
      "Episode Reward: 3.0\n",
      "Step 442 (1456188) @ Episode 110/2500, loss: 0.0226458273828029636\n",
      "Episode Reward: 4.0\n",
      "Step 393 (1456581) @ Episode 111/2500, loss: 0.0175512246787548075\n",
      "Episode Reward: 3.0\n",
      "Step 341 (1456922) @ Episode 112/2500, loss: 0.0052835391834378244\n",
      "Episode Reward: 3.0\n",
      "Step 400 (1457322) @ Episode 113/2500, loss: 0.0041497824713587765\n",
      "Episode Reward: 3.0\n",
      "Step 340 (1457662) @ Episode 114/2500, loss: 0.0049765799194574365\n",
      "Episode Reward: 3.0\n",
      "Step 361 (1458023) @ Episode 115/2500, loss: 0.0090622585266828545\n",
      "Episode Reward: 3.0\n",
      "Step 391 (1458414) @ Episode 116/2500, loss: 0.0078511377796530726\n",
      "Episode Reward: 3.0\n",
      "Step 413 (1458827) @ Episode 117/2500, loss: 0.0440602898597717346\n",
      "Episode Reward: 3.0\n",
      "Step 350 (1459177) @ Episode 118/2500, loss: 0.0935767516493797366\n",
      "Episode Reward: 2.0\n",
      "Step 286 (1459463) @ Episode 119/2500, loss: 0.0035324909258633855\n",
      "Episode Reward: 1.0\n",
      "Step 248 (1459711) @ Episode 120/2500, loss: 0.0272886659950017935\n",
      "Episode Reward: 2.0\n",
      "Step 302 (1460013) @ Episode 121/2500, loss: 0.0067100049927830784\n",
      "Episode Reward: 2.0\n",
      "Step 386 (1460399) @ Episode 122/2500, loss: 0.0758976712822914185\n",
      "Episode Reward: 4.0\n",
      "Step 325 (1460724) @ Episode 123/2500, loss: 0.0084557561203837455\n",
      "Episode Reward: 3.0\n",
      "Step 260 (1460984) @ Episode 124/2500, loss: 0.0051754144951701165\n",
      "Episode Reward: 1.0\n",
      "Step 398 (1461382) @ Episode 125/2500, loss: 0.0130093796178698545\n",
      "Episode Reward: 4.0\n",
      "Step 318 (1461700) @ Episode 126/2500, loss: 0.0059561748057603846\n",
      "Episode Reward: 2.0\n",
      "Step 574 (1462274) @ Episode 127/2500, loss: 0.0133739467710256585\n",
      "Episode Reward: 6.0\n",
      "Step 568 (1462842) @ Episode 128/2500, loss: 0.1060924157500267245\n",
      "Episode Reward: 3.0\n",
      "Step 235 (1463077) @ Episode 129/2500, loss: 0.0304723661392927175\n",
      "Episode Reward: 1.0\n",
      "Step 345 (1463422) @ Episode 130/2500, loss: 0.0071288365870714195\n",
      "Episode Reward: 4.0\n",
      "Step 333 (1463755) @ Episode 131/2500, loss: 0.0092599699273705485\n",
      "Episode Reward: 3.0\n",
      "Step 395 (1464150) @ Episode 132/2500, loss: 0.0056609753519296656\n",
      "Episode Reward: 4.0\n",
      "Step 359 (1464509) @ Episode 133/2500, loss: 0.0127193722873926165\n",
      "Episode Reward: 3.0\n",
      "Step 354 (1464863) @ Episode 134/2500, loss: 0.0062785055488348016\n",
      "Episode Reward: 2.0\n",
      "Step 394 (1465257) @ Episode 135/2500, loss: 0.0047304504550993446\n",
      "Episode Reward: 3.0\n",
      "Step 409 (1465666) @ Episode 136/2500, loss: 0.0052876491099596025\n",
      "Episode Reward: 5.0\n",
      "Step 352 (1466018) @ Episode 137/2500, loss: 0.0074753854423761375\n",
      "Episode Reward: 2.0\n",
      "Step 350 (1466368) @ Episode 138/2500, loss: 0.0051707364618778237\n",
      "Episode Reward: 3.0\n",
      "Step 384 (1466752) @ Episode 139/2500, loss: 0.0062334584072232256\n",
      "Episode Reward: 1.0\n",
      "Step 318 (1467070) @ Episode 140/2500, loss: 0.0093115121126174934\n",
      "Episode Reward: 2.0\n",
      "Step 338 (1467408) @ Episode 141/2500, loss: 0.0121281957253813745\n",
      "Episode Reward: 2.0\n",
      "Step 446 (1467854) @ Episode 142/2500, loss: 0.0085083767771720895\n",
      "Episode Reward: 4.0\n",
      "Step 423 (1468277) @ Episode 143/2500, loss: 0.0054199639707803735\n",
      "Episode Reward: 4.0\n",
      "Step 427 (1468704) @ Episode 144/2500, loss: 0.0078469375148415579\n",
      "Episode Reward: 3.0\n",
      "Step 372 (1469076) @ Episode 145/2500, loss: 0.0065042544156312945\n",
      "Episode Reward: 2.0\n",
      "Step 281 (1469357) @ Episode 146/2500, loss: 0.0060505392029881486\n",
      "Episode Reward: 1.0\n",
      "Step 421 (1469778) @ Episode 147/2500, loss: 0.0114696267992258074\n",
      "Episode Reward: 3.0\n",
      "Step 299 (1470077) @ Episode 148/2500, loss: 0.0077589168213307866\n",
      "Episode Reward: 3.0\n",
      "Step 259 (1470336) @ Episode 149/2500, loss: 0.0066524059511721134\n",
      "Episode Reward: 2.0\n",
      "Step 404 (1470740) @ Episode 150/2500, loss: 0.0845979824662208667\n",
      "Episode Reward: 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-12 16:01:16,352] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video000150.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 349 (1471089) @ Episode 151/2500, loss: 0.0077546313405036935\n",
      "Episode Reward: 4.0\n",
      "Step 380 (1471469) @ Episode 152/2500, loss: 0.0064271991141140465\n",
      "Episode Reward: 4.0\n",
      "Step 291 (1471760) @ Episode 153/2500, loss: 0.0096812639385461876\n",
      "Episode Reward: 2.0\n",
      "Step 509 (1472269) @ Episode 154/2500, loss: 0.0028244014829397235\n",
      "Episode Reward: 4.0\n",
      "Step 294 (1472563) @ Episode 155/2500, loss: 0.0116036292165517815\n",
      "Episode Reward: 2.0\n",
      "Step 236 (1472799) @ Episode 156/2500, loss: 0.0040653413161635465\n",
      "Episode Reward: 1.0\n",
      "Step 316 (1473115) @ Episode 157/2500, loss: 0.0073161786422133455\n",
      "Episode Reward: 2.0\n",
      "Step 292 (1473407) @ Episode 158/2500, loss: 0.0059024705551564697\n",
      "Episode Reward: 3.0\n",
      "Step 479 (1473886) @ Episode 159/2500, loss: 0.0575007572770118725\n",
      "Episode Reward: 6.0\n",
      "Step 428 (1474314) @ Episode 160/2500, loss: 0.0106258448213338856\n",
      "Episode Reward: 4.0\n",
      "Step 316 (1474630) @ Episode 161/2500, loss: 0.0141163580119609834\n",
      "Episode Reward: 4.0\n",
      "Step 431 (1475061) @ Episode 162/2500, loss: 0.0178003348410129556\n",
      "Episode Reward: 4.0\n",
      "Step 288 (1475349) @ Episode 163/2500, loss: 0.0086624315008521085\n",
      "Episode Reward: 2.0\n",
      "Step 313 (1475662) @ Episode 164/2500, loss: 0.0092564029619097715\n",
      "Episode Reward: 1.0\n",
      "Step 309 (1475971) @ Episode 165/2500, loss: 0.0121876616030931475\n",
      "Episode Reward: 2.0\n",
      "Step 401 (1476372) @ Episode 166/2500, loss: 0.0065228426828980454\n",
      "Episode Reward: 4.0\n",
      "Step 239 (1476611) @ Episode 167/2500, loss: 0.0113664604723453525\n",
      "Episode Reward: 1.0\n",
      "Step 435 (1477046) @ Episode 168/2500, loss: 0.0127106448635458955\n",
      "Episode Reward: 5.0\n",
      "Step 324 (1477370) @ Episode 169/2500, loss: 0.0120826158672571185\n",
      "Episode Reward: 3.0\n",
      "Step 376 (1477746) @ Episode 170/2500, loss: 0.0064355460926890375\n",
      "Episode Reward: 4.0\n",
      "Step 247 (1477993) @ Episode 171/2500, loss: 0.0134001094847917565\n",
      "Episode Reward: 1.0\n",
      "Step 489 (1478482) @ Episode 172/2500, loss: 0.0074754301458597185\n",
      "Episode Reward: 5.0\n",
      "Step 386 (1478868) @ Episode 173/2500, loss: 0.0062382188625633726\n",
      "Episode Reward: 3.0\n",
      "Step 411 (1479279) @ Episode 174/2500, loss: 0.0041689006611704835\n",
      "Episode Reward: 5.0\n",
      "Step 331 (1479610) @ Episode 175/2500, loss: 0.0047382004559040075\n",
      "Episode Reward: 3.0\n",
      "Step 417 (1480027) @ Episode 176/2500, loss: 0.0068049849942326557\n",
      "Episode Reward: 4.0\n",
      "Step 659 (1480686) @ Episode 177/2500, loss: 0.0084395799785852438\n",
      "Episode Reward: 7.0\n",
      "Step 346 (1481032) @ Episode 178/2500, loss: 0.0169034283608198175\n",
      "Episode Reward: 2.0\n",
      "Step 444 (1481476) @ Episode 179/2500, loss: 0.0082830088213086135\n",
      "Episode Reward: 4.0\n",
      "Step 424 (1481900) @ Episode 180/2500, loss: 0.0077033112756907946\n",
      "Episode Reward: 3.0\n",
      "Step 517 (1482417) @ Episode 181/2500, loss: 0.0081194359809160235\n",
      "Episode Reward: 6.0\n",
      "Step 310 (1482727) @ Episode 182/2500, loss: 0.0068187103606760595\n",
      "Episode Reward: 2.0\n",
      "Step 479 (1483206) @ Episode 183/2500, loss: 0.0050475504249334335\n",
      "Episode Reward: 4.0\n",
      "Step 231 (1483437) @ Episode 184/2500, loss: 0.0864403471350669974\n",
      "Episode Reward: 1.0\n",
      "Step 317 (1483754) @ Episode 185/2500, loss: 0.0212325491011142735\n",
      "Episode Reward: 2.0\n",
      "Step 518 (1484272) @ Episode 186/2500, loss: 0.1340077221393585235\n",
      "Episode Reward: 5.0\n",
      "Step 359 (1484631) @ Episode 187/2500, loss: 0.0111688319593667985\n",
      "Episode Reward: 7.0\n",
      "Step 472 (1485103) @ Episode 188/2500, loss: 0.0102967591956257826\n",
      "Episode Reward: 5.0\n",
      "Step 331 (1485434) @ Episode 189/2500, loss: 0.0069764954969286925\n",
      "Episode Reward: 3.0\n",
      "Step 355 (1485789) @ Episode 190/2500, loss: 0.0092580039054155355\n",
      "Episode Reward: 2.0\n",
      "Step 340 (1486129) @ Episode 191/2500, loss: 0.0041416333988308915\n",
      "Episode Reward: 2.0\n",
      "Step 302 (1486431) @ Episode 192/2500, loss: 0.0102748936042189697\n",
      "Episode Reward: 2.0\n",
      "Step 247 (1486678) @ Episode 193/2500, loss: 0.0125183835625648555\n",
      "Episode Reward: 1.0\n",
      "Step 421 (1487099) @ Episode 194/2500, loss: 0.0081619359552860264\n",
      "Episode Reward: 3.0\n",
      "Step 411 (1487510) @ Episode 195/2500, loss: 0.0080497097223997125\n",
      "Episode Reward: 4.0\n",
      "Step 427 (1487937) @ Episode 196/2500, loss: 0.0050789779052138335\n",
      "Episode Reward: 2.0\n",
      "Step 299 (1488236) @ Episode 197/2500, loss: 0.0531344190239906325\n",
      "Episode Reward: 2.0\n",
      "Step 348 (1488584) @ Episode 198/2500, loss: 0.0099296635016798975\n",
      "Episode Reward: 2.0\n",
      "Step 311 (1488895) @ Episode 199/2500, loss: 0.0114466305822134022\n",
      "Episode Reward: 3.0\n",
      "Step 352 (1489247) @ Episode 200/2500, loss: 0.0803770869970321753\n",
      "Episode Reward: 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-12 16:36:36,089] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video000200.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 263 (1489510) @ Episode 201/2500, loss: 0.0147779723629355435\n",
      "Episode Reward: 1.0\n",
      "Step 400 (1489910) @ Episode 202/2500, loss: 0.0099155846983194355\n",
      "Episode Reward: 4.0\n",
      "Step 398 (1490308) @ Episode 203/2500, loss: 0.0132350008934736255\n",
      "Episode Reward: 4.0\n",
      "Step 420 (1490728) @ Episode 204/2500, loss: 0.0314476527273654946\n",
      "Episode Reward: 4.0\n",
      "Step 308 (1491036) @ Episode 205/2500, loss: 0.0216068513691425328\n",
      "Episode Reward: 3.0\n",
      "Step 279 (1491315) @ Episode 206/2500, loss: 0.0105506442487239846\n",
      "Episode Reward: 2.0\n",
      "Step 333 (1491648) @ Episode 207/2500, loss: 0.0080774445086717625\n",
      "Episode Reward: 3.0\n",
      "Step 223 (1491871) @ Episode 208/2500, loss: 0.0473830327391624455\n",
      "Episode Reward: 1.0\n",
      "Step 388 (1492259) @ Episode 209/2500, loss: 0.0102135110646486286\n",
      "Episode Reward: 4.0\n",
      "Step 311 (1492570) @ Episode 210/2500, loss: 0.0063206925988197335\n",
      "Episode Reward: 2.0\n",
      "Step 310 (1492880) @ Episode 211/2500, loss: 0.0057586329057812694\n",
      "Episode Reward: 2.0\n",
      "Step 272 (1493152) @ Episode 212/2500, loss: 0.0074729765765368946\n",
      "Episode Reward: 2.0\n",
      "Step 372 (1493524) @ Episode 213/2500, loss: 0.1267372071743011597\n",
      "Episode Reward: 4.0\n",
      "Step 383 (1493907) @ Episode 214/2500, loss: 0.0041940929368138313\n",
      "Episode Reward: 4.0\n",
      "Step 303 (1494210) @ Episode 215/2500, loss: 0.0066767581738531596\n",
      "Episode Reward: 1.0\n",
      "Step 284 (1494494) @ Episode 216/2500, loss: 0.0031529203988611745\n",
      "Episode Reward: 2.0\n",
      "Step 420 (1494914) @ Episode 217/2500, loss: 0.0603453181684017235\n",
      "Episode Reward: 5.0\n",
      "Step 321 (1495235) @ Episode 218/2500, loss: 0.0090078748762607575\n",
      "Episode Reward: 1.0\n",
      "Step 312 (1495547) @ Episode 219/2500, loss: 0.0085413083434104924\n",
      "Episode Reward: 3.0\n",
      "Step 326 (1495873) @ Episode 220/2500, loss: 0.0046679293736815457\n",
      "Episode Reward: 2.0\n",
      "Step 341 (1496214) @ Episode 221/2500, loss: 0.0081337168812751775\n",
      "Episode Reward: 1.0\n",
      "Step 328 (1496542) @ Episode 222/2500, loss: 0.0173221807926893233\n",
      "Episode Reward: 2.0\n",
      "Step 396 (1496938) @ Episode 223/2500, loss: 0.0064145675860345367\n",
      "Episode Reward: 3.0\n",
      "Step 413 (1497351) @ Episode 224/2500, loss: 0.0021556499414145947\n",
      "Episode Reward: 4.0\n",
      "Step 402 (1497753) @ Episode 225/2500, loss: 0.0088476669043302545\n",
      "Episode Reward: 4.0\n",
      "Step 429 (1498182) @ Episode 226/2500, loss: 0.0098568359389901165\n",
      "Episode Reward: 5.0\n",
      "Step 413 (1498595) @ Episode 227/2500, loss: 0.0058946087956428536\n",
      "Episode Reward: 5.0\n",
      "Step 278 (1498873) @ Episode 228/2500, loss: 0.0096063073724508296\n",
      "Episode Reward: 2.0\n",
      "Step 411 (1499284) @ Episode 229/2500, loss: 0.0074673150666058065\n",
      "Episode Reward: 4.0\n",
      "Step 309 (1499593) @ Episode 230/2500, loss: 0.0152161549776792537\n",
      "Episode Reward: 3.0\n",
      "Step 343 (1499936) @ Episode 231/2500, loss: 0.0042851911857724196\n",
      "Episode Reward: 3.0\n",
      "Step 283 (1500219) @ Episode 232/2500, loss: 0.0086761116981506356\n",
      "Episode Reward: 2.0\n",
      "Step 448 (1500667) @ Episode 233/2500, loss: 0.0069047482684254655\n",
      "Episode Reward: 4.0\n",
      "Step 310 (1500977) @ Episode 234/2500, loss: 0.0100829815492033965\n",
      "Episode Reward: 3.0\n",
      "Step 341 (1501318) @ Episode 235/2500, loss: 0.0132801141589879995\n",
      "Episode Reward: 3.0\n",
      "Step 444 (1501762) @ Episode 236/2500, loss: 0.0144908204674720763\n",
      "Episode Reward: 3.0\n",
      "Step 264 (1502026) @ Episode 237/2500, loss: 0.0060370746068656445\n",
      "Episode Reward: 1.0\n",
      "Step 465 (1502491) @ Episode 238/2500, loss: 0.0211516357958316855\n",
      "Episode Reward: 3.0\n",
      "Step 274 (1502765) @ Episode 239/2500, loss: 0.0059648910537362126\n",
      "Episode Reward: 2.0\n",
      "Step 235 (1503000) @ Episode 240/2500, loss: 0.0247952714562416085\n",
      "Episode Reward: 1.0\n",
      "Step 452 (1503452) @ Episode 241/2500, loss: 0.0174668803811073356\n",
      "Episode Reward: 4.0\n",
      "Step 453 (1503905) @ Episode 242/2500, loss: 0.0091804359108209615\n",
      "Episode Reward: 2.0\n",
      "Step 267 (1504172) @ Episode 243/2500, loss: 0.0067993253469467164\n",
      "Episode Reward: 2.0\n",
      "Step 375 (1504547) @ Episode 244/2500, loss: 0.0060306675732135776\n",
      "Episode Reward: 4.0\n",
      "Step 458 (1505005) @ Episode 245/2500, loss: 0.0176708921790123654\n",
      "Episode Reward: 4.0\n",
      "Step 365 (1505370) @ Episode 246/2500, loss: 0.0070756608620285995\n",
      "Episode Reward: 4.0\n",
      "Step 341 (1505711) @ Episode 247/2500, loss: 0.0055921431630849847\n",
      "Episode Reward: 3.0\n",
      "Step 366 (1506077) @ Episode 248/2500, loss: 0.0437892153859138525\n",
      "Episode Reward: 2.0\n",
      "Step 326 (1506403) @ Episode 249/2500, loss: 0.0089926421642303475\n",
      "Episode Reward: 3.0\n",
      "Step 300 (1506703) @ Episode 250/2500, loss: 0.0040241843089461335\n",
      "Episode Reward: 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-12 17:09:19,197] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video000250.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 315 (1507018) @ Episode 251/2500, loss: 0.0063853939063847065\n",
      "Episode Reward: 2.0\n",
      "Step 630 (1507648) @ Episode 252/2500, loss: 0.0172880087047815325\n",
      "Episode Reward: 8.0\n",
      "Step 486 (1508134) @ Episode 253/2500, loss: 0.0192913636565208445\n",
      "Episode Reward: 6.0\n",
      "Step 427 (1508561) @ Episode 254/2500, loss: 0.0114806154742836955\n",
      "Episode Reward: 7.0\n",
      "Step 373 (1508934) @ Episode 255/2500, loss: 0.2569680511951446574\n",
      "Episode Reward: 3.0\n",
      "Step 336 (1509270) @ Episode 256/2500, loss: 0.0365872867405414665\n",
      "Episode Reward: 3.0\n",
      "Step 358 (1509628) @ Episode 257/2500, loss: 0.0209040585905313595\n",
      "Episode Reward: 3.0\n",
      "Step 321 (1509949) @ Episode 258/2500, loss: 0.0091123767197132114\n",
      "Episode Reward: 3.0\n",
      "Step 290 (1510239) @ Episode 259/2500, loss: 0.0456504300236702354\n",
      "Episode Reward: 2.0\n",
      "Step 456 (1510695) @ Episode 260/2500, loss: 0.0082195512950420384\n",
      "Episode Reward: 3.0\n",
      "Step 299 (1510994) @ Episode 261/2500, loss: 0.0991817414760589604\n",
      "Episode Reward: 2.0\n",
      "Step 483 (1511477) @ Episode 262/2500, loss: 0.0048286207020282745\n",
      "Episode Reward: 3.0\n",
      "Step 496 (1511973) @ Episode 263/2500, loss: 0.0156526993960142145\n",
      "Episode Reward: 6.0\n",
      "Step 276 (1512249) @ Episode 264/2500, loss: 0.0044301180168986325\n",
      "Episode Reward: 2.0\n",
      "Step 404 (1512653) @ Episode 265/2500, loss: 0.0142601290717720995\n",
      "Episode Reward: 3.0\n",
      "Step 241 (1512894) @ Episode 266/2500, loss: 0.0241116806864738465\n",
      "Episode Reward: 1.0\n",
      "Step 519 (1513413) @ Episode 267/2500, loss: 0.0093714846298098566\n",
      "Episode Reward: 7.0\n",
      "Step 369 (1513782) @ Episode 268/2500, loss: 0.0094965398311614997\n",
      "Episode Reward: 3.0\n",
      "Step 413 (1514195) @ Episode 269/2500, loss: 0.0084028281271457675\n",
      "Episode Reward: 2.0\n",
      "Step 361 (1514556) @ Episode 270/2500, loss: 0.0096089895814657216\n",
      "Episode Reward: 3.0\n",
      "Step 211 (1514767) @ Episode 271/2500, loss: 0.0067504690960049635\n",
      "Episode Reward: 1.0\n",
      "Step 475 (1515242) @ Episode 272/2500, loss: 0.0105278547853231435\n",
      "Episode Reward: 5.0\n",
      "Step 424 (1515666) @ Episode 273/2500, loss: 0.0106765590608119965\n",
      "Episode Reward: 5.0\n",
      "Step 276 (1515942) @ Episode 274/2500, loss: 0.0215199589729309084\n",
      "Episode Reward: 2.0\n",
      "Step 339 (1516281) @ Episode 275/2500, loss: 0.0540721490979194646\n",
      "Episode Reward: 2.0\n",
      "Step 304 (1516585) @ Episode 276/2500, loss: 0.0094509255141019826\n",
      "Episode Reward: 2.0\n",
      "Step 312 (1516897) @ Episode 277/2500, loss: 0.0056201564148068435\n",
      "Episode Reward: 3.0\n",
      "Step 294 (1517191) @ Episode 278/2500, loss: 0.0067656706087291245\n",
      "Episode Reward: 1.0\n",
      "Step 427 (1517618) @ Episode 279/2500, loss: 0.0092941448092460637\n",
      "Episode Reward: 5.0\n",
      "Step 430 (1518048) @ Episode 280/2500, loss: 0.0050492715090513235\n",
      "Episode Reward: 4.0\n",
      "Step 348 (1518396) @ Episode 281/2500, loss: 0.0063456972129642965\n",
      "Episode Reward: 4.0\n",
      "Step 220 (1518616) @ Episode 282/2500, loss: 0.0517588891088962558\n",
      "Episode Reward: 1.0\n",
      "Step 284 (1518900) @ Episode 283/2500, loss: 0.0063601569272577765\n",
      "Episode Reward: 2.0\n",
      "Step 302 (1519202) @ Episode 284/2500, loss: 0.0058628148399293424\n",
      "Episode Reward: 2.0\n",
      "Step 433 (1519635) @ Episode 285/2500, loss: 0.0068502905778586864\n",
      "Episode Reward: 6.0\n",
      "Step 278 (1519913) @ Episode 286/2500, loss: 0.0054291225969791413\n",
      "Episode Reward: 2.0\n",
      "Step 285 (1520198) @ Episode 287/2500, loss: 0.0872564762830734347\n",
      "Episode Reward: 3.0\n",
      "Step 316 (1520514) @ Episode 288/2500, loss: 0.0133572109043598185\n",
      "Episode Reward: 2.0\n",
      "Step 238 (1520752) @ Episode 289/2500, loss: 0.0062783341854810715\n",
      "Episode Reward: 1.0\n",
      "Step 406 (1521158) @ Episode 290/2500, loss: 0.0059656221419572836\n",
      "Episode Reward: 3.0\n",
      "Step 414 (1521572) @ Episode 291/2500, loss: 0.0038965116254985332\n",
      "Episode Reward: 3.0\n",
      "Step 299 (1521871) @ Episode 292/2500, loss: 0.0096353124827146535\n",
      "Episode Reward: 2.0\n",
      "Step 224 (1522095) @ Episode 293/2500, loss: 0.0136835929006338124\n",
      "Episode Reward: 1.0\n",
      "Step 328 (1522423) @ Episode 294/2500, loss: 0.1213927716016769495\n",
      "Episode Reward: 3.0\n",
      "Step 221 (1522644) @ Episode 295/2500, loss: 0.0269714631140232155\n",
      "Episode Reward: 1.0\n",
      "Step 439 (1523083) @ Episode 296/2500, loss: 0.0137466061860322955\n",
      "Episode Reward: 4.0\n",
      "Step 344 (1523427) @ Episode 297/2500, loss: 0.0048491493798792365\n",
      "Episode Reward: 3.0\n",
      "Step 309 (1523736) @ Episode 298/2500, loss: 0.0073129157535731795\n",
      "Episode Reward: 3.0\n",
      "Step 560 (1524296) @ Episode 299/2500, loss: 0.0122194085270166436\n",
      "Episode Reward: 5.0\n",
      "Step 350 (1524646) @ Episode 300/2500, loss: 0.0165653526782989526\n",
      "Episode Reward: 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-12 17:42:26,655] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video000300.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 202 (1524848) @ Episode 301/2500, loss: 0.0120208328589797025\n",
      "Episode Reward: 1.0\n",
      "Step 266 (1525114) @ Episode 302/2500, loss: 0.0179473366588354155\n",
      "Episode Reward: 1.0\n",
      "Step 277 (1525391) @ Episode 303/2500, loss: 0.0096497172489762338\n",
      "Episode Reward: 1.0\n",
      "Step 435 (1525826) @ Episode 304/2500, loss: 0.0285458136349916465\n",
      "Episode Reward: 4.0\n",
      "Step 218 (1526044) @ Episode 305/2500, loss: 0.0126711549237370495\n",
      "Episode Reward: 1.0\n",
      "Step 389 (1526433) @ Episode 306/2500, loss: 0.0057538449764251715\n",
      "Episode Reward: 4.0\n",
      "Step 328 (1526761) @ Episode 307/2500, loss: 0.0130428243428468746\n",
      "Episode Reward: 3.0\n",
      "Step 348 (1527109) @ Episode 308/2500, loss: 0.0086313709616661076\n",
      "Episode Reward: 3.0\n",
      "Step 305 (1527414) @ Episode 309/2500, loss: 0.0041708424687385566\n",
      "Episode Reward: 3.0\n",
      "Step 373 (1527787) @ Episode 310/2500, loss: 0.0367941185832023645\n",
      "Episode Reward: 3.0\n",
      "Step 250 (1528037) @ Episode 311/2500, loss: 0.0325993001461029053\n",
      "Episode Reward: 1.0\n",
      "Step 466 (1528503) @ Episode 312/2500, loss: 0.0060707926750183105\n",
      "Episode Reward: 3.0\n",
      "Step 306 (1528809) @ Episode 313/2500, loss: 0.0054031983017921455\n",
      "Episode Reward: 2.0\n",
      "Step 267 (1529076) @ Episode 314/2500, loss: 0.0078927446156740196\n",
      "Episode Reward: 2.0\n",
      "Step 350 (1529426) @ Episode 315/2500, loss: 0.0073530455119907866\n",
      "Episode Reward: 3.0\n",
      "Step 380 (1529806) @ Episode 316/2500, loss: 0.0144646931439638145\n",
      "Episode Reward: 3.0\n",
      "Step 412 (1530218) @ Episode 317/2500, loss: 0.0077639180235564715\n",
      "Episode Reward: 3.0\n",
      "Step 426 (1530644) @ Episode 318/2500, loss: 0.0078673195093870165\n",
      "Episode Reward: 5.0\n",
      "Step 362 (1531006) @ Episode 319/2500, loss: 0.0139332115650177355\n",
      "Episode Reward: 3.0\n",
      "Step 521 (1531527) @ Episode 320/2500, loss: 0.0661671608686447165\n",
      "Episode Reward: 6.0\n",
      "Step 317 (1531844) @ Episode 321/2500, loss: 0.0168326087296009065\n",
      "Episode Reward: 2.0\n",
      "Step 252 (1532096) @ Episode 322/2500, loss: 0.0186854023486375855\n",
      "Episode Reward: 1.0\n",
      "Step 555 (1532651) @ Episode 323/2500, loss: 0.0108032329007983225\n",
      "Episode Reward: 6.0\n",
      "Step 451 (1533102) @ Episode 324/2500, loss: 0.0109080420807003978\n",
      "Episode Reward: 5.0\n",
      "Step 386 (1533488) @ Episode 325/2500, loss: 0.0129892798140645035\n",
      "Episode Reward: 3.0\n",
      "Step 459 (1533947) @ Episode 326/2500, loss: 0.0116499317809939385\n",
      "Episode Reward: 6.0\n",
      "Step 346 (1534293) @ Episode 327/2500, loss: 0.0272642187774181375\n",
      "Episode Reward: 3.0\n",
      "Step 209 (1534502) @ Episode 328/2500, loss: 0.0077817626297473915\n",
      "Episode Reward: 1.0\n",
      "Step 338 (1534840) @ Episode 329/2500, loss: 0.2444205135107040415\n",
      "Episode Reward: 3.0\n",
      "Step 291 (1535131) @ Episode 330/2500, loss: 0.0096173519268631947\n",
      "Episode Reward: 2.0\n",
      "Step 386 (1535517) @ Episode 331/2500, loss: 0.2152890861034393365\n",
      "Episode Reward: 3.0\n",
      "Step 405 (1535922) @ Episode 332/2500, loss: 0.0062779448926448826\n",
      "Episode Reward: 4.0\n",
      "Step 379 (1536301) @ Episode 333/2500, loss: 0.0128222433850169186\n",
      "Episode Reward: 4.0\n",
      "Step 186 (1536487) @ Episode 334/2500, loss: 0.0102875400334596636\n",
      "Episode Reward: 0.0\n",
      "Step 550 (1537037) @ Episode 335/2500, loss: 0.0118709057569503785\n",
      "Episode Reward: 8.0\n",
      "Step 442 (1537479) @ Episode 336/2500, loss: 0.0108684189617633825\n",
      "Episode Reward: 5.0\n",
      "Step 431 (1537910) @ Episode 337/2500, loss: 0.0087102688848972325\n",
      "Episode Reward: 3.0\n",
      "Step 297 (1538207) @ Episode 338/2500, loss: 0.0951057076454162645\n",
      "Episode Reward: 3.0\n",
      "Step 388 (1538595) @ Episode 339/2500, loss: 0.0028173443861305714\n",
      "Episode Reward: 3.0\n",
      "Step 378 (1538973) @ Episode 340/2500, loss: 0.0100343944504857065\n",
      "Episode Reward: 3.0\n",
      "Step 370 (1539343) @ Episode 341/2500, loss: 0.0078464886173605926\n",
      "Episode Reward: 3.0\n",
      "Step 451 (1539794) @ Episode 342/2500, loss: 0.0057633481919765476\n",
      "Episode Reward: 3.0\n",
      "Step 274 (1540068) @ Episode 343/2500, loss: 0.0112245157361030585\n",
      "Episode Reward: 2.0\n",
      "Step 353 (1540421) @ Episode 344/2500, loss: 0.0150909274816513065\n",
      "Episode Reward: 4.0\n",
      "Step 263 (1540684) @ Episode 345/2500, loss: 0.0130074303597211844\n",
      "Episode Reward: 2.0\n",
      "Step 448 (1541132) @ Episode 346/2500, loss: 0.0075713824480772024\n",
      "Episode Reward: 5.0\n",
      "Step 477 (1541609) @ Episode 347/2500, loss: 0.0085067767649889383\n",
      "Episode Reward: 4.0\n",
      "Step 303 (1541912) @ Episode 348/2500, loss: 0.0170236639678478246\n",
      "Episode Reward: 3.0\n",
      "Step 252 (1542164) @ Episode 349/2500, loss: 0.0043346504680812367\n",
      "Episode Reward: 2.0\n",
      "Step 332 (1542496) @ Episode 350/2500, loss: 0.0052162054926157946\n",
      "Episode Reward: 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-12 18:14:22,632] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video000350.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 560 (1543056) @ Episode 351/2500, loss: 0.0712928995490074287\n",
      "Episode Reward: 2.0\n",
      "Step 267 (1543323) @ Episode 352/2500, loss: 0.0413957759737968444\n",
      "Episode Reward: 2.0\n",
      "Step 352 (1543675) @ Episode 353/2500, loss: 0.0099451690912246784\n",
      "Episode Reward: 4.0\n",
      "Step 336 (1544011) @ Episode 354/2500, loss: 0.0069322525523602966\n",
      "Episode Reward: 3.0\n",
      "Step 221 (1544232) @ Episode 355/2500, loss: 0.0187873542308807376\n",
      "Episode Reward: 1.0\n",
      "Step 385 (1544617) @ Episode 356/2500, loss: 0.0077747898176312454\n",
      "Episode Reward: 3.0\n",
      "Step 432 (1545049) @ Episode 357/2500, loss: 0.0084628053009510045\n",
      "Episode Reward: 5.0\n",
      "Step 440 (1545489) @ Episode 358/2500, loss: 0.0048957392573356635\n",
      "Episode Reward: 3.0\n",
      "Step 379 (1545868) @ Episode 359/2500, loss: 0.0112022999674081855\n",
      "Episode Reward: 4.0\n",
      "Step 451 (1546319) @ Episode 360/2500, loss: 0.1056980118155479414\n",
      "Episode Reward: 4.0\n",
      "Step 269 (1546588) @ Episode 361/2500, loss: 0.0098931975662708286\n",
      "Episode Reward: 2.0\n",
      "Step 363 (1546951) @ Episode 362/2500, loss: 0.0094261076301336296\n",
      "Episode Reward: 3.0\n",
      "Step 330 (1547281) @ Episode 363/2500, loss: 0.0099324481561779985\n",
      "Episode Reward: 3.0\n",
      "Step 391 (1547672) @ Episode 364/2500, loss: 0.0128435529768466954\n",
      "Episode Reward: 3.0\n",
      "Step 428 (1548100) @ Episode 365/2500, loss: 0.0091363787651062014\n",
      "Episode Reward: 2.0\n",
      "Step 331 (1548431) @ Episode 366/2500, loss: 0.0171247832477092745\n",
      "Episode Reward: 3.0\n",
      "Step 347 (1548778) @ Episode 367/2500, loss: 0.0059169419109821325\n",
      "Episode Reward: 3.0\n",
      "Step 535 (1549313) @ Episode 368/2500, loss: 0.0142169240862131125\n",
      "Episode Reward: 7.0\n",
      "Step 628 (1549941) @ Episode 369/2500, loss: 0.0066455327905714514\n",
      "Episode Reward: 7.0\n",
      "Step 475 (1550416) @ Episode 370/2500, loss: 0.0115518532693386083\n",
      "Episode Reward: 4.0\n",
      "Step 292 (1550708) @ Episode 371/2500, loss: 0.0132146216928958915\n",
      "Episode Reward: 2.0\n",
      "Step 371 (1551079) @ Episode 372/2500, loss: 0.0107877738773822786\n",
      "Episode Reward: 4.0\n",
      "Step 248 (1551327) @ Episode 373/2500, loss: 0.0089760757982730876\n",
      "Episode Reward: 1.0\n",
      "Step 270 (1551597) @ Episode 374/2500, loss: 0.0205856189131736765\n",
      "Episode Reward: 2.0\n",
      "Step 346 (1551943) @ Episode 375/2500, loss: 0.0060755414888262757\n",
      "Episode Reward: 3.0\n",
      "Step 381 (1552324) @ Episode 376/2500, loss: 0.0500049442052841245\n",
      "Episode Reward: 3.0\n",
      "Step 271 (1552595) @ Episode 377/2500, loss: 0.0119932945817708974\n",
      "Episode Reward: 2.0\n",
      "Step 391 (1552986) @ Episode 378/2500, loss: 0.0082967048510909086\n",
      "Episode Reward: 3.0\n",
      "Step 387 (1553373) @ Episode 379/2500, loss: 0.0273941159248352055\n",
      "Episode Reward: 5.0\n",
      "Step 360 (1553733) @ Episode 380/2500, loss: 0.0217629857361316685\n",
      "Episode Reward: 3.0\n",
      "Step 249 (1553982) @ Episode 381/2500, loss: 0.0146782025694847154\n",
      "Episode Reward: 1.0\n",
      "Step 406 (1554388) @ Episode 382/2500, loss: 0.0331096313893795695\n",
      "Episode Reward: 4.0\n",
      "Step 275 (1554663) @ Episode 383/2500, loss: 0.0067341807298362255\n",
      "Episode Reward: 1.0\n",
      "Step 433 (1555096) @ Episode 384/2500, loss: 0.0208659358322620455\n",
      "Episode Reward: 4.0\n",
      "Step 298 (1555394) @ Episode 385/2500, loss: 0.0200547017157077865\n",
      "Episode Reward: 2.0\n",
      "Step 578 (1555972) @ Episode 386/2500, loss: 0.0497738644480705265\n",
      "Episode Reward: 7.0\n",
      "Step 298 (1556270) @ Episode 387/2500, loss: 0.0225069783627986935\n",
      "Episode Reward: 1.0\n",
      "Step 223 (1556493) @ Episode 388/2500, loss: 0.0073671475984156135\n",
      "Episode Reward: 1.0\n",
      "Step 497 (1556990) @ Episode 389/2500, loss: 0.0085115395486354835\n",
      "Episode Reward: 6.0\n",
      "Step 245 (1557235) @ Episode 390/2500, loss: 0.0075144739821553235\n",
      "Episode Reward: 1.0\n",
      "Step 276 (1557511) @ Episode 391/2500, loss: 0.0091051813215017325\n",
      "Episode Reward: 1.0\n",
      "Step 381 (1557892) @ Episode 392/2500, loss: 0.0092425234615802766\n",
      "Episode Reward: 3.0\n",
      "Step 293 (1558185) @ Episode 393/2500, loss: 0.0051989094354212285\n",
      "Episode Reward: 2.0\n",
      "Step 424 (1558609) @ Episode 394/2500, loss: 0.0122044272720813758\n",
      "Episode Reward: 3.0\n",
      "Step 574 (1559183) @ Episode 395/2500, loss: 0.0091610169038176546\n",
      "Episode Reward: 3.0\n",
      "Step 206 (1559389) @ Episode 396/2500, loss: 0.0159315392374992373\n",
      "Episode Reward: 1.0\n",
      "Step 587 (1559976) @ Episode 397/2500, loss: 0.0157529022544622426\n",
      "Episode Reward: 6.0\n",
      "Step 333 (1560309) @ Episode 398/2500, loss: 0.0156340990215539934\n",
      "Episode Reward: 3.0\n",
      "Step 213 (1560522) @ Episode 399/2500, loss: 0.0216874536126852046\n",
      "Episode Reward: 1.0\n",
      "Step 301 (1560823) @ Episode 400/2500, loss: 0.0051960819400846965\n",
      "Episode Reward: 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-12 18:47:33,055] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video000400.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 743 (1561566) @ Episode 401/2500, loss: 0.0021206224337220195\n",
      "Episode Reward: 8.0\n",
      "Step 331 (1561897) @ Episode 402/2500, loss: 0.0143137313425540925\n",
      "Episode Reward: 2.0\n",
      "Step 288 (1562185) @ Episode 403/2500, loss: 0.0223927199840545655\n",
      "Episode Reward: 2.0\n",
      "Step 307 (1562492) @ Episode 404/2500, loss: 0.0036357925273478035\n",
      "Episode Reward: 2.0\n",
      "Step 349 (1562841) @ Episode 405/2500, loss: 0.0835951417684555514\n",
      "Episode Reward: 1.0\n",
      "Step 277 (1563118) @ Episode 406/2500, loss: 0.0361878983676433565\n",
      "Episode Reward: 2.0\n",
      "Step 385 (1563503) @ Episode 407/2500, loss: 0.0063402922824025155\n",
      "Episode Reward: 3.0\n",
      "Step 237 (1563740) @ Episode 408/2500, loss: 0.0102023798972368245\n",
      "Episode Reward: 1.0\n",
      "Step 342 (1564082) @ Episode 409/2500, loss: 0.0230842493474483516\n",
      "Episode Reward: 3.0\n",
      "Step 416 (1564498) @ Episode 410/2500, loss: 0.1070183664560318765\n",
      "Episode Reward: 4.0\n",
      "Step 232 (1564730) @ Episode 411/2500, loss: 0.0052954088896512985\n",
      "Episode Reward: 1.0\n",
      "Step 396 (1565126) @ Episode 412/2500, loss: 0.0216535739600658494\n",
      "Episode Reward: 3.0\n",
      "Step 335 (1565461) @ Episode 413/2500, loss: 0.0074832048267126085\n",
      "Episode Reward: 3.0\n",
      "Step 542 (1566003) @ Episode 414/2500, loss: 0.0063173701055347926\n",
      "Episode Reward: 4.0\n",
      "Step 480 (1566483) @ Episode 415/2500, loss: 0.0037704650312662125\n",
      "Episode Reward: 6.0\n",
      "Step 359 (1566842) @ Episode 416/2500, loss: 0.0642806068062782345\n",
      "Episode Reward: 3.0\n",
      "Step 307 (1567149) @ Episode 417/2500, loss: 0.0202577430754899985\n",
      "Episode Reward: 2.0\n",
      "Step 347 (1567496) @ Episode 418/2500, loss: 0.0165982376784086235\n",
      "Episode Reward: 3.0\n",
      "Step 294 (1567790) @ Episode 419/2500, loss: 0.0066295135766267784\n",
      "Episode Reward: 2.0\n",
      "Step 345 (1568135) @ Episode 420/2500, loss: 0.0140286795794963844\n",
      "Episode Reward: 3.0\n",
      "Step 363 (1568498) @ Episode 421/2500, loss: 0.0113201793283224115\n",
      "Episode Reward: 3.0\n",
      "Step 388 (1568886) @ Episode 422/2500, loss: 0.0347784943878650675\n",
      "Episode Reward: 3.0\n",
      "Step 413 (1569299) @ Episode 423/2500, loss: 0.0280517507344484334\n",
      "Episode Reward: 4.0\n",
      "Step 300 (1569599) @ Episode 424/2500, loss: 0.0149292889982461936\n",
      "Episode Reward: 2.0\n",
      "Step 612 (1570211) @ Episode 425/2500, loss: 0.0084507092833518984\n",
      "Episode Reward: 7.0\n",
      "Step 237 (1570448) @ Episode 426/2500, loss: 0.0193736106157302866\n",
      "Episode Reward: 1.0\n",
      "Step 216 (1570664) @ Episode 427/2500, loss: 0.0689090192317962665\n",
      "Episode Reward: 1.0\n",
      "Step 201 (1570865) @ Episode 428/2500, loss: 0.0135594336315989535\n",
      "Episode Reward: 1.0\n",
      "Step 334 (1571199) @ Episode 429/2500, loss: 0.0082749845460057265\n",
      "Episode Reward: 3.0\n",
      "Step 393 (1571592) @ Episode 430/2500, loss: 0.0109978206455707555\n",
      "Episode Reward: 4.0\n",
      "Step 302 (1571894) @ Episode 431/2500, loss: 0.0104924105107784275\n",
      "Episode Reward: 2.0\n",
      "Step 410 (1572304) @ Episode 432/2500, loss: 0.0128925479948520665\n",
      "Episode Reward: 3.0\n",
      "Step 289 (1572593) @ Episode 433/2500, loss: 0.0048708887770771983\n",
      "Episode Reward: 2.0\n",
      "Step 276 (1572869) @ Episode 434/2500, loss: 0.0079198144376277924\n",
      "Episode Reward: 2.0\n",
      "Step 610 (1573479) @ Episode 435/2500, loss: 0.0080963084474205976\n",
      "Episode Reward: 6.0\n",
      "Step 433 (1573912) @ Episode 436/2500, loss: 0.1388579756021499626\n",
      "Episode Reward: 5.0\n",
      "Step 300 (1574212) @ Episode 437/2500, loss: 0.0447241961956024245\n",
      "Episode Reward: 2.0\n",
      "Step 314 (1574526) @ Episode 438/2500, loss: 0.0418908745050430355\n",
      "Episode Reward: 3.0\n",
      "Step 475 (1575001) @ Episode 439/2500, loss: 0.0087103378027677545\n",
      "Episode Reward: 8.0\n",
      "Step 444 (1575445) @ Episode 440/2500, loss: 0.0119721945375204095\n",
      "Episode Reward: 5.0\n",
      "Step 358 (1575803) @ Episode 441/2500, loss: 0.0176549404859542855\n",
      "Episode Reward: 3.0\n",
      "Step 471 (1576274) @ Episode 442/2500, loss: 0.0188743602484464655\n",
      "Episode Reward: 2.0\n",
      "Step 438 (1576712) @ Episode 443/2500, loss: 0.0083884354680776604\n",
      "Episode Reward: 5.0\n",
      "Step 416 (1577128) @ Episode 444/2500, loss: 0.0104062445461750034\n",
      "Episode Reward: 3.0\n",
      "Step 339 (1577467) @ Episode 445/2500, loss: 0.0147625040262937552\n",
      "Episode Reward: 4.0\n",
      "Step 397 (1577864) @ Episode 446/2500, loss: 0.0213845167309045883\n",
      "Episode Reward: 4.0\n",
      "Step 415 (1578279) @ Episode 447/2500, loss: 0.0078563820570707324\n",
      "Episode Reward: 4.0\n",
      "Step 463 (1578742) @ Episode 448/2500, loss: 0.0055010626092553146\n",
      "Episode Reward: 4.0\n",
      "Step 707 (1579449) @ Episode 449/2500, loss: 0.0048620090819895275\n",
      "Episode Reward: 7.0\n",
      "Step 386 (1579835) @ Episode 450/2500, loss: 0.0259054526686668466\n",
      "Episode Reward: 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-12 19:22:40,709] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video000450.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 310 (1580145) @ Episode 451/2500, loss: 0.0087531963363289835\n",
      "Episode Reward: 3.0\n",
      "Step 336 (1580481) @ Episode 452/2500, loss: 0.0176361761987209324\n",
      "Episode Reward: 3.0\n",
      "Step 328 (1580809) @ Episode 453/2500, loss: 0.0173858702182769785\n",
      "Episode Reward: 2.0\n",
      "Step 312 (1581121) @ Episode 454/2500, loss: 0.0072699645534157754\n",
      "Episode Reward: 2.0\n",
      "Step 502 (1581623) @ Episode 455/2500, loss: 0.0078440466895699545\n",
      "Episode Reward: 6.0\n",
      "Step 278 (1581901) @ Episode 456/2500, loss: 0.0094484947621822365\n",
      "Episode Reward: 2.0\n",
      "Step 361 (1582262) @ Episode 457/2500, loss: 0.0151684926822781565\n",
      "Episode Reward: 4.0\n",
      "Step 354 (1582616) @ Episode 458/2500, loss: 0.0096692591905593877\n",
      "Episode Reward: 3.0\n",
      "Step 288 (1582904) @ Episode 459/2500, loss: 0.0111835338175296785\n",
      "Episode Reward: 2.0\n",
      "Step 360 (1583264) @ Episode 460/2500, loss: 0.2091653347015380965\n",
      "Episode Reward: 3.0\n",
      "Step 372 (1583636) @ Episode 461/2500, loss: 0.0143605563789606136\n",
      "Episode Reward: 2.0\n",
      "Step 537 (1584173) @ Episode 462/2500, loss: 0.0152202621102333075\n",
      "Episode Reward: 3.0\n",
      "Step 405 (1584578) @ Episode 463/2500, loss: 0.0107365995645523076\n",
      "Episode Reward: 4.0\n",
      "Step 213 (1584791) @ Episode 464/2500, loss: 0.0068617928773164755\n",
      "Episode Reward: 1.0\n",
      "Step 266 (1585057) @ Episode 465/2500, loss: 0.0104896007105708122\n",
      "Episode Reward: 2.0\n",
      "Step 246 (1585303) @ Episode 466/2500, loss: 0.0133991902694106175\n",
      "Episode Reward: 1.0\n",
      "Step 536 (1585839) @ Episode 467/2500, loss: 0.0791393816471099915\n",
      "Episode Reward: 4.0\n",
      "Step 369 (1586208) @ Episode 468/2500, loss: 0.0270341411232948345\n",
      "Episode Reward: 3.0\n",
      "Step 341 (1586549) @ Episode 469/2500, loss: 0.0094997128471732145\n",
      "Episode Reward: 3.0\n",
      "Step 374 (1586923) @ Episode 470/2500, loss: 0.0067667579278349884\n",
      "Episode Reward: 2.0\n",
      "Step 509 (1587432) @ Episode 471/2500, loss: 0.0074564204551279545\n",
      "Episode Reward: 5.0\n",
      "Step 418 (1587850) @ Episode 472/2500, loss: 0.0282562673091888435\n",
      "Episode Reward: 4.0\n",
      "Step 509 (1588359) @ Episode 473/2500, loss: 0.0076373219490051274\n",
      "Episode Reward: 5.0\n",
      "Step 482 (1588841) @ Episode 474/2500, loss: 0.0091408025473356255\n",
      "Episode Reward: 6.0\n",
      "Step 825 (1589666) @ Episode 475/2500, loss: 0.0088606467470526795\n",
      "Episode Reward: 6.0\n",
      "Step 403 (1590069) @ Episode 476/2500, loss: 0.0117340562865138055\n",
      "Episode Reward: 5.0\n",
      "Step 266 (1590335) @ Episode 477/2500, loss: 0.0043043834157288074\n",
      "Episode Reward: 2.0\n",
      "Step 341 (1590676) @ Episode 478/2500, loss: 0.0134516954421997077\n",
      "Episode Reward: 2.0\n",
      "Step 545 (1591221) @ Episode 479/2500, loss: 0.0059961657971143725\n",
      "Episode Reward: 6.0\n",
      "Step 455 (1591676) @ Episode 480/2500, loss: 0.0105489268898963936\n",
      "Episode Reward: 4.0\n",
      "Step 437 (1592113) @ Episode 481/2500, loss: 0.0295335352420806925\n",
      "Episode Reward: 3.0\n",
      "Step 401 (1592514) @ Episode 482/2500, loss: 0.0138718951493501665\n",
      "Episode Reward: 4.0\n",
      "Step 269 (1592783) @ Episode 483/2500, loss: 0.0091542396694421775\n",
      "Episode Reward: 2.0\n",
      "Step 363 (1593146) @ Episode 484/2500, loss: 0.0253566838800907146\n",
      "Episode Reward: 3.0\n",
      "Step 451 (1593597) @ Episode 485/2500, loss: 0.0121620735153555876\n",
      "Episode Reward: 5.0\n",
      "Step 345 (1593942) @ Episode 486/2500, loss: 0.0057757683098316195\n",
      "Episode Reward: 3.0\n",
      "Step 417 (1594359) @ Episode 487/2500, loss: 0.0129037769511342055\n",
      "Episode Reward: 4.0\n",
      "Step 440 (1594799) @ Episode 488/2500, loss: 0.0105900904163718224\n",
      "Episode Reward: 5.0\n",
      "Step 444 (1595243) @ Episode 489/2500, loss: 0.0125308139249682435\n",
      "Episode Reward: 5.0\n",
      "Step 263 (1595506) @ Episode 490/2500, loss: 0.0056513668969273575\n",
      "Episode Reward: 2.0\n",
      "Step 308 (1595814) @ Episode 491/2500, loss: 0.0039788312278687955\n",
      "Episode Reward: 2.0\n",
      "Step 370 (1596184) @ Episode 492/2500, loss: 0.0063953138887882237\n",
      "Episode Reward: 4.0\n",
      "Step 374 (1596558) @ Episode 493/2500, loss: 0.0039107380434870726\n",
      "Episode Reward: 3.0\n",
      "Step 428 (1596986) @ Episode 494/2500, loss: 0.0096309687942266464\n",
      "Episode Reward: 5.0\n",
      "Step 331 (1597317) @ Episode 495/2500, loss: 0.0070816227234904275\n",
      "Episode Reward: 3.0\n",
      "Step 305 (1597622) @ Episode 496/2500, loss: 0.0058471979573369035\n",
      "Episode Reward: 3.0\n",
      "Step 423 (1598045) @ Episode 497/2500, loss: 0.0052344012074172535\n",
      "Episode Reward: 3.0\n",
      "Step 356 (1598401) @ Episode 498/2500, loss: 0.0180440507829189356\n",
      "Episode Reward: 3.0\n",
      "Step 408 (1598809) @ Episode 499/2500, loss: 0.0072740260511636735\n",
      "Episode Reward: 3.0\n",
      "Step 319 (1599128) @ Episode 500/2500, loss: 0.0072202142328023916\n",
      "Episode Reward: 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-12 19:58:35,641] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video000500.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 291 (1599419) @ Episode 501/2500, loss: 0.0070132981054484846\n",
      "Episode Reward: 1.0\n",
      "Step 440 (1599859) @ Episode 502/2500, loss: 0.0191306360065937045\n",
      "Episode Reward: 4.0\n",
      "Step 370 (1600229) @ Episode 503/2500, loss: 0.0084934420883655557\n",
      "Episode Reward: 3.0\n",
      "Step 314 (1600543) @ Episode 504/2500, loss: 0.0149475485086441045\n",
      "Episode Reward: 3.0\n",
      "Step 503 (1601046) @ Episode 505/2500, loss: 0.0077398358844220645\n",
      "Episode Reward: 5.0\n",
      "Step 229 (1601275) @ Episode 506/2500, loss: 0.2310506552457809436\n",
      "Episode Reward: 1.0\n",
      "Step 337 (1601612) @ Episode 507/2500, loss: 0.0060793580487370495\n",
      "Episode Reward: 3.0\n",
      "Step 404 (1602016) @ Episode 508/2500, loss: 0.0399171784520149256\n",
      "Episode Reward: 5.0\n",
      "Step 397 (1602413) @ Episode 509/2500, loss: 0.0064097400754690175\n",
      "Episode Reward: 4.0\n",
      "Step 384 (1602797) @ Episode 510/2500, loss: 0.0123771652579307564\n",
      "Episode Reward: 4.0\n",
      "Step 398 (1603195) @ Episode 511/2500, loss: 0.1732791066169738855\n",
      "Episode Reward: 3.0\n",
      "Step 335 (1603530) @ Episode 512/2500, loss: 0.0067038028500974186\n",
      "Episode Reward: 2.0\n",
      "Step 317 (1603847) @ Episode 513/2500, loss: 0.0087949335575103765\n",
      "Episode Reward: 2.0\n",
      "Step 442 (1604289) @ Episode 514/2500, loss: 0.0157858822494745255\n",
      "Episode Reward: 3.0\n",
      "Step 527 (1604816) @ Episode 515/2500, loss: 0.0120824184268713586\n",
      "Episode Reward: 6.0\n",
      "Step 475 (1605291) @ Episode 516/2500, loss: 0.0122565990313887635\n",
      "Episode Reward: 4.0\n",
      "Step 319 (1605610) @ Episode 517/2500, loss: 0.0147478766739368448\n",
      "Episode Reward: 2.0\n",
      "Step 376 (1605986) @ Episode 518/2500, loss: 0.0141917774453759245\n",
      "Episode Reward: 3.0\n",
      "Step 456 (1606442) @ Episode 519/2500, loss: 0.0086527392268180857\n",
      "Episode Reward: 3.0\n",
      "Step 402 (1606844) @ Episode 520/2500, loss: 0.0093806516379117975\n",
      "Episode Reward: 4.0\n",
      "Step 645 (1607489) @ Episode 521/2500, loss: 0.0120680928230285645\n",
      "Episode Reward: 6.0\n",
      "Step 389 (1607878) @ Episode 522/2500, loss: 0.1425054520368576643\n",
      "Episode Reward: 3.0\n",
      "Step 265 (1608143) @ Episode 523/2500, loss: 0.0051570534706115724\n",
      "Episode Reward: 2.0\n",
      "Step 446 (1608589) @ Episode 524/2500, loss: 0.0555524751543998765\n",
      "Episode Reward: 2.0\n",
      "Step 463 (1609052) @ Episode 525/2500, loss: 0.0071086576208472255\n",
      "Episode Reward: 5.0\n",
      "Step 346 (1609398) @ Episode 526/2500, loss: 0.0123449098318815235\n",
      "Episode Reward: 3.0\n",
      "Step 310 (1609708) @ Episode 527/2500, loss: 0.0134400110691785815\n",
      "Episode Reward: 3.0\n",
      "Step 393 (1610101) @ Episode 528/2500, loss: 0.0186853781342506446\n",
      "Episode Reward: 4.0\n",
      "Step 540 (1610641) @ Episode 529/2500, loss: 0.0044201938435435295\n",
      "Episode Reward: 7.0\n",
      "Step 420 (1611061) @ Episode 530/2500, loss: 0.0062726112082600593\n",
      "Episode Reward: 4.0\n",
      "Step 545 (1611606) @ Episode 531/2500, loss: 0.0124434968456625945\n",
      "Episode Reward: 4.0\n",
      "Step 373 (1611979) @ Episode 532/2500, loss: 0.1229208335280418435\n",
      "Episode Reward: 4.0\n",
      "Step 400 (1612379) @ Episode 533/2500, loss: 0.0118625508621335036\n",
      "Episode Reward: 2.0\n",
      "Step 273 (1612652) @ Episode 534/2500, loss: 0.0106751583516597757\n",
      "Episode Reward: 2.0\n",
      "Step 295 (1612947) @ Episode 535/2500, loss: 0.0143367983400821694\n",
      "Episode Reward: 2.0\n",
      "Step 370 (1613317) @ Episode 536/2500, loss: 0.0092988573014736186\n",
      "Episode Reward: 2.0\n",
      "Step 427 (1613744) @ Episode 537/2500, loss: 0.0237650778144598875\n",
      "Episode Reward: 4.0\n",
      "Step 457 (1614201) @ Episode 538/2500, loss: 0.0076362402178347116\n",
      "Episode Reward: 5.0\n",
      "Step 539 (1614740) @ Episode 539/2500, loss: 0.2573073804378509535\n",
      "Episode Reward: 5.0\n",
      "Step 411 (1615151) @ Episode 540/2500, loss: 0.0108770625665783885\n",
      "Episode Reward: 3.0\n",
      "Step 245 (1615396) @ Episode 541/2500, loss: 0.0063560972921550275\n",
      "Episode Reward: 1.0\n",
      "Step 666 (1616062) @ Episode 542/2500, loss: 0.0143139399588108066\n",
      "Episode Reward: 6.0\n",
      "Step 360 (1616422) @ Episode 543/2500, loss: 0.0239180326461792455\n",
      "Episode Reward: 3.0\n",
      "Step 229 (1616651) @ Episode 544/2500, loss: 0.0113360974937677384\n",
      "Episode Reward: 1.0\n",
      "Step 328 (1616979) @ Episode 545/2500, loss: 0.0772009417414665246\n",
      "Episode Reward: 3.0\n",
      "Step 374 (1617353) @ Episode 546/2500, loss: 0.0073551507666707044\n",
      "Episode Reward: 3.0\n",
      "Step 625 (1617978) @ Episode 547/2500, loss: 0.0430229268968105365\n",
      "Episode Reward: 10.0\n",
      "Step 354 (1618332) @ Episode 548/2500, loss: 0.0192191526293754584\n",
      "Episode Reward: 3.0\n",
      "Step 529 (1618861) @ Episode 549/2500, loss: 0.0092817628756165585\n",
      "Episode Reward: 5.0\n",
      "Step 291 (1619152) @ Episode 550/2500, loss: 0.0142916403710842136\n",
      "Episode Reward: 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-12 20:36:21,255] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video000550.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 370 (1619522) @ Episode 551/2500, loss: 0.0068280166015028955\n",
      "Episode Reward: 3.0\n",
      "Step 475 (1619997) @ Episode 552/2500, loss: 0.0394440181553363835\n",
      "Episode Reward: 3.0\n",
      "Step 351 (1620348) @ Episode 553/2500, loss: 0.0049247900024056435\n",
      "Episode Reward: 3.0\n",
      "Step 483 (1620831) @ Episode 554/2500, loss: 0.0082861324772238735\n",
      "Episode Reward: 5.0\n",
      "Step 349 (1621180) @ Episode 555/2500, loss: 0.0175481513142585755\n",
      "Episode Reward: 3.0\n",
      "Step 628 (1621808) @ Episode 556/2500, loss: 0.0310445576906204224\n",
      "Episode Reward: 7.0\n",
      "Step 471 (1622279) @ Episode 557/2500, loss: 0.0088342595845460985\n",
      "Episode Reward: 4.0\n",
      "Step 321 (1622600) @ Episode 558/2500, loss: 0.0109408963471651085\n",
      "Episode Reward: 2.0\n",
      "Step 311 (1622911) @ Episode 559/2500, loss: 0.0157030392438173335\n",
      "Episode Reward: 2.0\n",
      "Step 405 (1623316) @ Episode 560/2500, loss: 0.0073946746997535235\n",
      "Episode Reward: 2.0\n",
      "Step 281 (1623597) @ Episode 561/2500, loss: 0.0102373603731393815\n",
      "Episode Reward: 2.0\n",
      "Step 416 (1624013) @ Episode 562/2500, loss: 0.0084864161908626565\n",
      "Episode Reward: 5.0\n",
      "Step 467 (1624480) @ Episode 563/2500, loss: 0.0069834985770285136\n",
      "Episode Reward: 5.0\n",
      "Step 468 (1624948) @ Episode 564/2500, loss: 0.0122344400733709345\n",
      "Episode Reward: 5.0\n",
      "Step 503 (1625451) @ Episode 565/2500, loss: 0.0086884740740060825\n",
      "Episode Reward: 5.0\n",
      "Step 617 (1626068) @ Episode 566/2500, loss: 0.0093821771442890175\n",
      "Episode Reward: 6.0\n",
      "Step 281 (1626349) @ Episode 567/2500, loss: 0.0422320254147052765\n",
      "Episode Reward: 2.0\n",
      "Step 286 (1626635) @ Episode 568/2500, loss: 0.0120283216238021855\n",
      "Episode Reward: 2.0\n",
      "Step 424 (1627059) @ Episode 569/2500, loss: 0.0272203609347343444\n",
      "Episode Reward: 1.0\n",
      "Step 311 (1627370) @ Episode 570/2500, loss: 0.0108272684738039975\n",
      "Episode Reward: 2.0\n",
      "Step 265 (1627635) @ Episode 571/2500, loss: 0.0269576795399189565\n",
      "Episode Reward: 1.0\n",
      "Step 549 (1628184) @ Episode 572/2500, loss: 0.0069358362816274175\n",
      "Episode Reward: 6.0\n",
      "Step 639 (1628823) @ Episode 573/2500, loss: 0.0188224073499441155\n",
      "Episode Reward: 7.0\n",
      "Step 578 (1629401) @ Episode 574/2500, loss: 0.0070483065210282865\n",
      "Episode Reward: 6.0\n",
      "Step 506 (1629907) @ Episode 575/2500, loss: 0.0339587368071079256\n",
      "Episode Reward: 5.0\n",
      "Step 413 (1630320) @ Episode 576/2500, loss: 0.0046756085939705375\n",
      "Episode Reward: 4.0\n",
      "Step 390 (1630710) @ Episode 577/2500, loss: 0.0102865099906921395\n",
      "Episode Reward: 3.0\n",
      "Step 413 (1631123) @ Episode 578/2500, loss: 0.0598149634897708967\n",
      "Episode Reward: 2.0\n",
      "Step 464 (1631587) @ Episode 579/2500, loss: 0.0138883795589208625\n",
      "Episode Reward: 5.0\n",
      "Step 475 (1632062) @ Episode 580/2500, loss: 0.0177816525101661685\n",
      "Episode Reward: 5.0\n",
      "Step 572 (1632634) @ Episode 581/2500, loss: 0.0178201682865619665\n",
      "Episode Reward: 7.0\n",
      "Step 429 (1633063) @ Episode 582/2500, loss: 0.0137066133320331575\n",
      "Episode Reward: 4.0\n",
      "Step 210 (1633273) @ Episode 583/2500, loss: 0.0111305182799696925\n",
      "Episode Reward: 0.0\n",
      "Step 378 (1633651) @ Episode 584/2500, loss: 0.0177445150911808894\n",
      "Episode Reward: 4.0\n",
      "Step 561 (1634212) @ Episode 585/2500, loss: 0.0260955113917589225\n",
      "Episode Reward: 5.0\n",
      "Step 415 (1634627) @ Episode 586/2500, loss: 0.1729496866464615865\n",
      "Episode Reward: 3.0\n",
      "Step 476 (1635103) @ Episode 587/2500, loss: 0.0182400569319725044\n",
      "Episode Reward: 4.0\n",
      "Step 324 (1635427) @ Episode 588/2500, loss: 0.0087403813377022745\n",
      "Episode Reward: 3.0\n",
      "Step 499 (1635926) @ Episode 589/2500, loss: 0.0150607507675886155\n",
      "Episode Reward: 6.0\n",
      "Step 385 (1636311) @ Episode 590/2500, loss: 0.0183473750948905944\n",
      "Episode Reward: 4.0\n",
      "Step 428 (1636739) @ Episode 591/2500, loss: 0.0067631192505359655\n",
      "Episode Reward: 4.0\n",
      "Step 543 (1637282) @ Episode 592/2500, loss: 0.0107067599892616275\n",
      "Episode Reward: 8.0\n",
      "Step 416 (1637698) @ Episode 593/2500, loss: 0.0253153741359710756\n",
      "Episode Reward: 3.0\n",
      "Step 543 (1638241) @ Episode 594/2500, loss: 0.0413185842335224155\n",
      "Episode Reward: 3.0\n",
      "Step 354 (1638595) @ Episode 595/2500, loss: 0.0186655707657337275\n",
      "Episode Reward: 2.0\n",
      "Step 424 (1639019) @ Episode 596/2500, loss: 0.0127402618527412414\n",
      "Episode Reward: 4.0\n",
      "Step 409 (1639428) @ Episode 597/2500, loss: 0.0064256973564624795\n",
      "Episode Reward: 3.0\n",
      "Step 379 (1639807) @ Episode 598/2500, loss: 0.0194815583527088175\n",
      "Episode Reward: 4.0\n",
      "Step 363 (1640170) @ Episode 599/2500, loss: 0.0062282960861921315\n",
      "Episode Reward: 4.0\n",
      "Step 297 (1640467) @ Episode 600/2500, loss: 0.0191293004900217065\n",
      "Episode Reward: 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-12 21:18:05,740] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video000600.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 484 (1640951) @ Episode 601/2500, loss: 0.0188989564776420665\n",
      "Episode Reward: 8.0\n",
      "Step 426 (1641377) @ Episode 602/2500, loss: 0.0049057458527386195\n",
      "Episode Reward: 5.0\n",
      "Step 649 (1642026) @ Episode 603/2500, loss: 0.0082448525354266176\n",
      "Episode Reward: 6.0\n",
      "Step 534 (1642560) @ Episode 604/2500, loss: 0.0112693635746836665\n",
      "Episode Reward: 4.0\n",
      "Step 331 (1642891) @ Episode 605/2500, loss: 0.0092056449502706534\n",
      "Episode Reward: 3.0\n",
      "Step 427 (1643318) @ Episode 606/2500, loss: 0.0102420002222061166\n",
      "Episode Reward: 4.0\n",
      "Step 521 (1643839) @ Episode 607/2500, loss: 0.0077132480219006546\n",
      "Episode Reward: 7.0\n",
      "Step 379 (1644218) @ Episode 608/2500, loss: 0.0075261127203702934\n",
      "Episode Reward: 3.0\n",
      "Step 448 (1644666) @ Episode 609/2500, loss: 0.0087871458381414414\n",
      "Episode Reward: 4.0\n",
      "Step 405 (1645071) @ Episode 610/2500, loss: 0.0094790784642100334\n",
      "Episode Reward: 5.0\n",
      "Step 545 (1645616) @ Episode 611/2500, loss: 0.0232382994145154955\n",
      "Episode Reward: 6.0\n",
      "Step 488 (1646104) @ Episode 612/2500, loss: 0.0345924831926822664\n",
      "Episode Reward: 6.0\n",
      "Step 341 (1646445) @ Episode 613/2500, loss: 0.0070873326621949676\n",
      "Episode Reward: 3.0\n",
      "Step 257 (1646702) @ Episode 614/2500, loss: 0.0360843650996685086\n",
      "Episode Reward: 1.0\n",
      "Step 441 (1647143) @ Episode 615/2500, loss: 0.0131380828097462655\n",
      "Episode Reward: 4.0\n",
      "Step 379 (1647522) @ Episode 616/2500, loss: 0.0301322042942047125\n",
      "Episode Reward: 3.0\n",
      "Step 521 (1648043) @ Episode 617/2500, loss: 0.0100895585492253363\n",
      "Episode Reward: 3.0\n",
      "Step 526 (1648569) @ Episode 618/2500, loss: 0.0186616145074367524\n",
      "Episode Reward: 7.0\n",
      "Step 417 (1648986) @ Episode 619/2500, loss: 0.0131237339228391655\n",
      "Episode Reward: 2.0\n",
      "Step 491 (1649477) @ Episode 620/2500, loss: 0.0197536069899797445\n",
      "Episode Reward: 9.0\n",
      "Step 375 (1649852) @ Episode 621/2500, loss: 0.0215112790465354925\n",
      "Episode Reward: 3.0\n",
      "Step 549 (1650401) @ Episode 622/2500, loss: 0.0137323364615440375\n",
      "Episode Reward: 4.0\n",
      "Step 345 (1650746) @ Episode 623/2500, loss: 0.0140642542392015465\n",
      "Episode Reward: 2.0\n",
      "Step 611 (1651357) @ Episode 624/2500, loss: 0.0067380541004240515\n",
      "Episode Reward: 4.0\n",
      "Step 381 (1651738) @ Episode 625/2500, loss: 0.0075552631169557574\n",
      "Episode Reward: 3.0\n",
      "Step 480 (1652218) @ Episode 626/2500, loss: 0.0176081098616123295\n",
      "Episode Reward: 8.0\n",
      "Step 431 (1652649) @ Episode 627/2500, loss: 0.0059848241508007055\n",
      "Episode Reward: 5.0\n",
      "Step 380 (1653029) @ Episode 628/2500, loss: 0.0983281731605529815\n",
      "Episode Reward: 3.0\n",
      "Step 537 (1653566) @ Episode 629/2500, loss: 0.0143937086686491975\n",
      "Episode Reward: 6.0\n",
      "Step 302 (1653868) @ Episode 630/2500, loss: 0.0132948225364089015\n",
      "Episode Reward: 2.0\n",
      "Step 398 (1654266) @ Episode 631/2500, loss: 0.3122745156288147334\n",
      "Episode Reward: 4.0\n",
      "Step 218 (1654484) @ Episode 632/2500, loss: 0.0085850032046437264\n",
      "Episode Reward: 1.0\n",
      "Step 275 (1654759) @ Episode 633/2500, loss: 0.0093589881435036666\n",
      "Episode Reward: 2.0\n",
      "Step 364 (1655123) @ Episode 634/2500, loss: 0.0130428113043308265\n",
      "Episode Reward: 3.0\n",
      "Step 342 (1655465) @ Episode 635/2500, loss: 0.0074462904594838625\n",
      "Episode Reward: 3.0\n",
      "Step 347 (1655812) @ Episode 636/2500, loss: 0.0912756174802780265\n",
      "Episode Reward: 2.0\n",
      "Step 366 (1656178) @ Episode 637/2500, loss: 0.0131849627941846855\n",
      "Episode Reward: 3.0\n",
      "Step 521 (1656699) @ Episode 638/2500, loss: 0.0171301905065774925\n",
      "Episode Reward: 6.0\n",
      "Step 446 (1657145) @ Episode 639/2500, loss: 0.0158262569457292566\n",
      "Episode Reward: 5.0\n",
      "Step 260 (1657405) @ Episode 640/2500, loss: 0.0107583813369274144\n",
      "Episode Reward: 2.0\n",
      "Step 301 (1657706) @ Episode 641/2500, loss: 0.0067354943603277215\n",
      "Episode Reward: 2.0\n",
      "Step 523 (1658229) @ Episode 642/2500, loss: 0.0160682555288076445\n",
      "Episode Reward: 3.0\n",
      "Step 358 (1658587) @ Episode 643/2500, loss: 0.0124663794413208965\n",
      "Episode Reward: 4.0\n",
      "Step 320 (1658907) @ Episode 644/2500, loss: 0.0081453910097479825\n",
      "Episode Reward: 3.0\n",
      "Step 463 (1659370) @ Episode 645/2500, loss: 0.0097735505551099788\n",
      "Episode Reward: 4.0\n",
      "Step 342 (1659712) @ Episode 646/2500, loss: 0.0056753177195787435\n",
      "Episode Reward: 2.0\n",
      "Step 431 (1660143) @ Episode 647/2500, loss: 0.0096493791788816454\n",
      "Episode Reward: 5.0\n",
      "Step 469 (1660612) @ Episode 648/2500, loss: 0.0074531622231006625\n",
      "Episode Reward: 6.0\n",
      "Step 442 (1661054) @ Episode 649/2500, loss: 0.0116360122337937364\n",
      "Episode Reward: 5.0\n",
      "Step 405 (1661459) @ Episode 650/2500, loss: 0.0168999806046485984\n",
      "Episode Reward: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-12 21:56:26,432] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video000650.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 446 (1661905) @ Episode 651/2500, loss: 0.0141666308045387275\n",
      "Episode Reward: 4.0\n",
      "Step 403 (1662308) @ Episode 652/2500, loss: 0.0125000085681676865\n",
      "Episode Reward: 3.0\n",
      "Step 459 (1662767) @ Episode 653/2500, loss: 0.0236716270446777343\n",
      "Episode Reward: 4.0\n",
      "Step 430 (1663197) @ Episode 654/2500, loss: 0.0096370773389935525\n",
      "Episode Reward: 3.0\n",
      "Step 459 (1663656) @ Episode 655/2500, loss: 0.0136089958250522615\n",
      "Episode Reward: 4.0\n",
      "Step 561 (1664217) @ Episode 656/2500, loss: 0.0163661129772663125\n",
      "Episode Reward: 5.0\n",
      "Step 862 (1665079) @ Episode 657/2500, loss: 0.0160566009581089028\n",
      "Episode Reward: 10.0\n",
      "Step 251 (1665330) @ Episode 658/2500, loss: 0.0087602231651544575\n",
      "Episode Reward: 1.0\n",
      "Step 367 (1665697) @ Episode 659/2500, loss: 0.0098861241713166246\n",
      "Episode Reward: 2.0\n",
      "Step 489 (1666186) @ Episode 660/2500, loss: 0.0125851538032293325\n",
      "Episode Reward: 4.0\n",
      "Step 523 (1666709) @ Episode 661/2500, loss: 0.0104979295283555985\n",
      "Episode Reward: 6.0\n",
      "Step 346 (1667055) @ Episode 662/2500, loss: 0.0075021628290414817\n",
      "Episode Reward: 3.0\n",
      "Step 583 (1667638) @ Episode 663/2500, loss: 0.0100600458681583443\n",
      "Episode Reward: 7.0\n",
      "Step 347 (1667985) @ Episode 664/2500, loss: 0.0067300181835889825\n",
      "Episode Reward: 3.0\n",
      "Step 496 (1668481) @ Episode 665/2500, loss: 0.0090503450483083727\n",
      "Episode Reward: 6.0\n",
      "Step 406 (1668887) @ Episode 666/2500, loss: 0.0054002660326659684\n",
      "Episode Reward: 5.0\n",
      "Step 365 (1669252) @ Episode 667/2500, loss: 0.0117803122848272325\n",
      "Episode Reward: 3.0\n",
      "Step 513 (1669765) @ Episode 668/2500, loss: 0.0145967947319149975\n",
      "Episode Reward: 7.0\n",
      "Step 415 (1670180) @ Episode 669/2500, loss: 0.0085307667031884285\n",
      "Episode Reward: 2.0\n",
      "Step 485 (1670665) @ Episode 670/2500, loss: 0.0113683529198169795\n",
      "Episode Reward: 6.0\n",
      "Step 410 (1671075) @ Episode 671/2500, loss: 0.0124849081039428712\n",
      "Episode Reward: 4.0\n",
      "Step 512 (1671587) @ Episode 672/2500, loss: 0.0056027723476290745\n",
      "Episode Reward: 6.0\n",
      "Step 268 (1671855) @ Episode 673/2500, loss: 0.0076928073540329935\n",
      "Episode Reward: 1.0\n",
      "Step 562 (1672417) @ Episode 674/2500, loss: 0.0119023472070693975\n",
      "Episode Reward: 5.0\n",
      "Step 440 (1672857) @ Episode 675/2500, loss: 0.0107025634497404175\n",
      "Episode Reward: 4.0\n",
      "Step 435 (1673292) @ Episode 676/2500, loss: 0.0375441163778305055\n",
      "Episode Reward: 4.0\n",
      "Step 684 (1673976) @ Episode 677/2500, loss: 0.0070381895639002325\n",
      "Episode Reward: 9.0\n",
      "Step 459 (1674435) @ Episode 678/2500, loss: 0.0228101573884487155\n",
      "Episode Reward: 4.0\n",
      "Step 361 (1674796) @ Episode 679/2500, loss: 0.0050610131584107886\n",
      "Episode Reward: 3.0\n",
      "Step 319 (1675115) @ Episode 680/2500, loss: 0.0133733786642551425\n",
      "Episode Reward: 6.0\n",
      "Step 514 (1675629) @ Episode 681/2500, loss: 0.0151539705693721775\n",
      "Episode Reward: 4.0\n",
      "Step 491 (1676120) @ Episode 682/2500, loss: 0.0067123258486390116\n",
      "Episode Reward: 4.0\n",
      "Step 347 (1676467) @ Episode 683/2500, loss: 0.0087376600131392485\n",
      "Episode Reward: 3.0\n",
      "Step 510 (1676977) @ Episode 684/2500, loss: 0.0235725231468677525\n",
      "Episode Reward: 6.0\n",
      "Step 398 (1677375) @ Episode 685/2500, loss: 0.0081526711583137516\n",
      "Episode Reward: 4.0\n",
      "Step 363 (1677738) @ Episode 686/2500, loss: 0.0091122779995203025\n",
      "Episode Reward: 2.0\n",
      "Step 674 (1678412) @ Episode 687/2500, loss: 0.0046889306977391245\n",
      "Episode Reward: 6.0\n",
      "Step 440 (1678852) @ Episode 688/2500, loss: 0.0068172244355082515\n",
      "Episode Reward: 4.0\n",
      "Step 445 (1679297) @ Episode 689/2500, loss: 0.0155577138066291815\n",
      "Episode Reward: 5.0\n",
      "Step 331 (1679628) @ Episode 690/2500, loss: 0.0085607049986720095\n",
      "Episode Reward: 2.0\n",
      "Step 403 (1680031) @ Episode 691/2500, loss: 0.0249756891280412675\n",
      "Episode Reward: 4.0\n",
      "Step 391 (1680422) @ Episode 692/2500, loss: 0.0492245703935623274\n",
      "Episode Reward: 4.0\n",
      "Step 414 (1680836) @ Episode 693/2500, loss: 0.0128897661343216946\n",
      "Episode Reward: 4.0\n",
      "Step 382 (1681218) @ Episode 694/2500, loss: 0.0211124643683433535\n",
      "Episode Reward: 3.0\n",
      "Step 551 (1681769) @ Episode 695/2500, loss: 0.0071708317846059815\n",
      "Episode Reward: 4.0\n",
      "Step 568 (1682337) @ Episode 696/2500, loss: 0.0055899643339216715\n",
      "Episode Reward: 7.0\n",
      "Step 571 (1682908) @ Episode 697/2500, loss: 0.0091613829135894785\n",
      "Episode Reward: 5.0\n",
      "Step 444 (1683352) @ Episode 698/2500, loss: 0.0334741547703743825\n",
      "Episode Reward: 5.0\n",
      "Step 447 (1683799) @ Episode 699/2500, loss: 0.0112122921273112336\n",
      "Episode Reward: 4.0\n",
      "Step 488 (1684287) @ Episode 700/2500, loss: 0.0208293125033378656\n",
      "Episode Reward: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-12 22:41:30,957] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video000700.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 273 (1684560) @ Episode 701/2500, loss: 0.0150768235325813365\n",
      "Episode Reward: 2.0\n",
      "Step 291 (1684851) @ Episode 702/2500, loss: 0.0086164977401494985\n",
      "Episode Reward: 2.0\n",
      "Step 388 (1685239) @ Episode 703/2500, loss: 0.0030858335085213184\n",
      "Episode Reward: 4.0\n",
      "Step 307 (1685546) @ Episode 704/2500, loss: 0.0125677147880196575\n",
      "Episode Reward: 2.0\n",
      "Step 602 (1686148) @ Episode 705/2500, loss: 0.0136638432741165165\n",
      "Episode Reward: 4.0\n",
      "Step 467 (1686615) @ Episode 706/2500, loss: 0.0078895557671785355\n",
      "Episode Reward: 8.0\n",
      "Step 486 (1687101) @ Episode 707/2500, loss: 0.0047595528885722165\n",
      "Episode Reward: 4.0\n",
      "Step 480 (1687581) @ Episode 708/2500, loss: 0.0176772642880678185\n",
      "Episode Reward: 4.0\n",
      "Step 384 (1687965) @ Episode 709/2500, loss: 0.0062147872522473335\n",
      "Episode Reward: 3.0\n",
      "Step 367 (1688332) @ Episode 710/2500, loss: 0.2606035470962524435\n",
      "Episode Reward: 4.0\n",
      "Step 602 (1688934) @ Episode 711/2500, loss: 0.0072170156054198745\n",
      "Episode Reward: 5.0\n",
      "Step 292 (1689226) @ Episode 712/2500, loss: 0.0119575941935181625\n",
      "Episode Reward: 2.0\n",
      "Step 493 (1689719) @ Episode 713/2500, loss: 0.0054176216945052155\n",
      "Episode Reward: 4.0\n",
      "Step 360 (1690079) @ Episode 714/2500, loss: 0.0104644298553466815\n",
      "Episode Reward: 4.0\n",
      "Step 350 (1690429) @ Episode 715/2500, loss: 0.0568029955029487686\n",
      "Episode Reward: 3.0\n",
      "Step 626 (1691055) @ Episode 716/2500, loss: 0.0118628907948732383\n",
      "Episode Reward: 4.0\n",
      "Step 468 (1691523) @ Episode 717/2500, loss: 0.0583767667412757925\n",
      "Episode Reward: 5.0\n",
      "Step 556 (1692079) @ Episode 718/2500, loss: 0.0433357097208499954\n",
      "Episode Reward: 7.0\n",
      "Step 391 (1692470) @ Episode 719/2500, loss: 0.0096443351358175285\n",
      "Episode Reward: 3.0\n",
      "Step 281 (1692751) @ Episode 720/2500, loss: 0.0041571389883756645\n",
      "Episode Reward: 2.0\n",
      "Step 566 (1693317) @ Episode 721/2500, loss: 0.0063598784618079665\n",
      "Episode Reward: 7.0\n",
      "Step 340 (1693657) @ Episode 722/2500, loss: 0.0140050556510686875\n",
      "Episode Reward: 4.0\n",
      "Step 560 (1694217) @ Episode 723/2500, loss: 0.0154521567746996885\n",
      "Episode Reward: 2.0\n",
      "Step 515 (1694732) @ Episode 724/2500, loss: 0.0082547152414917955\n",
      "Episode Reward: 6.0\n",
      "Step 355 (1695087) @ Episode 725/2500, loss: 0.0467618852853775624\n",
      "Episode Reward: 4.0\n",
      "Step 387 (1695474) @ Episode 726/2500, loss: 0.0092264618724584585\n",
      "Episode Reward: 4.0\n",
      "Step 646 (1696120) @ Episode 727/2500, loss: 0.0085907289758324625\n",
      "Episode Reward: 5.0\n",
      "Step 480 (1696600) @ Episode 728/2500, loss: 0.0067816195078194145\n",
      "Episode Reward: 6.0\n",
      "Step 428 (1697028) @ Episode 729/2500, loss: 0.0116519443690776826\n",
      "Episode Reward: 4.0\n",
      "Step 486 (1697514) @ Episode 730/2500, loss: 0.0507753230631351525\n",
      "Episode Reward: 5.0\n",
      "Step 305 (1697819) @ Episode 731/2500, loss: 0.0099619459360837945\n",
      "Episode Reward: 2.0\n",
      "Step 412 (1698231) @ Episode 732/2500, loss: 0.2078543901443481445\n",
      "Episode Reward: 4.0\n",
      "Step 316 (1698547) @ Episode 733/2500, loss: 0.0060632391832768925\n",
      "Episode Reward: 3.0\n",
      "Step 508 (1699055) @ Episode 734/2500, loss: 0.0088430671021342285\n",
      "Episode Reward: 7.0\n",
      "Step 298 (1699353) @ Episode 735/2500, loss: 0.0106949675828218464\n",
      "Episode Reward: 3.0\n",
      "Step 272 (1699625) @ Episode 736/2500, loss: 0.0097489412873983386\n",
      "Episode Reward: 2.0\n",
      "Step 391 (1700016) @ Episode 737/2500, loss: 0.0110765639692544946\n",
      "Episode Reward: 4.0\n",
      "Step 482 (1700498) @ Episode 738/2500, loss: 0.0140879005193710335\n",
      "Episode Reward: 6.0\n",
      "Step 464 (1700962) @ Episode 739/2500, loss: 0.0429085493087768554\n",
      "Episode Reward: 4.0\n",
      "Step 381 (1701343) @ Episode 740/2500, loss: 0.0425003096461296115\n",
      "Episode Reward: 4.0\n",
      "Step 416 (1701759) @ Episode 741/2500, loss: 0.0067813456989824777\n",
      "Episode Reward: 4.0\n",
      "Step 373 (1702132) @ Episode 742/2500, loss: 0.0204679295420646674\n",
      "Episode Reward: 3.0\n",
      "Step 277 (1702409) @ Episode 743/2500, loss: 0.0334443971514701845\n",
      "Episode Reward: 2.0\n",
      "Step 556 (1702965) @ Episode 744/2500, loss: 0.0121374521404504785\n",
      "Episode Reward: 7.0\n",
      "Step 405 (1703370) @ Episode 745/2500, loss: 0.0877027213573455885\n",
      "Episode Reward: 3.0\n",
      "Step 390 (1703760) @ Episode 746/2500, loss: 0.0086597241461277416\n",
      "Episode Reward: 4.0\n",
      "Step 570 (1704330) @ Episode 747/2500, loss: 0.0095028113573789614\n",
      "Episode Reward: 7.0\n",
      "Step 336 (1704666) @ Episode 748/2500, loss: 0.0224603712558746346\n",
      "Episode Reward: 2.0\n",
      "Step 338 (1705004) @ Episode 749/2500, loss: 0.0149460770189762126\n",
      "Episode Reward: 1.0\n",
      "Step 329 (1705333) @ Episode 750/2500, loss: 0.0082830553874373444\n",
      "Episode Reward: 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-12 23:21:39,116] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video000750.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 487 (1705820) @ Episode 751/2500, loss: 0.0079486528411507665\n",
      "Episode Reward: 6.0\n",
      "Step 342 (1706162) @ Episode 752/2500, loss: 0.0073199104517698295\n",
      "Episode Reward: 3.0\n",
      "Step 487 (1706649) @ Episode 753/2500, loss: 0.0116798169910907755\n",
      "Episode Reward: 5.0\n",
      "Step 308 (1706957) @ Episode 754/2500, loss: 0.0069802328944206245\n",
      "Episode Reward: 3.0\n",
      "Step 420 (1707377) @ Episode 755/2500, loss: 0.0095345191657543185\n",
      "Episode Reward: 3.0\n",
      "Step 385 (1707762) @ Episode 756/2500, loss: 0.0121160019189119345\n",
      "Episode Reward: 4.0\n",
      "Step 304 (1708066) @ Episode 757/2500, loss: 0.0145576503127813344\n",
      "Episode Reward: 3.0\n",
      "Step 254 (1708320) @ Episode 758/2500, loss: 0.0108723193407058725\n",
      "Episode Reward: 2.0\n",
      "Step 596 (1708916) @ Episode 759/2500, loss: 0.0074675632640719415\n",
      "Episode Reward: 6.0\n",
      "Step 400 (1709316) @ Episode 760/2500, loss: 0.0119972387328743935\n",
      "Episode Reward: 4.0\n",
      "Step 665 (1709981) @ Episode 761/2500, loss: 0.0105310445651412015\n",
      "Episode Reward: 7.0\n",
      "Step 455 (1710436) @ Episode 762/2500, loss: 0.1228890419006347745\n",
      "Episode Reward: 4.0\n",
      "Step 336 (1710772) @ Episode 763/2500, loss: 0.0108945500105619437\n",
      "Episode Reward: 3.0\n",
      "Step 400 (1711172) @ Episode 764/2500, loss: 0.0331720635294914255\n",
      "Episode Reward: 4.0\n",
      "Step 485 (1711657) @ Episode 765/2500, loss: 0.0104678142815828325\n",
      "Episode Reward: 4.0\n",
      "Step 222 (1711879) @ Episode 766/2500, loss: 0.0143090737983584468\n",
      "Episode Reward: 1.0\n",
      "Step 331 (1712210) @ Episode 767/2500, loss: 0.0088490284979343415\n",
      "Episode Reward: 3.0\n",
      "Step 376 (1712586) @ Episode 768/2500, loss: 0.0075230859220027926\n",
      "Episode Reward: 4.0\n",
      "Step 556 (1713142) @ Episode 769/2500, loss: 0.0070674205198884015\n",
      "Episode Reward: 4.0\n",
      "Step 454 (1713596) @ Episode 770/2500, loss: 0.0078402087092399665\n",
      "Episode Reward: 2.0\n",
      "Step 361 (1713957) @ Episode 771/2500, loss: 0.0175358634442091955\n",
      "Episode Reward: 4.0\n",
      "Step 380 (1714337) @ Episode 772/2500, loss: 0.0131359491497278215\n",
      "Episode Reward: 3.0\n",
      "Step 564 (1714901) @ Episode 773/2500, loss: 0.1593358218669891425\n",
      "Episode Reward: 5.0\n",
      "Step 299 (1715200) @ Episode 774/2500, loss: 0.0122143868356943136\n",
      "Episode Reward: 3.0\n",
      "Step 429 (1715629) @ Episode 775/2500, loss: 0.0097656091675162327\n",
      "Episode Reward: 4.0\n",
      "Step 413 (1716042) @ Episode 776/2500, loss: 0.0148843042552471163\n",
      "Episode Reward: 4.0\n",
      "Step 404 (1716446) @ Episode 777/2500, loss: 0.0104662673547863965\n",
      "Episode Reward: 5.0\n",
      "Step 422 (1716868) @ Episode 778/2500, loss: 0.0526476018130779346\n",
      "Episode Reward: 4.0\n",
      "Step 609 (1717477) @ Episode 779/2500, loss: 0.0089350119233131418\n",
      "Episode Reward: 5.0\n",
      "Step 444 (1717921) @ Episode 780/2500, loss: 0.2852822244167328145\n",
      "Episode Reward: 5.0\n",
      "Step 437 (1718358) @ Episode 781/2500, loss: 0.0106487507000565534\n",
      "Episode Reward: 3.0\n",
      "Step 492 (1718850) @ Episode 782/2500, loss: 0.0067769652232527735\n",
      "Episode Reward: 5.0\n",
      "Step 457 (1719307) @ Episode 783/2500, loss: 0.0338850095868110665\n",
      "Episode Reward: 4.0\n",
      "Step 339 (1719646) @ Episode 784/2500, loss: 0.0117005873471498495\n",
      "Episode Reward: 1.0\n",
      "Step 294 (1719940) @ Episode 785/2500, loss: 0.0058889919891953476\n",
      "Episode Reward: 2.0\n",
      "Step 431 (1720371) @ Episode 786/2500, loss: 0.0081315040588378945\n",
      "Episode Reward: 4.0\n",
      "Step 387 (1720758) @ Episode 787/2500, loss: 0.0074681593105196958\n",
      "Episode Reward: 4.0\n",
      "Step 428 (1721186) @ Episode 788/2500, loss: 0.0147922430187463765\n",
      "Episode Reward: 4.0\n",
      "Step 466 (1721652) @ Episode 789/2500, loss: 0.0196858346462249766\n",
      "Episode Reward: 5.0\n",
      "Step 320 (1721972) @ Episode 790/2500, loss: 0.0059523126110434536\n",
      "Episode Reward: 2.0\n",
      "Step 291 (1722263) @ Episode 791/2500, loss: 0.0141853429377079013\n",
      "Episode Reward: 2.0\n",
      "Step 355 (1722618) @ Episode 792/2500, loss: 0.1485679298639297525\n",
      "Episode Reward: 4.0\n",
      "Step 319 (1722937) @ Episode 793/2500, loss: 0.0110456971451640135\n",
      "Episode Reward: 3.0\n",
      "Step 539 (1723476) @ Episode 794/2500, loss: 0.0584328249096870434\n",
      "Episode Reward: 7.0\n",
      "Step 384 (1723860) @ Episode 795/2500, loss: 0.0102046765387058265\n",
      "Episode Reward: 4.0\n",
      "Step 443 (1724303) @ Episode 796/2500, loss: 0.0112983491271734245\n",
      "Episode Reward: 3.0\n",
      "Step 440 (1724743) @ Episode 797/2500, loss: 0.0121206734329462055\n",
      "Episode Reward: 5.0\n",
      "Step 387 (1725130) @ Episode 798/2500, loss: 0.0128765795379877096\n",
      "Episode Reward: 4.0\n",
      "Step 438 (1725568) @ Episode 799/2500, loss: 0.0081686787307262424\n",
      "Episode Reward: 4.0\n",
      "Step 336 (1725904) @ Episode 800/2500, loss: 0.0035406686365604435\n",
      "Episode Reward: 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 00:00:08,935] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video000800.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 211 (1726115) @ Episode 801/2500, loss: 0.0479013323783874564\n",
      "Episode Reward: 1.0\n",
      "Step 388 (1726503) @ Episode 802/2500, loss: 0.0137938354164361955\n",
      "Episode Reward: 3.0\n",
      "Step 451 (1726954) @ Episode 803/2500, loss: 0.0135669726878404626\n",
      "Episode Reward: 4.0\n",
      "Step 373 (1727327) @ Episode 804/2500, loss: 0.0480395779013633775\n",
      "Episode Reward: 3.0\n",
      "Step 583 (1727910) @ Episode 805/2500, loss: 0.0115700922906398775\n",
      "Episode Reward: 11.0\n",
      "Step 415 (1728325) @ Episode 806/2500, loss: 0.0507163517177104954\n",
      "Episode Reward: 5.0\n",
      "Step 387 (1728712) @ Episode 807/2500, loss: 0.0211307965219020844\n",
      "Episode Reward: 4.0\n",
      "Step 427 (1729139) @ Episode 808/2500, loss: 0.0257041472941637045\n",
      "Episode Reward: 4.0\n",
      "Step 413 (1729552) @ Episode 809/2500, loss: 0.0114728538319468515\n",
      "Episode Reward: 2.0\n",
      "Step 406 (1729958) @ Episode 810/2500, loss: 0.0046446914784610273\n",
      "Episode Reward: 4.0\n",
      "Step 380 (1730338) @ Episode 811/2500, loss: 0.0069884806871414185\n",
      "Episode Reward: 3.0\n",
      "Step 451 (1730789) @ Episode 812/2500, loss: 0.0213451161980628975\n",
      "Episode Reward: 4.0\n",
      "Step 484 (1731273) @ Episode 813/2500, loss: 0.0247229784727096565\n",
      "Episode Reward: 6.0\n",
      "Step 411 (1731684) @ Episode 814/2500, loss: 0.0134779848158359537\n",
      "Episode Reward: 3.0\n",
      "Step 385 (1732069) @ Episode 815/2500, loss: 0.0112833064049482355\n",
      "Episode Reward: 3.0\n",
      "Step 445 (1732514) @ Episode 816/2500, loss: 0.0048467647284269335\n",
      "Episode Reward: 5.0\n",
      "Step 291 (1732805) @ Episode 817/2500, loss: 0.0077732158824801445\n",
      "Episode Reward: 2.0\n",
      "Step 413 (1733218) @ Episode 818/2500, loss: 0.0033389218151569366\n",
      "Episode Reward: 4.0\n",
      "Step 581 (1733799) @ Episode 819/2500, loss: 0.0088668065145611764\n",
      "Episode Reward: 9.0\n",
      "Step 418 (1734217) @ Episode 820/2500, loss: 0.3039639592170715365\n",
      "Episode Reward: 3.0\n",
      "Step 511 (1734728) @ Episode 821/2500, loss: 0.0605931505560874945\n",
      "Episode Reward: 4.0\n",
      "Step 421 (1735149) @ Episode 822/2500, loss: 0.0061149680987000465\n",
      "Episode Reward: 4.0\n",
      "Step 355 (1735504) @ Episode 823/2500, loss: 0.0074370680376887326\n",
      "Episode Reward: 3.0\n",
      "Step 550 (1736054) @ Episode 824/2500, loss: 0.0167164262384176254\n",
      "Episode Reward: 6.0\n",
      "Step 389 (1736443) @ Episode 825/2500, loss: 0.0134851187467575075\n",
      "Episode Reward: 5.0\n",
      "Step 365 (1736808) @ Episode 826/2500, loss: 0.0288455821573734346\n",
      "Episode Reward: 4.0\n",
      "Step 268 (1737076) @ Episode 827/2500, loss: 0.0391966477036476145\n",
      "Episode Reward: 2.0\n",
      "Step 484 (1737560) @ Episode 828/2500, loss: 0.0115741537883877756\n",
      "Episode Reward: 5.0\n",
      "Step 524 (1738084) @ Episode 829/2500, loss: 0.0153145669028162965\n",
      "Episode Reward: 5.0\n",
      "Step 271 (1738355) @ Episode 830/2500, loss: 0.0137930884957313545\n",
      "Episode Reward: 1.0\n",
      "Step 550 (1738905) @ Episode 831/2500, loss: 0.0108512071892619135\n",
      "Episode Reward: 5.0\n",
      "Step 410 (1739315) @ Episode 832/2500, loss: 0.0133052198216319084\n",
      "Episode Reward: 4.0\n",
      "Step 301 (1739616) @ Episode 833/2500, loss: 0.0208347253501415255\n",
      "Episode Reward: 2.0\n",
      "Step 504 (1740120) @ Episode 834/2500, loss: 0.0069689177908003334\n",
      "Episode Reward: 4.0\n",
      "Step 457 (1740577) @ Episode 835/2500, loss: 0.0049025197513401515\n",
      "Episode Reward: 5.0\n",
      "Step 572 (1741149) @ Episode 836/2500, loss: 0.0125044044107198725\n",
      "Episode Reward: 4.0\n",
      "Step 267 (1741416) @ Episode 837/2500, loss: 0.0126753998920321465\n",
      "Episode Reward: 2.0\n",
      "Step 342 (1741758) @ Episode 838/2500, loss: 0.0076149557717144495\n",
      "Episode Reward: 3.0\n",
      "Step 389 (1742147) @ Episode 839/2500, loss: 0.0100413616746664056\n",
      "Episode Reward: 4.0\n",
      "Step 471 (1742618) @ Episode 840/2500, loss: 0.0201027113944292075\n",
      "Episode Reward: 7.0\n",
      "Step 530 (1743148) @ Episode 841/2500, loss: 0.2158388793468475346\n",
      "Episode Reward: 8.0\n",
      "Step 376 (1743524) @ Episode 842/2500, loss: 0.0164966769516468054\n",
      "Episode Reward: 3.0\n",
      "Step 270 (1743794) @ Episode 843/2500, loss: 0.0057437745854258546\n",
      "Episode Reward: 2.0\n",
      "Step 479 (1744273) @ Episode 844/2500, loss: 0.0158923510462045675\n",
      "Episode Reward: 8.0\n",
      "Step 378 (1744651) @ Episode 845/2500, loss: 0.0102029740810394295\n",
      "Episode Reward: 2.0\n",
      "Step 509 (1745160) @ Episode 846/2500, loss: 0.0047594080679118635\n",
      "Episode Reward: 4.0\n",
      "Step 454 (1745614) @ Episode 847/2500, loss: 0.0217962767928838738\n",
      "Episode Reward: 4.0\n",
      "Step 365 (1745979) @ Episode 848/2500, loss: 0.1225804015994072575\n",
      "Episode Reward: 1.0\n",
      "Step 347 (1746326) @ Episode 849/2500, loss: 0.0115382988005876546\n",
      "Episode Reward: 2.0\n",
      "Step 440 (1746766) @ Episode 850/2500, loss: 0.0143600907176733025\n",
      "Episode Reward: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 00:39:42,940] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video000850.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 289 (1747055) @ Episode 851/2500, loss: 0.0132409911602735525\n",
      "Episode Reward: 3.0\n",
      "Step 449 (1747504) @ Episode 852/2500, loss: 0.0168018415570259195\n",
      "Episode Reward: 4.0\n",
      "Step 521 (1748025) @ Episode 853/2500, loss: 0.0132175404578447344\n",
      "Episode Reward: 4.0\n",
      "Step 275 (1748300) @ Episode 854/2500, loss: 0.0281932894140481956\n",
      "Episode Reward: 2.0\n",
      "Step 371 (1748671) @ Episode 855/2500, loss: 0.0188174638897180565\n",
      "Episode Reward: 2.0\n",
      "Step 354 (1749025) @ Episode 856/2500, loss: 0.0097053805366158495\n",
      "Episode Reward: 3.0\n",
      "Step 396 (1749421) @ Episode 857/2500, loss: 0.0257789473980665296\n",
      "Episode Reward: 4.0\n",
      "Step 351 (1749772) @ Episode 858/2500, loss: 0.0159740876406431225\n",
      "Episode Reward: 3.0\n",
      "Step 241 (1750013) @ Episode 859/2500, loss: 0.0314069651067256944\n",
      "Episode Reward: 1.0\n",
      "Step 379 (1750392) @ Episode 860/2500, loss: 0.0120507813990116123\n",
      "Episode Reward: 4.0\n",
      "Step 205 (1750597) @ Episode 861/2500, loss: 0.0722492784261703534\n",
      "Episode Reward: 1.0\n",
      "Step 407 (1751004) @ Episode 862/2500, loss: 0.0086700860410928735\n",
      "Episode Reward: 4.0\n",
      "Step 573 (1751577) @ Episode 863/2500, loss: 0.0059694461524486545\n",
      "Episode Reward: 6.0\n",
      "Step 760 (1752337) @ Episode 864/2500, loss: 0.0109727224335074423\n",
      "Episode Reward: 6.0\n",
      "Step 420 (1752757) @ Episode 865/2500, loss: 0.0137418247759342225\n",
      "Episode Reward: 4.0\n",
      "Step 434 (1753191) @ Episode 866/2500, loss: 0.0129181947559118278\n",
      "Episode Reward: 6.0\n",
      "Step 396 (1753587) @ Episode 867/2500, loss: 0.0049173561856150636\n",
      "Episode Reward: 5.0\n",
      "Step 484 (1754071) @ Episode 868/2500, loss: 0.0189191959798336035\n",
      "Episode Reward: 6.0\n",
      "Step 510 (1754581) @ Episode 869/2500, loss: 0.0111607890576124195\n",
      "Episode Reward: 5.0\n",
      "Step 229 (1754810) @ Episode 870/2500, loss: 0.0045759011991322045\n",
      "Episode Reward: 1.0\n",
      "Step 351 (1755161) @ Episode 871/2500, loss: 0.0094375442713499076\n",
      "Episode Reward: 3.0\n",
      "Step 363 (1755524) @ Episode 872/2500, loss: 0.0170616898685693744\n",
      "Episode Reward: 4.0\n",
      "Step 326 (1755850) @ Episode 873/2500, loss: 0.0194456316530704535\n",
      "Episode Reward: 2.0\n",
      "Step 463 (1756313) @ Episode 874/2500, loss: 0.0111134722828865055\n",
      "Episode Reward: 5.0\n",
      "Step 488 (1756801) @ Episode 875/2500, loss: 0.2834730744361877414\n",
      "Episode Reward: 5.0\n",
      "Step 489 (1757290) @ Episode 876/2500, loss: 0.0192081481218338465\n",
      "Episode Reward: 3.0\n",
      "Step 408 (1757698) @ Episode 877/2500, loss: 0.0266347657889127734\n",
      "Episode Reward: 4.0\n",
      "Step 450 (1758148) @ Episode 878/2500, loss: 0.2363439202308654826\n",
      "Episode Reward: 3.0\n",
      "Step 610 (1758758) @ Episode 879/2500, loss: 0.0235459264367818835\n",
      "Episode Reward: 8.0\n",
      "Step 430 (1759188) @ Episode 880/2500, loss: 0.0097078643739223487\n",
      "Episode Reward: 3.0\n",
      "Step 428 (1759616) @ Episode 881/2500, loss: 0.0070823449641466145\n",
      "Episode Reward: 2.0\n",
      "Step 507 (1760123) @ Episode 882/2500, loss: 0.0459723658859729853\n",
      "Episode Reward: 5.0\n",
      "Step 625 (1760748) @ Episode 883/2500, loss: 0.0212459862232208255\n",
      "Episode Reward: 5.0\n",
      "Step 606 (1761354) @ Episode 884/2500, loss: 0.0062906788662076335\n",
      "Episode Reward: 9.0\n",
      "Step 326 (1761680) @ Episode 885/2500, loss: 0.0169889051467180255\n",
      "Episode Reward: 3.0\n",
      "Step 437 (1762117) @ Episode 886/2500, loss: 0.0108795221894979485\n",
      "Episode Reward: 5.0\n",
      "Step 316 (1762433) @ Episode 887/2500, loss: 0.0103530837222933776\n",
      "Episode Reward: 2.0\n",
      "Step 558 (1762991) @ Episode 888/2500, loss: 0.0564801618456840575\n",
      "Episode Reward: 9.0\n",
      "Step 609 (1763600) @ Episode 889/2500, loss: 0.0045196041464805685\n",
      "Episode Reward: 10.0\n",
      "Step 439 (1764039) @ Episode 890/2500, loss: 0.0322934389114379975\n",
      "Episode Reward: 5.0\n",
      "Step 460 (1764499) @ Episode 891/2500, loss: 0.0230617001652717664\n",
      "Episode Reward: 5.0\n",
      "Step 443 (1764942) @ Episode 892/2500, loss: 0.0075647667981684215\n",
      "Episode Reward: 5.0\n",
      "Step 458 (1765400) @ Episode 893/2500, loss: 0.0050727641209959986\n",
      "Episode Reward: 5.0\n",
      "Step 296 (1765696) @ Episode 894/2500, loss: 0.0194851458072662352\n",
      "Episode Reward: 3.0\n",
      "Step 466 (1766162) @ Episode 895/2500, loss: 0.0137725174427032475\n",
      "Episode Reward: 6.0\n",
      "Step 440 (1766602) @ Episode 896/2500, loss: 0.0071207107976078994\n",
      "Episode Reward: 5.0\n",
      "Step 432 (1767034) @ Episode 897/2500, loss: 0.0065372050739824776\n",
      "Episode Reward: 3.0\n",
      "Step 415 (1767449) @ Episode 898/2500, loss: 0.0437937751412391665\n",
      "Episode Reward: 4.0\n",
      "Step 303 (1767752) @ Episode 899/2500, loss: 0.0211141183972358796\n",
      "Episode Reward: 3.0\n",
      "Step 442 (1768194) @ Episode 900/2500, loss: 0.0095631573349237445\n",
      "Episode Reward: 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 01:20:49,289] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video000900.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 288 (1768482) @ Episode 901/2500, loss: 0.0127215357497334485\n",
      "Episode Reward: 3.0\n",
      "Step 521 (1769003) @ Episode 902/2500, loss: 0.0071050380356609825\n",
      "Episode Reward: 6.0\n",
      "Step 691 (1769694) @ Episode 903/2500, loss: 0.0167134944349527366\n",
      "Episode Reward: 7.0\n",
      "Step 426 (1770120) @ Episode 904/2500, loss: 0.0104438550770282754\n",
      "Episode Reward: 5.0\n",
      "Step 391 (1770511) @ Episode 905/2500, loss: 0.0168935917317867285\n",
      "Episode Reward: 4.0\n",
      "Step 330 (1770841) @ Episode 906/2500, loss: 0.0324244275689125066\n",
      "Episode Reward: 3.0\n",
      "Step 485 (1771326) @ Episode 907/2500, loss: 0.0040041124448180263\n",
      "Episode Reward: 5.0\n",
      "Step 467 (1771793) @ Episode 908/2500, loss: 0.0056236577220261115\n",
      "Episode Reward: 6.0\n",
      "Step 497 (1772290) @ Episode 909/2500, loss: 0.2870000004768371615\n",
      "Episode Reward: 4.0\n",
      "Step 302 (1772592) @ Episode 910/2500, loss: 0.0101984599605202672\n",
      "Episode Reward: 2.0\n",
      "Step 266 (1772858) @ Episode 911/2500, loss: 0.0154464123770594695\n",
      "Episode Reward: 1.0\n",
      "Step 439 (1773297) @ Episode 912/2500, loss: 0.0106645077466964725\n",
      "Episode Reward: 3.0\n",
      "Step 649 (1773946) @ Episode 913/2500, loss: 0.0083289155736565594\n",
      "Episode Reward: 7.0\n",
      "Step 397 (1774343) @ Episode 914/2500, loss: 0.0159853324294090275\n",
      "Episode Reward: 4.0\n",
      "Step 379 (1774722) @ Episode 915/2500, loss: 0.0103140417486429214\n",
      "Episode Reward: 3.0\n",
      "Step 424 (1775146) @ Episode 916/2500, loss: 0.0071079819463193425\n",
      "Episode Reward: 4.0\n",
      "Step 370 (1775516) @ Episode 917/2500, loss: 0.0101474672555923465\n",
      "Episode Reward: 3.0\n",
      "Step 348 (1775864) @ Episode 918/2500, loss: 0.0042387722060084345\n",
      "Episode Reward: 2.0\n",
      "Step 573 (1776437) @ Episode 919/2500, loss: 0.0142062725499272357\n",
      "Episode Reward: 7.0\n",
      "Step 475 (1776912) @ Episode 920/2500, loss: 0.0245083607733249666\n",
      "Episode Reward: 4.0\n",
      "Step 498 (1777410) @ Episode 921/2500, loss: 0.0051380507647991185\n",
      "Episode Reward: 6.0\n",
      "Step 511 (1777921) @ Episode 922/2500, loss: 0.0135942539200186735\n",
      "Episode Reward: 4.0\n",
      "Step 490 (1778411) @ Episode 923/2500, loss: 0.0141427032649517066\n",
      "Episode Reward: 5.0\n",
      "Step 290 (1778701) @ Episode 924/2500, loss: 0.0160655900835990945\n",
      "Episode Reward: 1.0\n",
      "Step 584 (1779285) @ Episode 925/2500, loss: 0.0166461467742919923\n",
      "Episode Reward: 6.0\n",
      "Step 484 (1779769) @ Episode 926/2500, loss: 0.0116776116192340856\n",
      "Episode Reward: 5.0\n",
      "Step 462 (1780231) @ Episode 927/2500, loss: 0.0061022746376693252\n",
      "Episode Reward: 3.0\n",
      "Step 425 (1780656) @ Episode 928/2500, loss: 0.0121962483972311024\n",
      "Episode Reward: 4.0\n",
      "Step 505 (1781161) @ Episode 929/2500, loss: 0.0133765246719121935\n",
      "Episode Reward: 5.0\n",
      "Step 439 (1781600) @ Episode 930/2500, loss: 0.0129359979182481775\n",
      "Episode Reward: 3.0\n",
      "Step 364 (1781964) @ Episode 931/2500, loss: 0.0080823190510272985\n",
      "Episode Reward: 4.0\n",
      "Step 554 (1782518) @ Episode 932/2500, loss: 0.0092597454786300665\n",
      "Episode Reward: 6.0\n",
      "Step 266 (1782784) @ Episode 933/2500, loss: 0.0146214757114648825\n",
      "Episode Reward: 1.0\n",
      "Step 580 (1783364) @ Episode 934/2500, loss: 0.0073453509248793125\n",
      "Episode Reward: 7.0\n",
      "Step 323 (1783687) @ Episode 935/2500, loss: 0.0154642518609762234\n",
      "Episode Reward: 3.0\n",
      "Step 412 (1784099) @ Episode 936/2500, loss: 0.0086174942553043375\n",
      "Episode Reward: 4.0\n",
      "Step 373 (1784472) @ Episode 937/2500, loss: 0.0113187972456216815\n",
      "Episode Reward: 4.0\n",
      "Step 542 (1785014) @ Episode 938/2500, loss: 0.1281947046518325867\n",
      "Episode Reward: 9.0\n",
      "Step 357 (1785371) @ Episode 939/2500, loss: 0.0288719236850738535\n",
      "Episode Reward: 3.0\n",
      "Step 387 (1785758) @ Episode 940/2500, loss: 0.0132959792390465742\n",
      "Episode Reward: 4.0\n",
      "Step 406 (1786164) @ Episode 941/2500, loss: 0.0969600677490234473\n",
      "Episode Reward: 3.0\n",
      "Step 574 (1786738) @ Episode 942/2500, loss: 0.0128746712580323225\n",
      "Episode Reward: 6.0\n",
      "Step 416 (1787154) @ Episode 943/2500, loss: 0.0178644452244043355\n",
      "Episode Reward: 3.0\n",
      "Step 341 (1787495) @ Episode 944/2500, loss: 0.0038606389425694942\n",
      "Episode Reward: 3.0\n",
      "Step 459 (1787954) @ Episode 945/2500, loss: 0.0104607958346605375\n",
      "Episode Reward: 5.0\n",
      "Step 675 (1788629) @ Episode 946/2500, loss: 0.0324361287057399755\n",
      "Episode Reward: 8.0\n",
      "Step 341 (1788970) @ Episode 947/2500, loss: 0.0446034967899322545\n",
      "Episode Reward: 3.0\n",
      "Step 496 (1789466) @ Episode 948/2500, loss: 0.0055345268920063975\n",
      "Episode Reward: 5.0\n",
      "Step 456 (1789922) @ Episode 949/2500, loss: 0.0151584986597299585\n",
      "Episode Reward: 4.0\n",
      "Step 486 (1790408) @ Episode 950/2500, loss: 0.0097583569586277035\n",
      "Episode Reward: 6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 02:04:08,973] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video000950.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 448 (1790856) @ Episode 951/2500, loss: 0.0084615983068943026\n",
      "Episode Reward: 3.0\n",
      "Step 412 (1791268) @ Episode 952/2500, loss: 0.0126124816015362745\n",
      "Episode Reward: 8.0\n",
      "Step 529 (1791797) @ Episode 953/2500, loss: 0.0221200063824653636\n",
      "Episode Reward: 6.0\n",
      "Step 354 (1792151) @ Episode 954/2500, loss: 0.0085283787921071054\n",
      "Episode Reward: 3.0\n",
      "Step 506 (1792657) @ Episode 955/2500, loss: 0.0084952265024185187\n",
      "Episode Reward: 5.0\n",
      "Step 467 (1793124) @ Episode 956/2500, loss: 0.0356588289141655835\n",
      "Episode Reward: 3.0\n",
      "Step 396 (1793520) @ Episode 957/2500, loss: 0.0059344992041587835\n",
      "Episode Reward: 3.0\n",
      "Step 277 (1793797) @ Episode 958/2500, loss: 0.0097067849710583695\n",
      "Episode Reward: 2.0\n",
      "Step 474 (1794271) @ Episode 959/2500, loss: 0.0611265189945697855\n",
      "Episode Reward: 5.0\n",
      "Step 249 (1794520) @ Episode 960/2500, loss: 0.0077809565700590615\n",
      "Episode Reward: 1.0\n",
      "Step 737 (1795257) @ Episode 961/2500, loss: 0.0074505475349724297\n",
      "Episode Reward: 8.0\n",
      "Step 607 (1795864) @ Episode 962/2500, loss: 0.0076051950454711914\n",
      "Episode Reward: 7.0\n",
      "Step 632 (1796496) @ Episode 963/2500, loss: 0.0126954820007085836\n",
      "Episode Reward: 7.0\n",
      "Step 202 (1796698) @ Episode 964/2500, loss: 0.0144955292344093325\n",
      "Episode Reward: 1.0\n",
      "Step 393 (1797091) @ Episode 965/2500, loss: 0.0139505956321954734\n",
      "Episode Reward: 4.0\n",
      "Step 429 (1797520) @ Episode 966/2500, loss: 0.0109958071261644365\n",
      "Episode Reward: 5.0\n",
      "Step 494 (1798014) @ Episode 967/2500, loss: 0.0121842361986637124\n",
      "Episode Reward: 6.0\n",
      "Step 631 (1798645) @ Episode 968/2500, loss: 0.0166646279394626625\n",
      "Episode Reward: 9.0\n",
      "Step 310 (1798955) @ Episode 969/2500, loss: 0.0145937316119670875\n",
      "Episode Reward: 1.0\n",
      "Step 456 (1799411) @ Episode 970/2500, loss: 0.0074926018714904785\n",
      "Episode Reward: 5.0\n",
      "Step 374 (1799785) @ Episode 971/2500, loss: 0.0165337640792131426\n",
      "Episode Reward: 4.0\n",
      "Step 399 (1800184) @ Episode 972/2500, loss: 0.0107623767107725146\n",
      "Episode Reward: 3.0\n",
      "Step 215 (1800399) @ Episode 973/2500, loss: 0.0129822436720132835\n",
      "Episode Reward: 1.0\n",
      "Step 300 (1800699) @ Episode 974/2500, loss: 0.0054123355075716975\n",
      "Episode Reward: 2.0\n",
      "Step 450 (1801149) @ Episode 975/2500, loss: 0.0436556935310363855\n",
      "Episode Reward: 8.0\n",
      "Step 625 (1801774) @ Episode 976/2500, loss: 0.0358564481139183045\n",
      "Episode Reward: 5.0\n",
      "Step 658 (1802432) @ Episode 977/2500, loss: 0.0200796611607074745\n",
      "Episode Reward: 7.0\n",
      "Step 305 (1802737) @ Episode 978/2500, loss: 0.0086345504969358445\n",
      "Episode Reward: 2.0\n",
      "Step 436 (1803173) @ Episode 979/2500, loss: 0.0119034163653850568\n",
      "Episode Reward: 5.0\n",
      "Step 255 (1803428) @ Episode 980/2500, loss: 0.0118854381144046785\n",
      "Episode Reward: 2.0\n",
      "Step 391 (1803819) @ Episode 981/2500, loss: 0.2432534396648407697\n",
      "Episode Reward: 4.0\n",
      "Step 373 (1804192) @ Episode 982/2500, loss: 0.0129564600065350535\n",
      "Episode Reward: 4.0\n",
      "Step 407 (1804599) @ Episode 983/2500, loss: 0.0283914040774106986\n",
      "Episode Reward: 5.0\n",
      "Step 391 (1804990) @ Episode 984/2500, loss: 0.0104255862534046177\n",
      "Episode Reward: 3.0\n",
      "Step 308 (1805298) @ Episode 985/2500, loss: 0.0062633119523525245\n",
      "Episode Reward: 3.0\n",
      "Step 615 (1805913) @ Episode 986/2500, loss: 0.0071140746586024765\n",
      "Episode Reward: 7.0\n",
      "Step 433 (1806346) @ Episode 987/2500, loss: 0.0075524738058447846\n",
      "Episode Reward: 5.0\n",
      "Step 602 (1806948) @ Episode 988/2500, loss: 0.0071738287806510925\n",
      "Episode Reward: 5.0\n",
      "Step 515 (1807463) @ Episode 989/2500, loss: 0.0095801427960395815\n",
      "Episode Reward: 6.0\n",
      "Step 350 (1807813) @ Episode 990/2500, loss: 0.0172996222972869875\n",
      "Episode Reward: 3.0\n",
      "Step 431 (1808244) @ Episode 991/2500, loss: 0.0112132541835308075\n",
      "Episode Reward: 4.0\n",
      "Step 476 (1808720) @ Episode 992/2500, loss: 0.0127203110605478295\n",
      "Episode Reward: 5.0\n",
      "Step 475 (1809195) @ Episode 993/2500, loss: 0.0800818204879760735\n",
      "Episode Reward: 3.0\n",
      "Step 352 (1809547) @ Episode 994/2500, loss: 0.0157939996570348745\n",
      "Episode Reward: 3.0\n",
      "Step 392 (1809939) @ Episode 995/2500, loss: 0.0065150940790772445\n",
      "Episode Reward: 4.0\n",
      "Step 461 (1810400) @ Episode 996/2500, loss: 0.1702718734741211985\n",
      "Episode Reward: 3.0\n",
      "Step 380 (1810780) @ Episode 997/2500, loss: 0.0569648183882236525\n",
      "Episode Reward: 4.0\n",
      "Step 281 (1811061) @ Episode 998/2500, loss: 0.0520997792482376143\n",
      "Episode Reward: 1.0\n",
      "Step 366 (1811427) @ Episode 999/2500, loss: 0.0074230753816664226\n",
      "Episode Reward: 3.0\n",
      "Step 605 (1812032) @ Episode 1000/2500, loss: 0.0063808201812207785\n",
      "Episode Reward: 6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 02:45:40,425] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video001000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 511 (1812543) @ Episode 1001/2500, loss: 0.0137297101318836215\n",
      "Episode Reward: 5.0\n",
      "Step 490 (1813033) @ Episode 1002/2500, loss: 0.0166382957249879846\n",
      "Episode Reward: 4.0\n",
      "Step 588 (1813621) @ Episode 1003/2500, loss: 0.0534296743571758345\n",
      "Episode Reward: 5.0\n",
      "Step 458 (1814079) @ Episode 1004/2500, loss: 0.0083469171077013025\n",
      "Episode Reward: 4.0\n",
      "Step 366 (1814445) @ Episode 1005/2500, loss: 0.0164407100528478624\n",
      "Episode Reward: 4.0\n",
      "Step 491 (1814936) @ Episode 1006/2500, loss: 0.0132138673216104554\n",
      "Episode Reward: 5.0\n",
      "Step 566 (1815502) @ Episode 1007/2500, loss: 0.0084052057936787615\n",
      "Episode Reward: 3.0\n",
      "Step 455 (1815957) @ Episode 1008/2500, loss: 0.0324347317218780525\n",
      "Episode Reward: 5.0\n",
      "Step 467 (1816424) @ Episode 1009/2500, loss: 0.0070532262325286865\n",
      "Episode Reward: 5.0\n",
      "Step 420 (1816844) @ Episode 1010/2500, loss: 0.0052824812009930617\n",
      "Episode Reward: 2.0\n",
      "Step 462 (1817306) @ Episode 1011/2500, loss: 0.0193665437400341036\n",
      "Episode Reward: 4.0\n",
      "Step 218 (1817524) @ Episode 1012/2500, loss: 0.0162362903356552125\n",
      "Episode Reward: 1.0\n",
      "Step 399 (1817923) @ Episode 1013/2500, loss: 0.0066777183674275875\n",
      "Episode Reward: 4.0\n",
      "Step 364 (1818287) @ Episode 1014/2500, loss: 0.0187396761029958725\n",
      "Episode Reward: 3.0\n",
      "Step 468 (1818755) @ Episode 1015/2500, loss: 0.0052703060209751135\n",
      "Episode Reward: 4.0\n",
      "Step 377 (1819132) @ Episode 1016/2500, loss: 0.0166274365037679675\n",
      "Episode Reward: 4.0\n",
      "Step 385 (1819517) @ Episode 1017/2500, loss: 0.0044718813151121145\n",
      "Episode Reward: 5.0\n",
      "Step 245 (1819762) @ Episode 1018/2500, loss: 0.0036311307922005653\n",
      "Episode Reward: 1.0\n",
      "Step 404 (1820166) @ Episode 1019/2500, loss: 0.0079812165349721925\n",
      "Episode Reward: 4.0\n",
      "Step 341 (1820507) @ Episode 1020/2500, loss: 0.0044475486502051352\n",
      "Episode Reward: 3.0\n",
      "Step 671 (1821178) @ Episode 1021/2500, loss: 0.0223860666155815125\n",
      "Episode Reward: 4.0\n",
      "Step 364 (1821542) @ Episode 1022/2500, loss: 0.2748160362243652367\n",
      "Episode Reward: 3.0\n",
      "Step 307 (1821849) @ Episode 1023/2500, loss: 0.0462602600455284175\n",
      "Episode Reward: 3.0\n",
      "Step 346 (1822195) @ Episode 1024/2500, loss: 0.0062459241598844534\n",
      "Episode Reward: 4.0\n",
      "Step 589 (1822784) @ Episode 1025/2500, loss: 0.0104532074183225634\n",
      "Episode Reward: 5.0\n",
      "Step 296 (1823080) @ Episode 1026/2500, loss: 0.0160801392048597344\n",
      "Episode Reward: 3.0\n",
      "Step 440 (1823520) @ Episode 1027/2500, loss: 0.0078597702085971835\n",
      "Episode Reward: 4.0\n",
      "Step 251 (1823771) @ Episode 1028/2500, loss: 0.0203998275101184845\n",
      "Episode Reward: 1.0\n",
      "Step 279 (1824050) @ Episode 1029/2500, loss: 0.0068524619564414024\n",
      "Episode Reward: 2.0\n",
      "Step 356 (1824406) @ Episode 1030/2500, loss: 0.0322018377482891125\n",
      "Episode Reward: 3.0\n",
      "Step 468 (1824874) @ Episode 1031/2500, loss: 0.2596372067928314635\n",
      "Episode Reward: 5.0\n",
      "Step 413 (1825287) @ Episode 1032/2500, loss: 0.0083119552582502375\n",
      "Episode Reward: 4.0\n",
      "Step 476 (1825763) @ Episode 1033/2500, loss: 0.0046159070916473866\n",
      "Episode Reward: 3.0\n",
      "Step 463 (1826226) @ Episode 1034/2500, loss: 0.0062348218634724623\n",
      "Episode Reward: 4.0\n",
      "Step 368 (1826594) @ Episode 1035/2500, loss: 0.0075343511998653415\n",
      "Episode Reward: 3.0\n",
      "Step 457 (1827051) @ Episode 1036/2500, loss: 0.0110273296013474465\n",
      "Episode Reward: 5.0\n",
      "Step 413 (1827464) @ Episode 1037/2500, loss: 0.0097104851156473165\n",
      "Episode Reward: 5.0\n",
      "Step 569 (1828033) @ Episode 1038/2500, loss: 0.0154619459062814715\n",
      "Episode Reward: 6.0\n",
      "Step 386 (1828419) @ Episode 1039/2500, loss: 0.0080036092549562455\n",
      "Episode Reward: 4.0\n",
      "Step 600 (1829019) @ Episode 1040/2500, loss: 0.0156827419996261625\n",
      "Episode Reward: 6.0\n",
      "Step 406 (1829425) @ Episode 1041/2500, loss: 0.0081563442945480355\n",
      "Episode Reward: 4.0\n",
      "Step 541 (1829966) @ Episode 1042/2500, loss: 0.0058653019368648535\n",
      "Episode Reward: 6.0\n",
      "Step 505 (1830471) @ Episode 1043/2500, loss: 0.0051354975439608125\n",
      "Episode Reward: 5.0\n",
      "Step 471 (1830942) @ Episode 1044/2500, loss: 0.0070102084428071985\n",
      "Episode Reward: 4.0\n",
      "Step 556 (1831498) @ Episode 1045/2500, loss: 0.0133960433304309845\n",
      "Episode Reward: 4.0\n",
      "Step 394 (1831892) @ Episode 1046/2500, loss: 0.0103805214166641245\n",
      "Episode Reward: 3.0\n",
      "Step 274 (1832166) @ Episode 1047/2500, loss: 0.0172539893537759784\n",
      "Episode Reward: 2.0\n",
      "Step 374 (1832540) @ Episode 1048/2500, loss: 0.0695269703865051376\n",
      "Episode Reward: 3.0\n",
      "Step 580 (1833120) @ Episode 1049/2500, loss: 0.0091212950646877295\n",
      "Episode Reward: 7.0\n",
      "Step 494 (1833614) @ Episode 1050/2500, loss: 0.0465422719717025765\n",
      "Episode Reward: 7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 03:26:14,567] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video001050.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 509 (1834123) @ Episode 1051/2500, loss: 0.2685453593730926565\n",
      "Episode Reward: 6.0\n",
      "Step 525 (1834648) @ Episode 1052/2500, loss: 0.0116945747286081316\n",
      "Episode Reward: 4.0\n",
      "Step 698 (1835346) @ Episode 1053/2500, loss: 0.0106176361441612245\n",
      "Episode Reward: 6.0\n",
      "Step 627 (1835973) @ Episode 1054/2500, loss: 0.0117264240980148325\n",
      "Episode Reward: 9.0\n",
      "Step 497 (1836470) @ Episode 1055/2500, loss: 0.0086977956816554075\n",
      "Episode Reward: 5.0\n",
      "Step 496 (1836966) @ Episode 1056/2500, loss: 0.0103915333747863775\n",
      "Episode Reward: 5.0\n",
      "Step 277 (1837243) @ Episode 1057/2500, loss: 0.0176645684987306665\n",
      "Episode Reward: 1.0\n",
      "Step 603 (1837846) @ Episode 1058/2500, loss: 0.0075585371814668185\n",
      "Episode Reward: 6.0\n",
      "Step 509 (1838355) @ Episode 1059/2500, loss: 0.0080699874088168145\n",
      "Episode Reward: 6.0\n",
      "Step 515 (1838870) @ Episode 1060/2500, loss: 0.0126265818253159525\n",
      "Episode Reward: 4.0\n",
      "Step 300 (1839170) @ Episode 1061/2500, loss: 0.0052575757727026944\n",
      "Episode Reward: 2.0\n",
      "Step 415 (1839585) @ Episode 1062/2500, loss: 0.0202415548264980345\n",
      "Episode Reward: 1.0\n",
      "Step 438 (1840023) @ Episode 1063/2500, loss: 0.0099640637636184755\n",
      "Episode Reward: 5.0\n",
      "Step 404 (1840427) @ Episode 1064/2500, loss: 0.0094778081402182585\n",
      "Episode Reward: 5.0\n",
      "Step 339 (1840766) @ Episode 1065/2500, loss: 0.0107544725760817535\n",
      "Episode Reward: 3.0\n",
      "Step 431 (1841197) @ Episode 1066/2500, loss: 0.0089768981561064724\n",
      "Episode Reward: 4.0\n",
      "Step 501 (1841698) @ Episode 1067/2500, loss: 0.0077043296769261365\n",
      "Episode Reward: 4.0\n",
      "Step 314 (1842012) @ Episode 1068/2500, loss: 0.0099437469616532335\n",
      "Episode Reward: 2.0\n",
      "Step 531 (1842543) @ Episode 1069/2500, loss: 0.0065106069669127465\n",
      "Episode Reward: 6.0\n",
      "Step 650 (1843193) @ Episode 1070/2500, loss: 0.0041853357106447225\n",
      "Episode Reward: 7.0\n",
      "Step 372 (1843565) @ Episode 1071/2500, loss: 0.0105478772893548014\n",
      "Episode Reward: 2.0\n",
      "Step 403 (1843968) @ Episode 1072/2500, loss: 0.0048325327225029475\n",
      "Episode Reward: 4.0\n",
      "Step 670 (1844638) @ Episode 1073/2500, loss: 0.0471984669566154524\n",
      "Episode Reward: 6.0\n",
      "Step 473 (1845111) @ Episode 1074/2500, loss: 0.0092025855556130414\n",
      "Episode Reward: 4.0\n",
      "Step 499 (1845610) @ Episode 1075/2500, loss: 0.0128351440653204923\n",
      "Episode Reward: 5.0\n",
      "Step 426 (1846036) @ Episode 1076/2500, loss: 0.0077608786523342136\n",
      "Episode Reward: 4.0\n",
      "Step 272 (1846308) @ Episode 1077/2500, loss: 0.0118844658136367892\n",
      "Episode Reward: 2.0\n",
      "Step 242 (1846550) @ Episode 1078/2500, loss: 0.0091271046549081875\n",
      "Episode Reward: 1.0\n",
      "Step 458 (1847008) @ Episode 1079/2500, loss: 0.0084969960153102875\n",
      "Episode Reward: 3.0\n",
      "Step 489 (1847497) @ Episode 1080/2500, loss: 0.0050860540941357615\n",
      "Episode Reward: 5.0\n",
      "Step 411 (1847908) @ Episode 1081/2500, loss: 0.0102746430784463885\n",
      "Episode Reward: 4.0\n",
      "Step 421 (1848329) @ Episode 1082/2500, loss: 0.0104967756196856555\n",
      "Episode Reward: 3.0\n",
      "Step 396 (1848725) @ Episode 1083/2500, loss: 0.0341530404984951094\n",
      "Episode Reward: 3.0\n",
      "Step 395 (1849120) @ Episode 1084/2500, loss: 0.0075887395069003105\n",
      "Episode Reward: 4.0\n",
      "Step 504 (1849624) @ Episode 1085/2500, loss: 0.0048880334943532945\n",
      "Episode Reward: 4.0\n",
      "Step 291 (1849915) @ Episode 1086/2500, loss: 0.0312547907233238255\n",
      "Episode Reward: 2.0\n",
      "Step 326 (1850241) @ Episode 1087/2500, loss: 0.0059763179160654545\n",
      "Episode Reward: 3.0\n",
      "Step 495 (1850736) @ Episode 1088/2500, loss: 0.0084437727928161625\n",
      "Episode Reward: 5.0\n",
      "Step 270 (1851006) @ Episode 1089/2500, loss: 0.0098919887095689775\n",
      "Episode Reward: 2.0\n",
      "Step 399 (1851405) @ Episode 1090/2500, loss: 0.0077541405335068754\n",
      "Episode Reward: 4.0\n",
      "Step 468 (1851873) @ Episode 1091/2500, loss: 0.0098963920027017648\n",
      "Episode Reward: 3.0\n",
      "Step 287 (1852160) @ Episode 1092/2500, loss: 0.0056975539773702623\n",
      "Episode Reward: 2.0\n",
      "Step 466 (1852626) @ Episode 1093/2500, loss: 0.0143987555056810385\n",
      "Episode Reward: 4.0\n",
      "Step 520 (1853146) @ Episode 1094/2500, loss: 0.0197970680892467585\n",
      "Episode Reward: 4.0\n",
      "Step 502 (1853648) @ Episode 1095/2500, loss: 0.0070785391144454485\n",
      "Episode Reward: 6.0\n",
      "Step 323 (1853971) @ Episode 1096/2500, loss: 0.0133860679343342785\n",
      "Episode Reward: 3.0\n",
      "Step 289 (1854260) @ Episode 1097/2500, loss: 0.0072575872763991365\n",
      "Episode Reward: 2.0\n",
      "Step 512 (1854772) @ Episode 1098/2500, loss: 0.0048121027648448944\n",
      "Episode Reward: 6.0\n",
      "Step 472 (1855244) @ Episode 1099/2500, loss: 0.0053527173586189756\n",
      "Episode Reward: 5.0\n",
      "Step 514 (1855758) @ Episode 1100/2500, loss: 0.0093918479979038245\n",
      "Episode Reward: 6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 04:07:35,008] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video001100.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 421 (1856179) @ Episode 1101/2500, loss: 0.0661471411585807825\n",
      "Episode Reward: 4.0\n",
      "Step 329 (1856508) @ Episode 1102/2500, loss: 0.0324151143431663545\n",
      "Episode Reward: 3.0\n",
      "Step 368 (1856876) @ Episode 1103/2500, loss: 0.0477697998285293695\n",
      "Episode Reward: 3.0\n",
      "Step 362 (1857238) @ Episode 1104/2500, loss: 0.0111701861023902984\n",
      "Episode Reward: 4.0\n",
      "Step 480 (1857718) @ Episode 1105/2500, loss: 0.0051494501531124115\n",
      "Episode Reward: 6.0\n",
      "Step 612 (1858330) @ Episode 1106/2500, loss: 0.0465282127261161875\n",
      "Episode Reward: 7.0\n",
      "Step 394 (1858724) @ Episode 1107/2500, loss: 0.0057194242253899576\n",
      "Episode Reward: 3.0\n",
      "Step 391 (1859115) @ Episode 1108/2500, loss: 0.0110898427665233615\n",
      "Episode Reward: 3.0\n",
      "Step 481 (1859596) @ Episode 1109/2500, loss: 0.0072591356001794345\n",
      "Episode Reward: 5.0\n",
      "Step 653 (1860249) @ Episode 1110/2500, loss: 0.0080109164118766785\n",
      "Episode Reward: 5.0\n",
      "Step 284 (1860533) @ Episode 1111/2500, loss: 0.0132607994601130496\n",
      "Episode Reward: 1.0\n",
      "Step 253 (1860786) @ Episode 1112/2500, loss: 0.0036595317069441085\n",
      "Episode Reward: 2.0\n",
      "Step 589 (1861375) @ Episode 1113/2500, loss: 0.0075927213765680796\n",
      "Episode Reward: 4.0\n",
      "Step 442 (1861817) @ Episode 1114/2500, loss: 0.0100078564137220385\n",
      "Episode Reward: 5.0\n",
      "Step 407 (1862224) @ Episode 1115/2500, loss: 0.0125184003263711934\n",
      "Episode Reward: 4.0\n",
      "Step 487 (1862711) @ Episode 1116/2500, loss: 0.0112340589985251435\n",
      "Episode Reward: 5.0\n",
      "Step 561 (1863272) @ Episode 1117/2500, loss: 0.0146548841148614885\n",
      "Episode Reward: 6.0\n",
      "Step 379 (1863651) @ Episode 1118/2500, loss: 0.1023599803447723496\n",
      "Episode Reward: 3.0\n",
      "Step 598 (1864249) @ Episode 1119/2500, loss: 0.0201943013817071924\n",
      "Episode Reward: 3.0\n",
      "Step 361 (1864610) @ Episode 1120/2500, loss: 0.0427194386720657356\n",
      "Episode Reward: 2.0\n",
      "Step 532 (1865142) @ Episode 1121/2500, loss: 0.0129336453974246985\n",
      "Episode Reward: 5.0\n",
      "Step 407 (1865549) @ Episode 1122/2500, loss: 0.0043102125637233266\n",
      "Episode Reward: 4.0\n",
      "Step 476 (1866025) @ Episode 1123/2500, loss: 0.0537750907242298195\n",
      "Episode Reward: 4.0\n",
      "Step 476 (1866501) @ Episode 1124/2500, loss: 0.0060776886530220515\n",
      "Episode Reward: 5.0\n",
      "Step 391 (1866892) @ Episode 1125/2500, loss: 0.0152177438139915475\n",
      "Episode Reward: 4.0\n",
      "Step 384 (1867276) @ Episode 1126/2500, loss: 0.0130382319912314415\n",
      "Episode Reward: 4.0\n",
      "Step 515 (1867791) @ Episode 1127/2500, loss: 0.0072438903152942665\n",
      "Episode Reward: 4.0\n",
      "Step 377 (1868168) @ Episode 1128/2500, loss: 0.0147111974656581886\n",
      "Episode Reward: 4.0\n",
      "Step 411 (1868579) @ Episode 1129/2500, loss: 0.0140736736357212075\n",
      "Episode Reward: 4.0\n",
      "Step 449 (1869028) @ Episode 1130/2500, loss: 0.0116261802613735235\n",
      "Episode Reward: 4.0\n",
      "Step 603 (1869631) @ Episode 1131/2500, loss: 0.0232547149062156685\n",
      "Episode Reward: 6.0\n",
      "Step 447 (1870078) @ Episode 1132/2500, loss: 0.0079226950183510786\n",
      "Episode Reward: 3.0\n",
      "Step 414 (1870492) @ Episode 1133/2500, loss: 0.0069556459784507753\n",
      "Episode Reward: 5.0\n",
      "Step 582 (1871074) @ Episode 1134/2500, loss: 0.0091004353016614915\n",
      "Episode Reward: 5.0\n",
      "Step 839 (1871913) @ Episode 1135/2500, loss: 0.0093665346503257755\n",
      "Episode Reward: 9.0\n",
      "Step 319 (1872232) @ Episode 1136/2500, loss: 0.0074491072446107864\n",
      "Episode Reward: 2.0\n",
      "Step 339 (1872571) @ Episode 1137/2500, loss: 0.0058418624103069305\n",
      "Episode Reward: 3.0\n",
      "Step 510 (1873081) @ Episode 1138/2500, loss: 0.0176131762564182284\n",
      "Episode Reward: 5.0\n",
      "Step 357 (1873438) @ Episode 1139/2500, loss: 0.0108052995055913936\n",
      "Episode Reward: 3.0\n",
      "Step 516 (1873954) @ Episode 1140/2500, loss: 0.0158088468015193942\n",
      "Episode Reward: 5.0\n",
      "Step 597 (1874551) @ Episode 1141/2500, loss: 0.0097329467535018926\n",
      "Episode Reward: 8.0\n",
      "Step 555 (1875106) @ Episode 1142/2500, loss: 0.0064783766865730286\n",
      "Episode Reward: 5.0\n",
      "Step 275 (1875381) @ Episode 1143/2500, loss: 0.0087151192128658325\n",
      "Episode Reward: 2.0\n",
      "Step 283 (1875664) @ Episode 1144/2500, loss: 0.1658219099044799835\n",
      "Episode Reward: 3.0\n",
      "Step 515 (1876179) @ Episode 1145/2500, loss: 0.0153932003304362354\n",
      "Episode Reward: 6.0\n",
      "Step 270 (1876449) @ Episode 1146/2500, loss: 0.0044680386781692505\n",
      "Episode Reward: 2.0\n",
      "Step 478 (1876927) @ Episode 1147/2500, loss: 0.0128253102302551275\n",
      "Episode Reward: 5.0\n",
      "Step 445 (1877372) @ Episode 1148/2500, loss: 0.0401829704642295845\n",
      "Episode Reward: 5.0\n",
      "Step 310 (1877682) @ Episode 1149/2500, loss: 0.0080881817266345025\n",
      "Episode Reward: 2.0\n",
      "Step 359 (1878041) @ Episode 1150/2500, loss: 0.0031558978371322155\n",
      "Episode Reward: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 04:49:47,234] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video001150.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 586 (1878627) @ Episode 1151/2500, loss: 0.0108698513358831428\n",
      "Episode Reward: 6.0\n",
      "Step 377 (1879004) @ Episode 1152/2500, loss: 0.0069035710766911515\n",
      "Episode Reward: 3.0\n",
      "Step 617 (1879621) @ Episode 1153/2500, loss: 0.0048620691522955894\n",
      "Episode Reward: 7.0\n",
      "Step 560 (1880181) @ Episode 1154/2500, loss: 0.0084491474553942685\n",
      "Episode Reward: 5.0\n",
      "Step 468 (1880649) @ Episode 1155/2500, loss: 0.0083530712872743643\n",
      "Episode Reward: 2.0\n",
      "Step 703 (1881352) @ Episode 1156/2500, loss: 0.0092098806053400045\n",
      "Episode Reward: 6.0\n",
      "Step 285 (1881637) @ Episode 1157/2500, loss: 0.0125695094466209415\n",
      "Episode Reward: 2.0\n",
      "Step 286 (1881923) @ Episode 1158/2500, loss: 0.0306557938456535345\n",
      "Episode Reward: 2.0\n",
      "Step 440 (1882363) @ Episode 1159/2500, loss: 0.0133660361170768745\n",
      "Episode Reward: 4.0\n",
      "Step 278 (1882641) @ Episode 1160/2500, loss: 0.0392016395926475535\n",
      "Episode Reward: 2.0\n",
      "Step 267 (1882908) @ Episode 1161/2500, loss: 0.0074690319597721145\n",
      "Episode Reward: 1.0\n",
      "Step 311 (1883219) @ Episode 1162/2500, loss: 0.0053374124690890315\n",
      "Episode Reward: 1.0\n",
      "Step 526 (1883745) @ Episode 1163/2500, loss: 0.0048691155388951345\n",
      "Episode Reward: 5.0\n",
      "Step 318 (1884063) @ Episode 1164/2500, loss: 0.0071508279070258145\n",
      "Episode Reward: 2.0\n",
      "Step 547 (1884610) @ Episode 1165/2500, loss: 0.0096112620085477834\n",
      "Episode Reward: 9.0\n",
      "Step 650 (1885260) @ Episode 1166/2500, loss: 0.0061756987124681473\n",
      "Episode Reward: 4.0\n",
      "Step 526 (1885786) @ Episode 1167/2500, loss: 0.0061929682269692426\n",
      "Episode Reward: 5.0\n",
      "Step 427 (1886213) @ Episode 1168/2500, loss: 0.0093306908383965534\n",
      "Episode Reward: 4.0\n",
      "Step 599 (1886812) @ Episode 1169/2500, loss: 0.0190729498863220276\n",
      "Episode Reward: 6.0\n",
      "Step 400 (1887212) @ Episode 1170/2500, loss: 0.0133606158196926126\n",
      "Episode Reward: 4.0\n",
      "Step 447 (1887659) @ Episode 1171/2500, loss: 0.0061719766817986965\n",
      "Episode Reward: 4.0\n",
      "Step 436 (1888095) @ Episode 1172/2500, loss: 0.0081078074872493745\n",
      "Episode Reward: 2.0\n",
      "Step 468 (1888563) @ Episode 1173/2500, loss: 0.2418667823076248256\n",
      "Episode Reward: 4.0\n",
      "Step 452 (1889015) @ Episode 1174/2500, loss: 0.0231214761734008895\n",
      "Episode Reward: 5.0\n",
      "Step 328 (1889343) @ Episode 1175/2500, loss: 0.0130769070237874985\n",
      "Episode Reward: 3.0\n",
      "Step 379 (1889722) @ Episode 1176/2500, loss: 0.0133654177188873294\n",
      "Episode Reward: 3.0\n",
      "Step 723 (1890445) @ Episode 1177/2500, loss: 0.0056280177086591728\n",
      "Episode Reward: 7.0\n",
      "Step 405 (1890850) @ Episode 1178/2500, loss: 0.0178728103637695385\n",
      "Episode Reward: 4.0\n",
      "Step 373 (1891223) @ Episode 1179/2500, loss: 0.0129576651379466066\n",
      "Episode Reward: 3.0\n",
      "Step 561 (1891784) @ Episode 1180/2500, loss: 0.0083992127329111115\n",
      "Episode Reward: 6.0\n",
      "Step 377 (1892161) @ Episode 1181/2500, loss: 0.0138705521821975785\n",
      "Episode Reward: 3.0\n",
      "Step 500 (1892661) @ Episode 1182/2500, loss: 0.0032520717941224575\n",
      "Episode Reward: 5.0\n",
      "Step 393 (1893054) @ Episode 1183/2500, loss: 0.0192906409502029426\n",
      "Episode Reward: 4.0\n",
      "Step 506 (1893560) @ Episode 1184/2500, loss: 0.0334242358803749124\n",
      "Episode Reward: 12.0\n",
      "Step 608 (1894168) @ Episode 1185/2500, loss: 0.0080244345590472224\n",
      "Episode Reward: 6.0\n",
      "Step 506 (1894674) @ Episode 1186/2500, loss: 0.0292397439479827885\n",
      "Episode Reward: 5.0\n",
      "Step 801 (1895475) @ Episode 1187/2500, loss: 0.0082786716520786296\n",
      "Episode Reward: 11.0\n",
      "Step 401 (1895876) @ Episode 1188/2500, loss: 0.0046579227782785895\n",
      "Episode Reward: 3.0\n",
      "Step 439 (1896315) @ Episode 1189/2500, loss: 0.0047737588174641133\n",
      "Episode Reward: 3.0\n",
      "Step 618 (1896933) @ Episode 1190/2500, loss: 0.0075275786221027376\n",
      "Episode Reward: 8.0\n",
      "Step 449 (1897382) @ Episode 1191/2500, loss: 0.0160017982125282335\n",
      "Episode Reward: 5.0\n",
      "Step 509 (1897891) @ Episode 1192/2500, loss: 0.1417907327413559155\n",
      "Episode Reward: 3.0\n",
      "Step 314 (1898205) @ Episode 1193/2500, loss: 0.0048675779253244455\n",
      "Episode Reward: 2.0\n",
      "Step 431 (1898636) @ Episode 1194/2500, loss: 0.0074578505009412766\n",
      "Episode Reward: 4.0\n",
      "Step 582 (1899218) @ Episode 1195/2500, loss: 0.0087982825934886935\n",
      "Episode Reward: 6.0\n",
      "Step 563 (1899781) @ Episode 1196/2500, loss: 0.0210899170488119135\n",
      "Episode Reward: 5.0\n",
      "Step 540 (1900321) @ Episode 1197/2500, loss: 0.0131877288222312937\n",
      "Episode Reward: 7.0\n",
      "Step 516 (1900837) @ Episode 1198/2500, loss: 0.0157888643443584444\n",
      "Episode Reward: 5.0\n",
      "Step 409 (1901246) @ Episode 1199/2500, loss: 0.0048241768963634976\n",
      "Episode Reward: 4.0\n",
      "Step 281 (1901527) @ Episode 1200/2500, loss: 0.0102029498666524895\n",
      "Episode Reward: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 05:35:00,293] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video001200.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 435 (1901962) @ Episode 1201/2500, loss: 0.0093409270048141484\n",
      "Episode Reward: 5.0\n",
      "Step 351 (1902313) @ Episode 1202/2500, loss: 0.0042794998735189445\n",
      "Episode Reward: 4.0\n",
      "Step 608 (1902921) @ Episode 1203/2500, loss: 0.0683597698807716495\n",
      "Episode Reward: 6.0\n",
      "Step 472 (1903393) @ Episode 1204/2500, loss: 0.0073599601164460185\n",
      "Episode Reward: 5.0\n",
      "Step 269 (1903662) @ Episode 1205/2500, loss: 0.0207064319401979456\n",
      "Episode Reward: 2.0\n",
      "Step 599 (1904261) @ Episode 1206/2500, loss: 0.0417982973158359545\n",
      "Episode Reward: 6.0\n",
      "Step 523 (1904784) @ Episode 1207/2500, loss: 0.0080466819927096376\n",
      "Episode Reward: 4.0\n",
      "Step 452 (1905236) @ Episode 1208/2500, loss: 0.0057834740728139886\n",
      "Episode Reward: 4.0\n",
      "Step 588 (1905824) @ Episode 1209/2500, loss: 0.0126179233193397525\n",
      "Episode Reward: 5.0\n",
      "Step 325 (1906149) @ Episode 1210/2500, loss: 0.0401010923087596994\n",
      "Episode Reward: 3.0\n",
      "Step 448 (1906597) @ Episode 1211/2500, loss: 0.0109738726168870936\n",
      "Episode Reward: 5.0\n",
      "Step 321 (1906918) @ Episode 1212/2500, loss: 0.0066801831126213077\n",
      "Episode Reward: 3.0\n",
      "Step 599 (1907517) @ Episode 1213/2500, loss: 0.0103901755064725886\n",
      "Episode Reward: 6.0\n",
      "Step 320 (1907837) @ Episode 1214/2500, loss: 0.0090592186897993097\n",
      "Episode Reward: 2.0\n",
      "Step 672 (1908509) @ Episode 1215/2500, loss: 0.0130647616460919385\n",
      "Episode Reward: 10.0\n",
      "Step 563 (1909072) @ Episode 1216/2500, loss: 0.0117369750514626545\n",
      "Episode Reward: 7.0\n",
      "Step 331 (1909403) @ Episode 1217/2500, loss: 0.0047845947556197645\n",
      "Episode Reward: 3.0\n",
      "Step 382 (1909785) @ Episode 1218/2500, loss: 0.0073394849896430975\n",
      "Episode Reward: 4.0\n",
      "Step 466 (1910251) @ Episode 1219/2500, loss: 0.0067670503631234174\n",
      "Episode Reward: 6.0\n",
      "Step 390 (1910641) @ Episode 1220/2500, loss: 0.0120057882741093645\n",
      "Episode Reward: 3.0\n",
      "Step 293 (1910934) @ Episode 1221/2500, loss: 0.0079479143023490923\n",
      "Episode Reward: 2.0\n",
      "Step 556 (1911490) @ Episode 1222/2500, loss: 0.0324264653027057656\n",
      "Episode Reward: 6.0\n",
      "Step 326 (1911816) @ Episode 1223/2500, loss: 0.1436542868614196895\n",
      "Episode Reward: 3.0\n",
      "Step 432 (1912248) @ Episode 1224/2500, loss: 0.0162314437329769135\n",
      "Episode Reward: 3.0\n",
      "Step 404 (1912652) @ Episode 1225/2500, loss: 0.0126725342124700555\n",
      "Episode Reward: 3.0\n",
      "Step 419 (1913071) @ Episode 1226/2500, loss: 0.0111274570226669315\n",
      "Episode Reward: 4.0\n",
      "Step 303 (1913374) @ Episode 1227/2500, loss: 0.0139095634222030645\n",
      "Episode Reward: 3.0\n",
      "Step 296 (1913670) @ Episode 1228/2500, loss: 0.0070408745668828495\n",
      "Episode Reward: 2.0\n",
      "Step 550 (1914220) @ Episode 1229/2500, loss: 0.0089618302881717687\n",
      "Episode Reward: 6.0\n",
      "Step 404 (1914624) @ Episode 1230/2500, loss: 0.0094483168795704848\n",
      "Episode Reward: 4.0\n",
      "Step 293 (1914917) @ Episode 1231/2500, loss: 0.0053952662274241457\n",
      "Episode Reward: 2.0\n",
      "Step 561 (1915478) @ Episode 1232/2500, loss: 0.0073436568491160875\n",
      "Episode Reward: 7.0\n",
      "Step 402 (1915880) @ Episode 1233/2500, loss: 0.0839245170354843125\n",
      "Episode Reward: 4.0\n",
      "Step 457 (1916337) @ Episode 1234/2500, loss: 0.0129663860425353053\n",
      "Episode Reward: 5.0\n",
      "Step 369 (1916706) @ Episode 1235/2500, loss: 0.0044504972174763683\n",
      "Episode Reward: 3.0\n",
      "Step 398 (1917104) @ Episode 1236/2500, loss: 0.0060065723955631266\n",
      "Episode Reward: 5.0\n",
      "Step 484 (1917588) @ Episode 1237/2500, loss: 0.0089419633150100736\n",
      "Episode Reward: 4.0\n",
      "Step 527 (1918115) @ Episode 1238/2500, loss: 0.0101272808387875565\n",
      "Episode Reward: 7.0\n",
      "Step 432 (1918547) @ Episode 1239/2500, loss: 0.0108578912913799295\n",
      "Episode Reward: 5.0\n",
      "Step 598 (1919145) @ Episode 1240/2500, loss: 0.0062475297600030934\n",
      "Episode Reward: 5.0\n",
      "Step 604 (1919749) @ Episode 1241/2500, loss: 0.0071374773979187016\n",
      "Episode Reward: 7.0\n",
      "Step 595 (1920344) @ Episode 1242/2500, loss: 0.2268230319023132365\n",
      "Episode Reward: 6.0\n",
      "Step 422 (1920766) @ Episode 1243/2500, loss: 0.0083956858143210415\n",
      "Episode Reward: 4.0\n",
      "Step 339 (1921105) @ Episode 1244/2500, loss: 0.0126906055957078935\n",
      "Episode Reward: 3.0\n",
      "Step 490 (1921595) @ Episode 1245/2500, loss: 0.0210944786667823894\n",
      "Episode Reward: 6.0\n",
      "Step 322 (1921917) @ Episode 1246/2500, loss: 0.0165576338768005373\n",
      "Episode Reward: 3.0\n",
      "Step 268 (1922185) @ Episode 1247/2500, loss: 0.0054537141695618635\n",
      "Episode Reward: 2.0\n",
      "Step 298 (1922483) @ Episode 1248/2500, loss: 0.0220850035548210145\n",
      "Episode Reward: 2.0\n",
      "Step 302 (1922785) @ Episode 1249/2500, loss: 0.0255406871438026434\n",
      "Episode Reward: 2.0\n",
      "Step 481 (1923266) @ Episode 1250/2500, loss: 0.0087723275646567345\n",
      "Episode Reward: 6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 06:17:17,211] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video001250.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 472 (1923738) @ Episode 1251/2500, loss: 0.0030960266012698415\n",
      "Episode Reward: 6.0\n",
      "Step 372 (1924110) @ Episode 1252/2500, loss: 0.0094426274299621585\n",
      "Episode Reward: 4.0\n",
      "Step 336 (1924446) @ Episode 1253/2500, loss: 0.0090099684894084935\n",
      "Episode Reward: 3.0\n",
      "Step 426 (1924872) @ Episode 1254/2500, loss: 0.0081196362152695665\n",
      "Episode Reward: 5.0\n",
      "Step 307 (1925179) @ Episode 1255/2500, loss: 0.0163475498557090766\n",
      "Episode Reward: 3.0\n",
      "Step 282 (1925461) @ Episode 1256/2500, loss: 0.0085385255515575416\n",
      "Episode Reward: 2.0\n",
      "Step 541 (1926002) @ Episode 1257/2500, loss: 0.0138831334188580515\n",
      "Episode Reward: 7.0\n",
      "Step 580 (1926582) @ Episode 1258/2500, loss: 0.0134306102991104135\n",
      "Episode Reward: 6.0\n",
      "Step 285 (1926867) @ Episode 1259/2500, loss: 0.0039305575191974643\n",
      "Episode Reward: 2.0\n",
      "Step 606 (1927473) @ Episode 1260/2500, loss: 0.0079089840874075895\n",
      "Episode Reward: 8.0\n",
      "Step 403 (1927876) @ Episode 1261/2500, loss: 0.0181829147040843965\n",
      "Episode Reward: 5.0\n",
      "Step 449 (1928325) @ Episode 1262/2500, loss: 0.0082461945712566385\n",
      "Episode Reward: 4.0\n",
      "Step 381 (1928706) @ Episode 1263/2500, loss: 0.0077006430365145215\n",
      "Episode Reward: 4.0\n",
      "Step 329 (1929035) @ Episode 1264/2500, loss: 0.0128045678138732913\n",
      "Episode Reward: 3.0\n",
      "Step 361 (1929396) @ Episode 1265/2500, loss: 0.0032897153869271285\n",
      "Episode Reward: 3.0\n",
      "Step 535 (1929931) @ Episode 1266/2500, loss: 0.0064000911079347135\n",
      "Episode Reward: 6.0\n",
      "Step 502 (1930433) @ Episode 1267/2500, loss: 0.0087399482727050788\n",
      "Episode Reward: 5.0\n",
      "Step 267 (1930700) @ Episode 1268/2500, loss: 0.0080465320497751245\n",
      "Episode Reward: 2.0\n",
      "Step 357 (1931057) @ Episode 1269/2500, loss: 0.0094729205593466764\n",
      "Episode Reward: 3.0\n",
      "Step 249 (1931306) @ Episode 1270/2500, loss: 0.0071042389608919625\n",
      "Episode Reward: 2.0\n",
      "Step 400 (1931706) @ Episode 1271/2500, loss: 0.0083040315657854085\n",
      "Episode Reward: 5.0\n",
      "Step 284 (1931990) @ Episode 1272/2500, loss: 0.0077290828339755535\n",
      "Episode Reward: 2.0\n",
      "Step 285 (1932275) @ Episode 1273/2500, loss: 0.0346741974353790343\n",
      "Episode Reward: 2.0\n",
      "Step 443 (1932718) @ Episode 1274/2500, loss: 0.0059716920368373395\n",
      "Episode Reward: 5.0\n",
      "Step 473 (1933191) @ Episode 1275/2500, loss: 0.0091364942491054535\n",
      "Episode Reward: 4.0\n",
      "Step 408 (1933599) @ Episode 1276/2500, loss: 0.0119440313428640374\n",
      "Episode Reward: 4.0\n",
      "Step 429 (1934028) @ Episode 1277/2500, loss: 0.1171426028013229444\n",
      "Episode Reward: 6.0\n",
      "Step 601 (1934629) @ Episode 1278/2500, loss: 0.0204845201224088675\n",
      "Episode Reward: 8.0\n",
      "Step 446 (1935075) @ Episode 1279/2500, loss: 0.1029041185975074873\n",
      "Episode Reward: 4.0\n",
      "Step 447 (1935522) @ Episode 1280/2500, loss: 0.0079430080950260165\n",
      "Episode Reward: 5.0\n",
      "Step 367 (1935889) @ Episode 1281/2500, loss: 0.0237869024276733434\n",
      "Episode Reward: 4.0\n",
      "Step 370 (1936259) @ Episode 1282/2500, loss: 0.0090564014390110972\n",
      "Episode Reward: 4.0\n",
      "Step 319 (1936578) @ Episode 1283/2500, loss: 0.0154801588505506523\n",
      "Episode Reward: 3.0\n",
      "Step 327 (1936905) @ Episode 1284/2500, loss: 0.0148553568869829185\n",
      "Episode Reward: 3.0\n",
      "Step 367 (1937272) @ Episode 1285/2500, loss: 0.0073847756721079353\n",
      "Episode Reward: 4.0\n",
      "Step 632 (1937904) @ Episode 1286/2500, loss: 0.0160270016640424736\n",
      "Episode Reward: 8.0\n",
      "Step 317 (1938221) @ Episode 1287/2500, loss: 0.0341906473040580755\n",
      "Episode Reward: 3.0\n",
      "Step 271 (1938492) @ Episode 1288/2500, loss: 0.0081706093624234225\n",
      "Episode Reward: 2.0\n",
      "Step 471 (1938963) @ Episode 1289/2500, loss: 0.0066824126988649375\n",
      "Episode Reward: 6.0\n",
      "Step 412 (1939375) @ Episode 1290/2500, loss: 0.0055731460452079775\n",
      "Episode Reward: 6.0\n",
      "Step 306 (1939681) @ Episode 1291/2500, loss: 0.0042636222206056125\n",
      "Episode Reward: 3.0\n",
      "Step 471 (1940152) @ Episode 1292/2500, loss: 0.0099959848448634155\n",
      "Episode Reward: 6.0\n",
      "Step 464 (1940616) @ Episode 1293/2500, loss: 0.0039923419244587425\n",
      "Episode Reward: 6.0\n",
      "Step 330 (1940946) @ Episode 1294/2500, loss: 0.0082076732069253925\n",
      "Episode Reward: 4.0\n",
      "Step 418 (1941364) @ Episode 1295/2500, loss: 0.0068957791663706395\n",
      "Episode Reward: 4.0\n",
      "Step 265 (1941629) @ Episode 1296/2500, loss: 0.0055314339697360994\n",
      "Episode Reward: 2.0\n",
      "Step 345 (1941974) @ Episode 1297/2500, loss: 0.0102905873209238055\n",
      "Episode Reward: 3.0\n",
      "Step 435 (1942409) @ Episode 1298/2500, loss: 0.0144668100401759155\n",
      "Episode Reward: 5.0\n",
      "Step 418 (1942827) @ Episode 1299/2500, loss: 0.0052750450558960444\n",
      "Episode Reward: 5.0\n",
      "Step 304 (1943131) @ Episode 1300/2500, loss: 0.0187447220087051493\n",
      "Episode Reward: 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 06:55:36,287] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video001300.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 438 (1943569) @ Episode 1301/2500, loss: 0.0104446820914745332\n",
      "Episode Reward: 5.0\n",
      "Step 415 (1943984) @ Episode 1302/2500, loss: 0.0139489807188510925\n",
      "Episode Reward: 5.0\n",
      "Step 389 (1944373) @ Episode 1303/2500, loss: 0.0154719930142164235\n",
      "Episode Reward: 5.0\n",
      "Step 545 (1944918) @ Episode 1304/2500, loss: 0.0074298344552516945\n",
      "Episode Reward: 7.0\n",
      "Step 445 (1945363) @ Episode 1305/2500, loss: 0.0065840105526149274\n",
      "Episode Reward: 5.0\n",
      "Step 531 (1945894) @ Episode 1306/2500, loss: 0.0050734551623463635\n",
      "Episode Reward: 7.0\n",
      "Step 488 (1946382) @ Episode 1307/2500, loss: 0.0068763028830289845\n",
      "Episode Reward: 6.0\n",
      "Step 329 (1946711) @ Episode 1308/2500, loss: 0.0071266959421336655\n",
      "Episode Reward: 2.0\n",
      "Step 552 (1947263) @ Episode 1309/2500, loss: 0.0112655982375144965\n",
      "Episode Reward: 4.0\n",
      "Step 271 (1947534) @ Episode 1310/2500, loss: 0.0437657907605171276\n",
      "Episode Reward: 2.0\n",
      "Step 494 (1948028) @ Episode 1311/2500, loss: 0.0057212277315557625\n",
      "Episode Reward: 7.0\n",
      "Step 459 (1948487) @ Episode 1312/2500, loss: 0.0041432119905948645\n",
      "Episode Reward: 5.0\n",
      "Step 439 (1948926) @ Episode 1313/2500, loss: 0.0062857870943844325\n",
      "Episode Reward: 5.0\n",
      "Step 380 (1949306) @ Episode 1314/2500, loss: 0.0055175693705677996\n",
      "Episode Reward: 4.0\n",
      "Step 339 (1949645) @ Episode 1315/2500, loss: 0.0151737760752439557\n",
      "Episode Reward: 3.0\n",
      "Step 382 (1950027) @ Episode 1316/2500, loss: 0.0039750104770064355\n",
      "Episode Reward: 3.0\n",
      "Step 254 (1950281) @ Episode 1317/2500, loss: 0.0094261039048433334\n",
      "Episode Reward: 1.0\n",
      "Step 557 (1950838) @ Episode 1318/2500, loss: 0.0093297846615314485\n",
      "Episode Reward: 5.0\n",
      "Step 449 (1951287) @ Episode 1319/2500, loss: 0.0626197010278701856\n",
      "Episode Reward: 5.0\n",
      "Step 336 (1951623) @ Episode 1320/2500, loss: 0.0084768664091825496\n",
      "Episode Reward: 3.0\n",
      "Step 501 (1952124) @ Episode 1321/2500, loss: 0.0189437456429004676\n",
      "Episode Reward: 6.0\n",
      "Step 370 (1952494) @ Episode 1322/2500, loss: 0.0098574515432119375\n",
      "Episode Reward: 4.0\n",
      "Step 295 (1952789) @ Episode 1323/2500, loss: 0.0154265109449625025\n",
      "Episode Reward: 3.0\n",
      "Step 479 (1953268) @ Episode 1324/2500, loss: 0.0060089565813541415\n",
      "Episode Reward: 6.0\n",
      "Step 422 (1953690) @ Episode 1325/2500, loss: 0.0063711917027831085\n",
      "Episode Reward: 5.0\n",
      "Step 426 (1954116) @ Episode 1326/2500, loss: 0.0822884961962699976\n",
      "Episode Reward: 5.0\n",
      "Step 448 (1954564) @ Episode 1327/2500, loss: 0.0031185166444629435\n",
      "Episode Reward: 5.0\n",
      "Step 423 (1954987) @ Episode 1328/2500, loss: 0.0040745204314589566\n",
      "Episode Reward: 5.0\n",
      "Step 654 (1955641) @ Episode 1329/2500, loss: 0.0111021203920245174\n",
      "Episode Reward: 10.0\n",
      "Step 448 (1956089) @ Episode 1330/2500, loss: 0.0072499001398682595\n",
      "Episode Reward: 6.0\n",
      "Step 401 (1956490) @ Episode 1331/2500, loss: 0.0157179869711399085\n",
      "Episode Reward: 5.0\n",
      "Step 719 (1957209) @ Episode 1332/2500, loss: 0.0093993209302425383\n",
      "Episode Reward: 6.0\n",
      "Step 500 (1957709) @ Episode 1333/2500, loss: 0.0052647572010755545\n",
      "Episode Reward: 5.0\n",
      "Step 269 (1957978) @ Episode 1334/2500, loss: 0.0114957615733146675\n",
      "Episode Reward: 2.0\n",
      "Step 262 (1958240) @ Episode 1335/2500, loss: 0.0936830639839172456\n",
      "Episode Reward: 2.0\n",
      "Step 391 (1958631) @ Episode 1336/2500, loss: 0.0101515557616949085\n",
      "Episode Reward: 4.0\n",
      "Step 289 (1958920) @ Episode 1337/2500, loss: 0.0225660260766744645\n",
      "Episode Reward: 2.0\n",
      "Step 367 (1959287) @ Episode 1338/2500, loss: 0.0106907188892364585\n",
      "Episode Reward: 4.0\n",
      "Step 620 (1959907) @ Episode 1339/2500, loss: 0.1029348373413086527\n",
      "Episode Reward: 7.0\n",
      "Step 364 (1960271) @ Episode 1340/2500, loss: 0.0029742333572357893\n",
      "Episode Reward: 4.0\n",
      "Step 406 (1960677) @ Episode 1341/2500, loss: 0.0224164091050624853\n",
      "Episode Reward: 5.0\n",
      "Step 286 (1960963) @ Episode 1342/2500, loss: 0.0067483251914381986\n",
      "Episode Reward: 2.0\n",
      "Step 370 (1961333) @ Episode 1343/2500, loss: 0.0099050886929035195\n",
      "Episode Reward: 4.0\n",
      "Step 404 (1961737) @ Episode 1344/2500, loss: 0.0192568991333246235\n",
      "Episode Reward: 5.0\n",
      "Step 296 (1962033) @ Episode 1345/2500, loss: 0.0118769407272338874\n",
      "Episode Reward: 2.0\n",
      "Step 310 (1962343) @ Episode 1346/2500, loss: 0.0091457888484001165\n",
      "Episode Reward: 2.0\n",
      "Step 426 (1962769) @ Episode 1347/2500, loss: 0.0088653229176998144\n",
      "Episode Reward: 4.0\n",
      "Step 239 (1963008) @ Episode 1348/2500, loss: 0.0398404002189636246\n",
      "Episode Reward: 2.0\n",
      "Step 417 (1963425) @ Episode 1349/2500, loss: 0.0057304976508021355\n",
      "Episode Reward: 5.0\n",
      "Step 473 (1963898) @ Episode 1350/2500, loss: 0.0133575079962611266\n",
      "Episode Reward: 6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 07:35:21,395] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video001350.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 336 (1964234) @ Episode 1351/2500, loss: 0.0125368591398000723\n",
      "Episode Reward: 3.0\n",
      "Step 483 (1964717) @ Episode 1352/2500, loss: 0.0062138927169144156\n",
      "Episode Reward: 6.0\n",
      "Step 508 (1965225) @ Episode 1353/2500, loss: 0.0125611443072557457\n",
      "Episode Reward: 6.0\n",
      "Step 481 (1965706) @ Episode 1354/2500, loss: 0.0103078829124569934\n",
      "Episode Reward: 6.0\n",
      "Step 474 (1966180) @ Episode 1355/2500, loss: 0.0094542102888226515\n",
      "Episode Reward: 6.0\n",
      "Step 504 (1966684) @ Episode 1356/2500, loss: 0.0182497613131999974\n",
      "Episode Reward: 7.0\n",
      "Step 445 (1967129) @ Episode 1357/2500, loss: 0.0057582454755902296\n",
      "Episode Reward: 5.0\n",
      "Step 436 (1967565) @ Episode 1358/2500, loss: 0.0086115859448909765\n",
      "Episode Reward: 5.0\n",
      "Step 516 (1968081) @ Episode 1359/2500, loss: 0.0141129232943058015\n",
      "Episode Reward: 6.0\n",
      "Step 655 (1968736) @ Episode 1360/2500, loss: 0.0063836015760898595\n",
      "Episode Reward: 9.0\n",
      "Step 303 (1969039) @ Episode 1361/2500, loss: 0.0058889822103083135\n",
      "Episode Reward: 3.0\n",
      "Step 284 (1969323) @ Episode 1362/2500, loss: 0.0034745279699563986\n",
      "Episode Reward: 3.0\n",
      "Step 345 (1969668) @ Episode 1363/2500, loss: 0.0045041991397738465\n",
      "Episode Reward: 3.0\n",
      "Step 497 (1970165) @ Episode 1364/2500, loss: 0.0174134578555822377\n",
      "Episode Reward: 7.0\n",
      "Step 340 (1970505) @ Episode 1365/2500, loss: 0.0069865044206380842\n",
      "Episode Reward: 3.0\n",
      "Step 403 (1970908) @ Episode 1366/2500, loss: 0.0114048570394515994\n",
      "Episode Reward: 5.0\n",
      "Step 447 (1971355) @ Episode 1367/2500, loss: 0.0115420697256922724\n",
      "Episode Reward: 6.0\n",
      "Step 388 (1971743) @ Episode 1368/2500, loss: 0.0134169394150376324\n",
      "Episode Reward: 4.0\n",
      "Step 474 (1972217) @ Episode 1369/2500, loss: 0.0131952976807951935\n",
      "Episode Reward: 6.0\n",
      "Step 411 (1972628) @ Episode 1370/2500, loss: 0.0353013575077056935\n",
      "Episode Reward: 4.0\n",
      "Step 354 (1972982) @ Episode 1371/2500, loss: 0.0183128267526626625\n",
      "Episode Reward: 4.0\n",
      "Step 341 (1973323) @ Episode 1372/2500, loss: 0.0182527154684066775\n",
      "Episode Reward: 2.0\n",
      "Step 346 (1973669) @ Episode 1373/2500, loss: 0.0073284790851175785\n",
      "Episode Reward: 4.0\n",
      "Step 413 (1974082) @ Episode 1374/2500, loss: 0.0059976885095238686\n",
      "Episode Reward: 5.0\n",
      "Step 323 (1974405) @ Episode 1375/2500, loss: 0.0063905054703354836\n",
      "Episode Reward: 3.0\n",
      "Step 279 (1974684) @ Episode 1376/2500, loss: 0.0191914830356836325\n",
      "Episode Reward: 2.0\n",
      "Step 463 (1975147) @ Episode 1377/2500, loss: 0.0255227573215961465\n",
      "Episode Reward: 5.0\n",
      "Step 392 (1975539) @ Episode 1378/2500, loss: 0.0154161173850297935\n",
      "Episode Reward: 5.0\n",
      "Step 419 (1975958) @ Episode 1379/2500, loss: 0.0144652109593153615\n",
      "Episode Reward: 5.0\n",
      "Step 389 (1976347) @ Episode 1380/2500, loss: 0.0183959882706403735\n",
      "Episode Reward: 5.0\n",
      "Step 409 (1976756) @ Episode 1381/2500, loss: 0.0056794043630361565\n",
      "Episode Reward: 5.0\n",
      "Step 421 (1977177) @ Episode 1382/2500, loss: 0.0073455534875392912\n",
      "Episode Reward: 5.0\n",
      "Step 421 (1977598) @ Episode 1383/2500, loss: 0.1258487403392791745\n",
      "Episode Reward: 5.0\n",
      "Step 442 (1978040) @ Episode 1384/2500, loss: 0.0134762180969119076\n",
      "Episode Reward: 5.0\n",
      "Step 539 (1978579) @ Episode 1385/2500, loss: 0.0285195372998714452\n",
      "Episode Reward: 7.0\n",
      "Step 498 (1979077) @ Episode 1386/2500, loss: 0.0037725602742284536\n",
      "Episode Reward: 6.0\n",
      "Step 628 (1979705) @ Episode 1387/2500, loss: 0.0341977365314960594\n",
      "Episode Reward: 10.0\n",
      "Step 397 (1980102) @ Episode 1388/2500, loss: 0.0100532583892345435\n",
      "Episode Reward: 5.0\n",
      "Step 341 (1980443) @ Episode 1389/2500, loss: 0.0163502246141433745\n",
      "Episode Reward: 4.0\n",
      "Step 478 (1980921) @ Episode 1390/2500, loss: 0.0069462982937693654\n",
      "Episode Reward: 4.0\n",
      "Step 370 (1981291) @ Episode 1391/2500, loss: 0.0937762856483459586\n",
      "Episode Reward: 3.0\n",
      "Step 325 (1981616) @ Episode 1392/2500, loss: 0.0922206044197082575\n",
      "Episode Reward: 3.0\n",
      "Step 356 (1981972) @ Episode 1393/2500, loss: 0.0044454783201217655\n",
      "Episode Reward: 3.0\n",
      "Step 389 (1982361) @ Episode 1394/2500, loss: 0.0085513778030872345\n",
      "Episode Reward: 4.0\n",
      "Step 409 (1982770) @ Episode 1395/2500, loss: 0.0065989028662443164\n",
      "Episode Reward: 5.0\n",
      "Step 371 (1983141) @ Episode 1396/2500, loss: 0.0350461825728416445\n",
      "Episode Reward: 4.0\n",
      "Step 400 (1983541) @ Episode 1397/2500, loss: 0.0054002376273274425\n",
      "Episode Reward: 5.0\n",
      "Step 285 (1983826) @ Episode 1398/2500, loss: 0.0042070318013429645\n",
      "Episode Reward: 3.0\n",
      "Step 349 (1984175) @ Episode 1399/2500, loss: 0.0083717387169599532\n",
      "Episode Reward: 4.0\n",
      "Step 243 (1984418) @ Episode 1400/2500, loss: 0.0048769987188279635\n",
      "Episode Reward: 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 08:14:47,754] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video001400.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 333 (1984751) @ Episode 1401/2500, loss: 0.1279128938913345346\n",
      "Episode Reward: 4.0\n",
      "Step 614 (1985365) @ Episode 1402/2500, loss: 0.0207343660295009635\n",
      "Episode Reward: 7.0\n",
      "Step 329 (1985694) @ Episode 1403/2500, loss: 0.0089115342125296685\n",
      "Episode Reward: 3.0\n",
      "Step 461 (1986155) @ Episode 1404/2500, loss: 0.0206845551729202274\n",
      "Episode Reward: 6.0\n",
      "Step 422 (1986577) @ Episode 1405/2500, loss: 0.0089012458920478824\n",
      "Episode Reward: 4.0\n",
      "Step 374 (1986951) @ Episode 1406/2500, loss: 0.0221487209200859075\n",
      "Episode Reward: 4.0\n",
      "Step 363 (1987314) @ Episode 1407/2500, loss: 0.0140751488506793985\n",
      "Episode Reward: 3.0\n",
      "Step 158 (1987472) @ Episode 1408/2500, loss: 0.0183463096618652345\n",
      "Episode Reward: 0.0\n",
      "Step 428 (1987900) @ Episode 1409/2500, loss: 0.0070365760475397115\n",
      "Episode Reward: 5.0\n",
      "Step 376 (1988276) @ Episode 1410/2500, loss: 0.0069151581265032295\n",
      "Episode Reward: 4.0\n",
      "Step 445 (1988721) @ Episode 1411/2500, loss: 0.0073304381221532826\n",
      "Episode Reward: 4.0\n",
      "Step 243 (1988964) @ Episode 1412/2500, loss: 0.0205247700214386712\n",
      "Episode Reward: 2.0\n",
      "Step 417 (1989381) @ Episode 1413/2500, loss: 0.0064021307043731215\n",
      "Episode Reward: 5.0\n",
      "Step 322 (1989703) @ Episode 1414/2500, loss: 0.0471386164426803635\n",
      "Episode Reward: 2.0\n",
      "Step 416 (1990119) @ Episode 1415/2500, loss: 0.0125583037734031687\n",
      "Episode Reward: 5.0\n",
      "Step 384 (1990503) @ Episode 1416/2500, loss: 0.0083913858979940415\n",
      "Episode Reward: 4.0\n",
      "Step 559 (1991062) @ Episode 1417/2500, loss: 0.0172940436750650425\n",
      "Episode Reward: 6.0\n",
      "Step 553 (1991615) @ Episode 1418/2500, loss: 0.0071079437620937824\n",
      "Episode Reward: 5.0\n",
      "Step 429 (1992044) @ Episode 1419/2500, loss: 0.0064372541382908826\n",
      "Episode Reward: 4.0\n",
      "Step 501 (1992545) @ Episode 1420/2500, loss: 0.0081135239452123645\n",
      "Episode Reward: 5.0\n",
      "Step 419 (1992964) @ Episode 1421/2500, loss: 0.0069067482836544515\n",
      "Episode Reward: 4.0\n",
      "Step 202 (1993166) @ Episode 1422/2500, loss: 0.0073338006623089315\n",
      "Episode Reward: 1.0\n",
      "Step 337 (1993503) @ Episode 1423/2500, loss: 0.0324505940079689895\n",
      "Episode Reward: 3.0\n",
      "Step 264 (1993767) @ Episode 1424/2500, loss: 0.0050137583166360855\n",
      "Episode Reward: 2.0\n",
      "Step 432 (1994199) @ Episode 1425/2500, loss: 0.0728208571672439664\n",
      "Episode Reward: 5.0\n",
      "Step 440 (1994639) @ Episode 1426/2500, loss: 0.0045323567464947785\n",
      "Episode Reward: 5.0\n",
      "Step 591 (1995230) @ Episode 1427/2500, loss: 0.0968450009822845523\n",
      "Episode Reward: 8.0\n",
      "Step 587 (1995817) @ Episode 1428/2500, loss: 0.0098796132951974878\n",
      "Episode Reward: 8.0\n",
      "Step 345 (1996162) @ Episode 1429/2500, loss: 0.0119973178952932364\n",
      "Episode Reward: 4.0\n",
      "Step 367 (1996529) @ Episode 1430/2500, loss: 0.0106470566242933273\n",
      "Episode Reward: 4.0\n",
      "Step 404 (1996933) @ Episode 1431/2500, loss: 0.0062530962750315675\n",
      "Episode Reward: 4.0\n",
      "Step 624 (1997557) @ Episode 1432/2500, loss: 0.0097598554566502575\n",
      "Episode Reward: 8.0\n",
      "Step 248 (1997805) @ Episode 1433/2500, loss: 0.0508176088333129947\n",
      "Episode Reward: 1.0\n",
      "Step 298 (1998103) @ Episode 1434/2500, loss: 0.0086421230807900433\n",
      "Episode Reward: 3.0\n",
      "Step 348 (1998451) @ Episode 1435/2500, loss: 0.0045037418603897095\n",
      "Episode Reward: 4.0\n",
      "Step 519 (1998970) @ Episode 1436/2500, loss: 0.0051647052168846132\n",
      "Episode Reward: 7.0\n",
      "Step 518 (1999488) @ Episode 1437/2500, loss: 0.0095553565770387655\n",
      "Episode Reward: 6.0\n",
      "Step 298 (1999786) @ Episode 1438/2500, loss: 0.0089088026434183125\n",
      "Episode Reward: 3.0\n",
      "Step 424 (2000210) @ Episode 1439/2500, loss: 0.0046256333589553837\n",
      "Episode Reward: 5.0\n",
      "Step 416 (2000626) @ Episode 1440/2500, loss: 0.0099806934595108033\n",
      "Episode Reward: 4.0\n",
      "Step 212 (2000838) @ Episode 1441/2500, loss: 0.0086584240198135387\n",
      "Episode Reward: 1.0\n",
      "Step 500 (2001338) @ Episode 1442/2500, loss: 0.0094117689877748495\n",
      "Episode Reward: 5.0\n",
      "Step 502 (2001840) @ Episode 1443/2500, loss: 0.0791080743074417126\n",
      "Episode Reward: 7.0\n",
      "Step 465 (2002305) @ Episode 1444/2500, loss: 0.0095566790550947195\n",
      "Episode Reward: 6.0\n",
      "Step 321 (2002626) @ Episode 1445/2500, loss: 0.0050549171864986425\n",
      "Episode Reward: 3.0\n",
      "Step 443 (2003069) @ Episode 1446/2500, loss: 0.0043832636438310155\n",
      "Episode Reward: 4.0\n",
      "Step 308 (2003377) @ Episode 1447/2500, loss: 0.0039640194736421115\n",
      "Episode Reward: 3.0\n",
      "Step 454 (2003831) @ Episode 1448/2500, loss: 0.0047823521308600935\n",
      "Episode Reward: 4.0\n",
      "Step 631 (2004462) @ Episode 1449/2500, loss: 0.0072441468946635726\n",
      "Episode Reward: 8.0\n",
      "Step 413 (2004875) @ Episode 1450/2500, loss: 0.0054998542182147545\n",
      "Episode Reward: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 08:53:39,562] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video001450.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 474 (2005349) @ Episode 1451/2500, loss: 0.1967827826738357584\n",
      "Episode Reward: 7.0\n",
      "Step 357 (2005706) @ Episode 1452/2500, loss: 0.0025430205278098583\n",
      "Episode Reward: 4.0\n",
      "Step 606 (2006312) @ Episode 1453/2500, loss: 0.0136544592678546965\n",
      "Episode Reward: 3.0\n",
      "Step 703 (2007015) @ Episode 1454/2500, loss: 0.0071528386324644094\n",
      "Episode Reward: 8.0\n",
      "Step 489 (2007504) @ Episode 1455/2500, loss: 0.0126610789448022845\n",
      "Episode Reward: 6.0\n",
      "Step 441 (2007945) @ Episode 1456/2500, loss: 0.0096893887966871265\n",
      "Episode Reward: 6.0\n",
      "Step 597 (2008542) @ Episode 1457/2500, loss: 0.0081329401582479485\n",
      "Episode Reward: 9.0\n",
      "Step 617 (2009159) @ Episode 1458/2500, loss: 0.0087370788678526883\n",
      "Episode Reward: 8.0\n",
      "Step 300 (2009459) @ Episode 1459/2500, loss: 0.0049390513449907354\n",
      "Episode Reward: 3.0\n",
      "Step 564 (2010023) @ Episode 1460/2500, loss: 0.0052282353863120085\n",
      "Episode Reward: 5.0\n",
      "Step 269 (2010292) @ Episode 1461/2500, loss: 0.0351530089974403443\n",
      "Episode Reward: 2.0\n",
      "Step 228 (2010520) @ Episode 1462/2500, loss: 0.0352743752300739395\n",
      "Episode Reward: 1.0\n",
      "Step 457 (2010977) @ Episode 1463/2500, loss: 0.0082420008257031443\n",
      "Episode Reward: 3.0\n",
      "Step 457 (2011434) @ Episode 1464/2500, loss: 0.0046722870320081715\n",
      "Episode Reward: 6.0\n",
      "Step 559 (2011993) @ Episode 1465/2500, loss: 0.0225570481270551683\n",
      "Episode Reward: 6.0\n",
      "Step 701 (2012694) @ Episode 1466/2500, loss: 0.0140268150717020034\n",
      "Episode Reward: 9.0\n",
      "Step 564 (2013258) @ Episode 1467/2500, loss: 0.0084688039496541023\n",
      "Episode Reward: 7.0\n",
      "Step 377 (2013635) @ Episode 1468/2500, loss: 0.0372275263071060285\n",
      "Episode Reward: 5.0\n",
      "Step 413 (2014048) @ Episode 1469/2500, loss: 0.0067559909075498585\n",
      "Episode Reward: 4.0\n",
      "Step 326 (2014374) @ Episode 1470/2500, loss: 0.0114307394251227385\n",
      "Episode Reward: 4.0\n",
      "Step 391 (2014765) @ Episode 1471/2500, loss: 0.0143001070246100435\n",
      "Episode Reward: 4.0\n",
      "Step 436 (2015201) @ Episode 1472/2500, loss: 0.0113417394459247595\n",
      "Episode Reward: 5.0\n",
      "Step 307 (2015508) @ Episode 1473/2500, loss: 0.0041295876726508146\n",
      "Episode Reward: 3.0\n",
      "Step 356 (2015864) @ Episode 1474/2500, loss: 0.0048593492247164254\n",
      "Episode Reward: 4.0\n",
      "Step 401 (2016265) @ Episode 1475/2500, loss: 0.0378671288490295476\n",
      "Episode Reward: 4.0\n",
      "Step 467 (2016732) @ Episode 1476/2500, loss: 0.0039285733364522464\n",
      "Episode Reward: 6.0\n",
      "Step 416 (2017148) @ Episode 1477/2500, loss: 0.0187841150909662255\n",
      "Episode Reward: 4.0\n",
      "Step 684 (2017832) @ Episode 1478/2500, loss: 0.0045860451646149165\n",
      "Episode Reward: 10.0\n",
      "Step 530 (2018362) @ Episode 1479/2500, loss: 0.0085824737325310755\n",
      "Episode Reward: 7.0\n",
      "Step 225 (2018587) @ Episode 1480/2500, loss: 0.0713159739971160915\n",
      "Episode Reward: 1.0\n",
      "Step 439 (2019026) @ Episode 1481/2500, loss: 0.0397617109119892185\n",
      "Episode Reward: 6.0\n",
      "Step 429 (2019455) @ Episode 1482/2500, loss: 0.0090364953503012665\n",
      "Episode Reward: 5.0\n",
      "Step 314 (2019769) @ Episode 1483/2500, loss: 0.0096013266593217854\n",
      "Episode Reward: 3.0\n",
      "Step 344 (2020113) @ Episode 1484/2500, loss: 0.0173936747014522553\n",
      "Episode Reward: 3.0\n",
      "Step 439 (2020552) @ Episode 1485/2500, loss: 0.0101813469082117086\n",
      "Episode Reward: 5.0\n",
      "Step 330 (2020882) @ Episode 1486/2500, loss: 0.0086592324078083045\n",
      "Episode Reward: 3.0\n",
      "Step 443 (2021325) @ Episode 1487/2500, loss: 0.0176847428083419877\n",
      "Episode Reward: 4.0\n",
      "Step 678 (2022003) @ Episode 1488/2500, loss: 0.0125367054715752626\n",
      "Episode Reward: 12.0\n",
      "Step 344 (2022347) @ Episode 1489/2500, loss: 0.0075631220825016546\n",
      "Episode Reward: 3.0\n",
      "Step 358 (2022705) @ Episode 1490/2500, loss: 0.0073772128671407748\n",
      "Episode Reward: 4.0\n",
      "Step 384 (2023089) @ Episode 1491/2500, loss: 0.0054027941077947625\n",
      "Episode Reward: 4.0\n",
      "Step 328 (2023417) @ Episode 1492/2500, loss: 0.0037006509955972433\n",
      "Episode Reward: 3.0\n",
      "Step 513 (2023930) @ Episode 1493/2500, loss: 0.0195635929703712464\n",
      "Episode Reward: 8.0\n",
      "Step 601 (2024531) @ Episode 1494/2500, loss: 0.0075907995924353635\n",
      "Episode Reward: 8.0\n",
      "Step 514 (2025045) @ Episode 1495/2500, loss: 0.0227141957730054866\n",
      "Episode Reward: 6.0\n",
      "Step 291 (2025336) @ Episode 1496/2500, loss: 0.2039382904767997264\n",
      "Episode Reward: 2.0\n",
      "Step 545 (2025881) @ Episode 1497/2500, loss: 0.0077071599662303925\n",
      "Episode Reward: 7.0\n",
      "Step 487 (2026368) @ Episode 1498/2500, loss: 0.1492638885974884796\n",
      "Episode Reward: 6.0\n",
      "Step 490 (2026858) @ Episode 1499/2500, loss: 0.0063692722469568253\n",
      "Episode Reward: 6.0\n",
      "Step 757 (2027615) @ Episode 1500/2500, loss: 0.0237766373902559284\n",
      "Episode Reward: 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 09:37:13,602] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video001500.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 438 (2028053) @ Episode 1501/2500, loss: 0.0096332821995019914\n",
      "Episode Reward: 5.0\n",
      "Step 443 (2028496) @ Episode 1502/2500, loss: 0.0053360732272267343\n",
      "Episode Reward: 6.0\n",
      "Step 458 (2028954) @ Episode 1503/2500, loss: 0.0047627948224544525\n",
      "Episode Reward: 6.0\n",
      "Step 448 (2029402) @ Episode 1504/2500, loss: 0.0054695331491529948\n",
      "Episode Reward: 5.0\n",
      "Step 594 (2029996) @ Episode 1505/2500, loss: 0.0156287252902984625\n",
      "Episode Reward: 12.0\n",
      "Step 249 (2030245) @ Episode 1506/2500, loss: 0.0040607936680316925\n",
      "Episode Reward: 2.0\n",
      "Step 444 (2030689) @ Episode 1507/2500, loss: 0.0125881507992744457\n",
      "Episode Reward: 6.0\n",
      "Step 589 (2031278) @ Episode 1508/2500, loss: 0.0122718941420316785\n",
      "Episode Reward: 7.0\n",
      "Step 430 (2031708) @ Episode 1509/2500, loss: 0.0050064520910382274\n",
      "Episode Reward: 4.0\n",
      "Step 401 (2032109) @ Episode 1510/2500, loss: 0.0074809072539210322\n",
      "Episode Reward: 4.0\n",
      "Step 269 (2032378) @ Episode 1511/2500, loss: 0.0119973132386803636\n",
      "Episode Reward: 2.0\n",
      "Step 290 (2032668) @ Episode 1512/2500, loss: 0.0053686019964516165\n",
      "Episode Reward: 2.0\n",
      "Step 450 (2033118) @ Episode 1513/2500, loss: 0.0042309407144784935\n",
      "Episode Reward: 6.0\n",
      "Step 339 (2033457) @ Episode 1514/2500, loss: 0.0066834837198257455\n",
      "Episode Reward: 3.0\n",
      "Step 530 (2033987) @ Episode 1515/2500, loss: 0.0095381429418921475\n",
      "Episode Reward: 7.0\n",
      "Step 670 (2034657) @ Episode 1516/2500, loss: 0.0055120550096035353\n",
      "Episode Reward: 6.0\n",
      "Step 417 (2035074) @ Episode 1517/2500, loss: 0.0123548777773976334\n",
      "Episode Reward: 5.0\n",
      "Step 685 (2035759) @ Episode 1518/2500, loss: 0.0277668647468090065\n",
      "Episode Reward: 5.0\n",
      "Step 333 (2036092) @ Episode 1519/2500, loss: 0.0307926125824451455\n",
      "Episode Reward: 3.0\n",
      "Step 362 (2036454) @ Episode 1520/2500, loss: 0.0049771121703088284\n",
      "Episode Reward: 3.0\n",
      "Step 543 (2036997) @ Episode 1521/2500, loss: 0.0061869644559919837\n",
      "Episode Reward: 7.0\n",
      "Step 351 (2037348) @ Episode 1522/2500, loss: 0.0074062766507267955\n",
      "Episode Reward: 4.0\n",
      "Step 333 (2037681) @ Episode 1523/2500, loss: 0.0055953934788703925\n",
      "Episode Reward: 3.0\n",
      "Step 675 (2038356) @ Episode 1524/2500, loss: 0.0088135749101638825\n",
      "Episode Reward: 7.0\n",
      "Step 427 (2038783) @ Episode 1525/2500, loss: 0.0085281804203987125\n",
      "Episode Reward: 3.0\n",
      "Step 535 (2039318) @ Episode 1526/2500, loss: 0.0133313909173011784\n",
      "Episode Reward: 7.0\n",
      "Step 369 (2039687) @ Episode 1527/2500, loss: 0.0048943376168608665\n",
      "Episode Reward: 3.0\n",
      "Step 448 (2040135) @ Episode 1528/2500, loss: 0.0061893677338957795\n",
      "Episode Reward: 5.0\n",
      "Step 459 (2040594) @ Episode 1529/2500, loss: 0.0084309838712215425\n",
      "Episode Reward: 5.0\n",
      "Step 498 (2041092) @ Episode 1530/2500, loss: 0.0079960804432630543\n",
      "Episode Reward: 7.0\n",
      "Step 498 (2041590) @ Episode 1531/2500, loss: 0.0137669267132878387\n",
      "Episode Reward: 5.0\n",
      "Step 394 (2041984) @ Episode 1532/2500, loss: 0.0112319495528936395\n",
      "Episode Reward: 4.0\n",
      "Step 498 (2042482) @ Episode 1533/2500, loss: 0.0619358345866203387\n",
      "Episode Reward: 6.0\n",
      "Step 437 (2042919) @ Episode 1534/2500, loss: 0.0105666099116206176\n",
      "Episode Reward: 4.0\n",
      "Step 460 (2043379) @ Episode 1535/2500, loss: 0.0042758774943649775\n",
      "Episode Reward: 4.0\n",
      "Step 452 (2043831) @ Episode 1536/2500, loss: 0.0087981959804892545\n",
      "Episode Reward: 5.0\n",
      "Step 355 (2044186) @ Episode 1537/2500, loss: 0.0047179097309708595\n",
      "Episode Reward: 4.0\n",
      "Step 345 (2044531) @ Episode 1538/2500, loss: 0.0252376515418291173\n",
      "Episode Reward: 4.0\n",
      "Step 624 (2045155) @ Episode 1539/2500, loss: 0.0114726340398192485\n",
      "Episode Reward: 8.0\n",
      "Step 384 (2045539) @ Episode 1540/2500, loss: 0.0129384007304906855\n",
      "Episode Reward: 5.0\n",
      "Step 286 (2045825) @ Episode 1541/2500, loss: 0.0163916070014238365\n",
      "Episode Reward: 3.0\n",
      "Step 317 (2046142) @ Episode 1542/2500, loss: 0.0061745150014758114\n",
      "Episode Reward: 3.0\n",
      "Step 354 (2046496) @ Episode 1543/2500, loss: 0.0130316130816936555\n",
      "Episode Reward: 2.0\n",
      "Step 539 (2047035) @ Episode 1544/2500, loss: 0.0042041153647005563\n",
      "Episode Reward: 6.0\n",
      "Step 483 (2047518) @ Episode 1545/2500, loss: 0.0098537672311067586\n",
      "Episode Reward: 5.0\n",
      "Step 440 (2047958) @ Episode 1546/2500, loss: 0.0157074499875307197\n",
      "Episode Reward: 5.0\n",
      "Step 433 (2048391) @ Episode 1547/2500, loss: 0.0122603308409452444\n",
      "Episode Reward: 5.0\n",
      "Step 565 (2048956) @ Episode 1548/2500, loss: 0.0069548189640045175\n",
      "Episode Reward: 5.0\n",
      "Step 316 (2049272) @ Episode 1549/2500, loss: 0.0085928654298186315\n",
      "Episode Reward: 3.0\n",
      "Step 506 (2049778) @ Episode 1550/2500, loss: 0.0080095669254660666\n",
      "Episode Reward: 6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 10:18:58,980] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video001550.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 389 (2050167) @ Episode 1551/2500, loss: 0.0061094155535101894\n",
      "Episode Reward: 4.0\n",
      "Step 347 (2050514) @ Episode 1552/2500, loss: 0.0522419326007366275\n",
      "Episode Reward: 3.0\n",
      "Step 356 (2050870) @ Episode 1553/2500, loss: 0.0048052528873085976\n",
      "Episode Reward: 3.0\n",
      "Step 376 (2051246) @ Episode 1554/2500, loss: 0.0140055073425173765\n",
      "Episode Reward: 4.0\n",
      "Step 394 (2051640) @ Episode 1555/2500, loss: 0.0074644973501563075\n",
      "Episode Reward: 4.0\n",
      "Step 415 (2052055) @ Episode 1556/2500, loss: 0.0037995188031345606\n",
      "Episode Reward: 5.0\n",
      "Step 671 (2052726) @ Episode 1557/2500, loss: 0.0126656387001276025\n",
      "Episode Reward: 9.0\n",
      "Step 649 (2053375) @ Episode 1558/2500, loss: 0.0144689492881298075\n",
      "Episode Reward: 7.0\n",
      "Step 435 (2053810) @ Episode 1559/2500, loss: 0.0171862002462148674\n",
      "Episode Reward: 5.0\n",
      "Step 425 (2054235) @ Episode 1560/2500, loss: 0.0060940971598029145\n",
      "Episode Reward: 4.0\n",
      "Step 323 (2054558) @ Episode 1561/2500, loss: 0.0194088444113731414\n",
      "Episode Reward: 3.0\n",
      "Step 533 (2055091) @ Episode 1562/2500, loss: 0.0057410267181694513\n",
      "Episode Reward: 6.0\n",
      "Step 486 (2055577) @ Episode 1563/2500, loss: 0.0059297289699316025\n",
      "Episode Reward: 5.0\n",
      "Step 408 (2055985) @ Episode 1564/2500, loss: 0.0057298489846289166\n",
      "Episode Reward: 5.0\n",
      "Step 273 (2056258) @ Episode 1565/2500, loss: 0.0084514431655406953\n",
      "Episode Reward: 2.0\n",
      "Step 505 (2056763) @ Episode 1566/2500, loss: 0.0066789779812097554\n",
      "Episode Reward: 4.0\n",
      "Step 485 (2057248) @ Episode 1567/2500, loss: 0.0067918607965111737\n",
      "Episode Reward: 4.0\n",
      "Step 473 (2057721) @ Episode 1568/2500, loss: 0.0222434327006340035\n",
      "Episode Reward: 5.0\n",
      "Step 343 (2058064) @ Episode 1569/2500, loss: 0.0094597730785608305\n",
      "Episode Reward: 3.0\n",
      "Step 434 (2058498) @ Episode 1570/2500, loss: 0.0233107656240463265\n",
      "Episode Reward: 5.0\n",
      "Step 273 (2058771) @ Episode 1571/2500, loss: 0.0093387141823768625\n",
      "Episode Reward: 2.0\n",
      "Step 618 (2059389) @ Episode 1572/2500, loss: 0.0088881505653262146\n",
      "Episode Reward: 12.0\n",
      "Step 294 (2059683) @ Episode 1573/2500, loss: 0.0149102574214339265\n",
      "Episode Reward: 2.0\n",
      "Step 230 (2059913) @ Episode 1574/2500, loss: 0.0050587919540703365\n",
      "Episode Reward: 1.0\n",
      "Step 278 (2060191) @ Episode 1575/2500, loss: 0.0107923932373523717\n",
      "Episode Reward: 1.0\n",
      "Step 366 (2060557) @ Episode 1576/2500, loss: 0.0055695767514407635\n",
      "Episode Reward: 3.0\n",
      "Step 466 (2061023) @ Episode 1577/2500, loss: 0.2024744004011154215\n",
      "Episode Reward: 5.0\n",
      "Step 580 (2061603) @ Episode 1578/2500, loss: 0.0119089260697364875\n",
      "Episode Reward: 6.0\n",
      "Step 406 (2062009) @ Episode 1579/2500, loss: 0.0312661714851856235\n",
      "Episode Reward: 5.0\n",
      "Step 372 (2062381) @ Episode 1580/2500, loss: 0.0067230276763439185\n",
      "Episode Reward: 4.0\n",
      "Step 364 (2062745) @ Episode 1581/2500, loss: 0.0128558371216058737\n",
      "Episode Reward: 4.0\n",
      "Step 432 (2063177) @ Episode 1582/2500, loss: 0.0067547163926064976\n",
      "Episode Reward: 4.0\n",
      "Step 428 (2063605) @ Episode 1583/2500, loss: 0.0063901022076606754\n",
      "Episode Reward: 5.0\n",
      "Step 334 (2063939) @ Episode 1584/2500, loss: 0.0088114403188228645\n",
      "Episode Reward: 2.0\n",
      "Step 506 (2064445) @ Episode 1585/2500, loss: 0.0092703271657228475\n",
      "Episode Reward: 4.0\n",
      "Step 506 (2064951) @ Episode 1586/2500, loss: 0.0081515619531273845\n",
      "Episode Reward: 5.0\n",
      "Step 474 (2065425) @ Episode 1587/2500, loss: 0.0151340048760175776\n",
      "Episode Reward: 6.0\n",
      "Step 398 (2065823) @ Episode 1588/2500, loss: 0.0052226823754608636\n",
      "Episode Reward: 4.0\n",
      "Step 388 (2066211) @ Episode 1589/2500, loss: 0.0327146984636783685\n",
      "Episode Reward: 4.0\n",
      "Step 741 (2066952) @ Episode 1590/2500, loss: 0.0143208317458629635\n",
      "Episode Reward: 15.0\n",
      "Step 525 (2067477) @ Episode 1591/2500, loss: 0.0062922146171331406\n",
      "Episode Reward: 6.0\n",
      "Step 372 (2067849) @ Episode 1592/2500, loss: 0.0079204803332686424\n",
      "Episode Reward: 4.0\n",
      "Step 277 (2068126) @ Episode 1593/2500, loss: 0.0056990692391991615\n",
      "Episode Reward: 1.0\n",
      "Step 461 (2068587) @ Episode 1594/2500, loss: 0.0076060369610786446\n",
      "Episode Reward: 5.0\n",
      "Step 299 (2068886) @ Episode 1595/2500, loss: 0.0760280787944793775\n",
      "Episode Reward: 2.0\n",
      "Step 309 (2069195) @ Episode 1596/2500, loss: 0.0072124963626265535\n",
      "Episode Reward: 3.0\n",
      "Step 368 (2069563) @ Episode 1597/2500, loss: 0.0128691354766488086\n",
      "Episode Reward: 4.0\n",
      "Step 352 (2069915) @ Episode 1598/2500, loss: 0.0164840370416641245\n",
      "Episode Reward: 3.0\n",
      "Step 328 (2070243) @ Episode 1599/2500, loss: 0.1090355515480041575\n",
      "Episode Reward: 3.0\n",
      "Step 461 (2070704) @ Episode 1600/2500, loss: 0.0058425962924957275\n",
      "Episode Reward: 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 10:59:24,447] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video001600.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 332 (2071036) @ Episode 1601/2500, loss: 0.0037530905101448298\n",
      "Episode Reward: 3.0\n",
      "Step 587 (2071623) @ Episode 1602/2500, loss: 0.0056184465065598495\n",
      "Episode Reward: 8.0\n",
      "Step 465 (2072088) @ Episode 1603/2500, loss: 0.0816657692193985025\n",
      "Episode Reward: 5.0\n",
      "Step 708 (2072796) @ Episode 1604/2500, loss: 0.0063740410842001445\n",
      "Episode Reward: 14.0\n",
      "Step 332 (2073128) @ Episode 1605/2500, loss: 0.0073426575399935246\n",
      "Episode Reward: 3.0\n",
      "Step 404 (2073532) @ Episode 1606/2500, loss: 0.0171864405274391175\n",
      "Episode Reward: 5.0\n",
      "Step 411 (2073943) @ Episode 1607/2500, loss: 0.0102032292634248735\n",
      "Episode Reward: 4.0\n",
      "Step 490 (2074433) @ Episode 1608/2500, loss: 0.2726681530475616514\n",
      "Episode Reward: 7.0\n",
      "Step 536 (2074969) @ Episode 1609/2500, loss: 0.0136036826297640865\n",
      "Episode Reward: 8.0\n",
      "Step 498 (2075467) @ Episode 1610/2500, loss: 0.0086755212396383295\n",
      "Episode Reward: 5.0\n",
      "Step 271 (2075738) @ Episode 1611/2500, loss: 0.0752328038215637237\n",
      "Episode Reward: 2.0\n",
      "Step 381 (2076119) @ Episode 1612/2500, loss: 0.0187322348356246954\n",
      "Episode Reward: 3.0\n",
      "Step 318 (2076437) @ Episode 1613/2500, loss: 0.0304281115531921445\n",
      "Episode Reward: 3.0\n",
      "Step 503 (2076940) @ Episode 1614/2500, loss: 0.0084966989234089856\n",
      "Episode Reward: 5.0\n",
      "Step 564 (2077504) @ Episode 1615/2500, loss: 0.0085671171545982368\n",
      "Episode Reward: 7.0\n",
      "Step 536 (2078040) @ Episode 1616/2500, loss: 0.0197135806083679265\n",
      "Episode Reward: 4.0\n",
      "Step 416 (2078456) @ Episode 1617/2500, loss: 0.0187715664505958567\n",
      "Episode Reward: 4.0\n",
      "Step 233 (2078689) @ Episode 1618/2500, loss: 0.0130661092698574076\n",
      "Episode Reward: 1.0\n",
      "Step 713 (2079402) @ Episode 1619/2500, loss: 0.0103045422583818444\n",
      "Episode Reward: 6.0\n",
      "Step 471 (2079873) @ Episode 1620/2500, loss: 0.0176680870354175575\n",
      "Episode Reward: 5.0\n",
      "Step 367 (2080240) @ Episode 1621/2500, loss: 0.0082761310040950785\n",
      "Episode Reward: 4.0\n",
      "Step 506 (2080746) @ Episode 1622/2500, loss: 0.0124845653772354133\n",
      "Episode Reward: 6.0\n",
      "Step 451 (2081197) @ Episode 1623/2500, loss: 0.0124951861798763285\n",
      "Episode Reward: 4.0\n",
      "Step 484 (2081681) @ Episode 1624/2500, loss: 0.0029734941199421883\n",
      "Episode Reward: 5.0\n",
      "Step 360 (2082041) @ Episode 1625/2500, loss: 0.0088292164728045465\n",
      "Episode Reward: 2.0\n",
      "Step 374 (2082415) @ Episode 1626/2500, loss: 0.0092764310538768775\n",
      "Episode Reward: 4.0\n",
      "Step 424 (2082839) @ Episode 1627/2500, loss: 0.0137209631502628336\n",
      "Episode Reward: 4.0\n",
      "Step 381 (2083220) @ Episode 1628/2500, loss: 0.0116340452805161488\n",
      "Episode Reward: 2.0\n",
      "Step 450 (2083670) @ Episode 1629/2500, loss: 0.0085830576717853556\n",
      "Episode Reward: 5.0\n",
      "Step 315 (2083985) @ Episode 1630/2500, loss: 0.0068171070888638555\n",
      "Episode Reward: 3.0\n",
      "Step 551 (2084536) @ Episode 1631/2500, loss: 0.0907154306769371165\n",
      "Episode Reward: 6.0\n",
      "Step 523 (2085059) @ Episode 1632/2500, loss: 0.0054080123081803325\n",
      "Episode Reward: 6.0\n",
      "Step 455 (2085514) @ Episode 1633/2500, loss: 0.0082853697240352635\n",
      "Episode Reward: 5.0\n",
      "Step 649 (2086163) @ Episode 1634/2500, loss: 0.0071966052055358895\n",
      "Episode Reward: 7.0\n",
      "Step 332 (2086495) @ Episode 1635/2500, loss: 0.0079576643183827415\n",
      "Episode Reward: 3.0\n",
      "Step 463 (2086958) @ Episode 1636/2500, loss: 0.0663295090198516854\n",
      "Episode Reward: 5.0\n",
      "Step 408 (2087366) @ Episode 1637/2500, loss: 0.0055770725011825567\n",
      "Episode Reward: 4.0\n",
      "Step 369 (2087735) @ Episode 1638/2500, loss: 0.0100244469940662385\n",
      "Episode Reward: 3.0\n",
      "Step 548 (2088283) @ Episode 1639/2500, loss: 0.0415028855204582255\n",
      "Episode Reward: 10.0\n",
      "Step 315 (2088598) @ Episode 1640/2500, loss: 0.0083247171714901924\n",
      "Episode Reward: 2.0\n",
      "Step 381 (2088979) @ Episode 1641/2500, loss: 0.0152495615184307136\n",
      "Episode Reward: 3.0\n",
      "Step 438 (2089417) @ Episode 1642/2500, loss: 0.0080183297395706185\n",
      "Episode Reward: 5.0\n",
      "Step 441 (2089858) @ Episode 1643/2500, loss: 0.0080625656992197046\n",
      "Episode Reward: 5.0\n",
      "Step 482 (2090340) @ Episode 1644/2500, loss: 0.0072685927152633675\n",
      "Episode Reward: 4.0\n",
      "Step 370 (2090710) @ Episode 1645/2500, loss: 0.0112362112849950798\n",
      "Episode Reward: 4.0\n",
      "Step 494 (2091204) @ Episode 1646/2500, loss: 0.0115211326628923426\n",
      "Episode Reward: 6.0\n",
      "Step 539 (2091743) @ Episode 1647/2500, loss: 0.0083648720756173134\n",
      "Episode Reward: 6.0\n",
      "Step 345 (2092088) @ Episode 1648/2500, loss: 0.0190326459705829625\n",
      "Episode Reward: 3.0\n",
      "Step 473 (2092561) @ Episode 1649/2500, loss: 0.0061510885134339335\n",
      "Episode Reward: 5.0\n",
      "Step 576 (2093137) @ Episode 1650/2500, loss: 0.0052437414415180685\n",
      "Episode Reward: 7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 11:42:38,155] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video001650.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 361 (2093498) @ Episode 1651/2500, loss: 0.0134387342259287834\n",
      "Episode Reward: 4.0\n",
      "Step 396 (2093894) @ Episode 1652/2500, loss: 0.0591756477952003587\n",
      "Episode Reward: 4.0\n",
      "Step 417 (2094311) @ Episode 1653/2500, loss: 0.0086455317214131365\n",
      "Episode Reward: 4.0\n",
      "Step 467 (2094778) @ Episode 1654/2500, loss: 0.0147455008700491995\n",
      "Episode Reward: 6.0\n",
      "Step 604 (2095382) @ Episode 1655/2500, loss: 0.0069547742605209355\n",
      "Episode Reward: 5.0\n",
      "Step 499 (2095881) @ Episode 1656/2500, loss: 0.0092813856899738315\n",
      "Episode Reward: 3.0\n",
      "Step 326 (2096207) @ Episode 1657/2500, loss: 0.0080213975161314015\n",
      "Episode Reward: 1.0\n",
      "Step 391 (2096598) @ Episode 1658/2500, loss: 0.0129167232662439357\n",
      "Episode Reward: 4.0\n",
      "Step 242 (2096840) @ Episode 1659/2500, loss: 0.0065993443131446845\n",
      "Episode Reward: 1.0\n",
      "Step 504 (2097344) @ Episode 1660/2500, loss: 0.0114176161587238317\n",
      "Episode Reward: 6.0\n",
      "Step 343 (2097687) @ Episode 1661/2500, loss: 0.0061218515038490295\n",
      "Episode Reward: 3.0\n",
      "Step 677 (2098364) @ Episode 1662/2500, loss: 0.0216692015528678975\n",
      "Episode Reward: 12.0\n",
      "Step 362 (2098726) @ Episode 1663/2500, loss: 0.0065754661336541184\n",
      "Episode Reward: 3.0\n",
      "Step 278 (2099004) @ Episode 1664/2500, loss: 0.0349943041801452642\n",
      "Episode Reward: 2.0\n",
      "Step 433 (2099437) @ Episode 1665/2500, loss: 0.0079811485484242445\n",
      "Episode Reward: 4.0\n",
      "Step 532 (2099969) @ Episode 1666/2500, loss: 0.0067072575911879545\n",
      "Episode Reward: 5.0\n",
      "Step 352 (2100321) @ Episode 1667/2500, loss: 0.0098005020990967755\n",
      "Episode Reward: 4.0\n",
      "Step 369 (2100690) @ Episode 1668/2500, loss: 0.0051596891134977346\n",
      "Episode Reward: 3.0\n",
      "Step 216 (2100906) @ Episode 1669/2500, loss: 0.0203084256500005725\n",
      "Episode Reward: 1.0\n",
      "Step 334 (2101240) @ Episode 1670/2500, loss: 0.0074872681871056565\n",
      "Episode Reward: 3.0\n",
      "Step 287 (2101527) @ Episode 1671/2500, loss: 0.0037939671892672777\n",
      "Episode Reward: 2.0\n",
      "Step 444 (2101971) @ Episode 1672/2500, loss: 0.0038934282492846255\n",
      "Episode Reward: 6.0\n",
      "Step 383 (2102354) @ Episode 1673/2500, loss: 0.0067621050402522095\n",
      "Episode Reward: 2.0\n",
      "Step 516 (2102870) @ Episode 1674/2500, loss: 0.0085584074258804326\n",
      "Episode Reward: 6.0\n",
      "Step 411 (2103281) @ Episode 1675/2500, loss: 0.0069912970066070564\n",
      "Episode Reward: 3.0\n",
      "Step 361 (2103642) @ Episode 1676/2500, loss: 0.0141236092895269495\n",
      "Episode Reward: 2.0\n",
      "Step 417 (2104059) @ Episode 1677/2500, loss: 0.0528797991573810625\n",
      "Episode Reward: 4.0\n",
      "Step 494 (2104553) @ Episode 1678/2500, loss: 0.0067336154170334345\n",
      "Episode Reward: 5.0\n",
      "Step 522 (2105075) @ Episode 1679/2500, loss: 0.0072050085291266445\n",
      "Episode Reward: 9.0\n",
      "Step 402 (2105477) @ Episode 1680/2500, loss: 0.0107741272076964385\n",
      "Episode Reward: 4.0\n",
      "Step 422 (2105899) @ Episode 1681/2500, loss: 0.0052306745201349265\n",
      "Episode Reward: 4.0\n",
      "Step 533 (2106432) @ Episode 1682/2500, loss: 0.0112506635487079625\n",
      "Episode Reward: 6.0\n",
      "Step 559 (2106991) @ Episode 1683/2500, loss: 0.0638476237654686707\n",
      "Episode Reward: 6.0\n",
      "Step 453 (2107444) @ Episode 1684/2500, loss: 0.0071867331862449655\n",
      "Episode Reward: 5.0\n",
      "Step 244 (2107688) @ Episode 1685/2500, loss: 0.0218121111392974854\n",
      "Episode Reward: 2.0\n",
      "Step 373 (2108061) @ Episode 1686/2500, loss: 0.0100131351500749595\n",
      "Episode Reward: 3.0\n",
      "Step 427 (2108488) @ Episode 1687/2500, loss: 0.0091467071324586875\n",
      "Episode Reward: 4.0\n",
      "Step 328 (2108816) @ Episode 1688/2500, loss: 0.0068542221561074265\n",
      "Episode Reward: 3.0\n",
      "Step 462 (2109278) @ Episode 1689/2500, loss: 0.0914304107427597265\n",
      "Episode Reward: 5.0\n",
      "Step 353 (2109631) @ Episode 1690/2500, loss: 0.0099583128467202198\n",
      "Episode Reward: 3.0\n",
      "Step 354 (2109985) @ Episode 1691/2500, loss: 0.0138826211914420135\n",
      "Episode Reward: 4.0\n",
      "Step 454 (2110439) @ Episode 1692/2500, loss: 0.0091560371220111855\n",
      "Episode Reward: 4.0\n",
      "Step 487 (2110926) @ Episode 1693/2500, loss: 0.0081605529412627225\n",
      "Episode Reward: 6.0\n",
      "Step 439 (2111365) @ Episode 1694/2500, loss: 0.0978481695055961675\n",
      "Episode Reward: 5.0\n",
      "Step 390 (2111755) @ Episode 1695/2500, loss: 0.1794441938400268685\n",
      "Episode Reward: 4.0\n",
      "Step 778 (2112533) @ Episode 1696/2500, loss: 0.0086583010852336885\n",
      "Episode Reward: 11.0\n",
      "Step 279 (2112812) @ Episode 1697/2500, loss: 0.0066692740656435495\n",
      "Episode Reward: 3.0\n",
      "Step 280 (2113092) @ Episode 1698/2500, loss: 0.0105261597782373437\n",
      "Episode Reward: 2.0\n",
      "Step 460 (2113552) @ Episode 1699/2500, loss: 0.0070123719051480293\n",
      "Episode Reward: 6.0\n",
      "Step 412 (2113964) @ Episode 1700/2500, loss: 0.0135202314704656625\n",
      "Episode Reward: 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 12:22:08,831] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video001700.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 328 (2114292) @ Episode 1701/2500, loss: 0.0092379916459321985\n",
      "Episode Reward: 3.0\n",
      "Step 353 (2114645) @ Episode 1702/2500, loss: 0.0069830538704991345\n",
      "Episode Reward: 3.0\n",
      "Step 519 (2115164) @ Episode 1703/2500, loss: 0.0041491077281534676\n",
      "Episode Reward: 6.0\n",
      "Step 429 (2115593) @ Episode 1704/2500, loss: 0.0113976262509822854\n",
      "Episode Reward: 5.0\n",
      "Step 369 (2115962) @ Episode 1705/2500, loss: 0.0074056424200534827\n",
      "Episode Reward: 4.0\n",
      "Step 416 (2116378) @ Episode 1706/2500, loss: 0.0088967401534318923\n",
      "Episode Reward: 5.0\n",
      "Step 400 (2116778) @ Episode 1707/2500, loss: 0.0056168586015701295\n",
      "Episode Reward: 4.0\n",
      "Step 381 (2117159) @ Episode 1708/2500, loss: 0.0130192711949348458\n",
      "Episode Reward: 4.0\n",
      "Step 660 (2117819) @ Episode 1709/2500, loss: 0.0049012685194611556\n",
      "Episode Reward: 8.0\n",
      "Step 321 (2118140) @ Episode 1710/2500, loss: 0.0039945989847183236\n",
      "Episode Reward: 1.0\n",
      "Step 492 (2118632) @ Episode 1711/2500, loss: 0.0100264307111501773\n",
      "Episode Reward: 6.0\n",
      "Step 604 (2119236) @ Episode 1712/2500, loss: 0.0077399695292115213\n",
      "Episode Reward: 5.0\n",
      "Step 229 (2119465) @ Episode 1713/2500, loss: 0.0851260945200920155\n",
      "Episode Reward: 1.0\n",
      "Step 446 (2119911) @ Episode 1714/2500, loss: 0.0098366970196366314\n",
      "Episode Reward: 4.0\n",
      "Step 351 (2120262) @ Episode 1715/2500, loss: 0.0048646284267306337\n",
      "Episode Reward: 2.0\n",
      "Step 496 (2120758) @ Episode 1716/2500, loss: 0.0149009283632040026\n",
      "Episode Reward: 5.0\n",
      "Step 388 (2121146) @ Episode 1717/2500, loss: 0.0047128992155194286\n",
      "Episode Reward: 4.0\n",
      "Step 345 (2121491) @ Episode 1718/2500, loss: 0.0123982122167944973\n",
      "Episode Reward: 2.0\n",
      "Step 305 (2121796) @ Episode 1719/2500, loss: 0.0085088126361370095\n",
      "Episode Reward: 2.0\n",
      "Step 379 (2122175) @ Episode 1720/2500, loss: 0.0115496572107076648\n",
      "Episode Reward: 3.0\n",
      "Step 315 (2122490) @ Episode 1721/2500, loss: 0.0075786551460623745\n",
      "Episode Reward: 2.0\n",
      "Step 357 (2122847) @ Episode 1722/2500, loss: 0.0121943801641464234\n",
      "Episode Reward: 3.0\n",
      "Step 473 (2123320) @ Episode 1723/2500, loss: 0.0177403073757886965\n",
      "Episode Reward: 6.0\n",
      "Step 691 (2124011) @ Episode 1724/2500, loss: 0.0275771263986825943\n",
      "Episode Reward: 12.0\n",
      "Step 565 (2124576) @ Episode 1725/2500, loss: 0.0044242907315492635\n",
      "Episode Reward: 7.0\n",
      "Step 435 (2125011) @ Episode 1726/2500, loss: 0.0045502772554755216\n",
      "Episode Reward: 3.0\n",
      "Step 368 (2125379) @ Episode 1727/2500, loss: 0.0063857818022370345\n",
      "Episode Reward: 3.0\n",
      "Step 268 (2125647) @ Episode 1728/2500, loss: 0.0170874539762735375\n",
      "Episode Reward: 2.0\n",
      "Step 631 (2126278) @ Episode 1729/2500, loss: 0.0207730792462825787\n",
      "Episode Reward: 7.0\n",
      "Step 288 (2126566) @ Episode 1730/2500, loss: 0.0131910247728228574\n",
      "Episode Reward: 3.0\n",
      "Step 321 (2126887) @ Episode 1731/2500, loss: 0.0102169504389166835\n",
      "Episode Reward: 3.0\n",
      "Step 544 (2127431) @ Episode 1732/2500, loss: 0.0046614389866590535\n",
      "Episode Reward: 6.0\n",
      "Step 436 (2127867) @ Episode 1733/2500, loss: 0.0305055994540452966\n",
      "Episode Reward: 3.0\n",
      "Step 393 (2128260) @ Episode 1734/2500, loss: 0.0076573654077947144\n",
      "Episode Reward: 4.0\n",
      "Step 432 (2128692) @ Episode 1735/2500, loss: 0.1989490985870361396\n",
      "Episode Reward: 5.0\n",
      "Step 347 (2129039) @ Episode 1736/2500, loss: 0.0089522721245884977\n",
      "Episode Reward: 1.0\n",
      "Step 377 (2129416) @ Episode 1737/2500, loss: 0.0282470975071191845\n",
      "Episode Reward: 4.0\n",
      "Step 204 (2129620) @ Episode 1738/2500, loss: 0.0073245400562882425\n",
      "Episode Reward: 0.0\n",
      "Step 365 (2129985) @ Episode 1739/2500, loss: 0.0064672892913222315\n",
      "Episode Reward: 4.0\n",
      "Step 405 (2130390) @ Episode 1740/2500, loss: 0.0105635644868016244\n",
      "Episode Reward: 3.0\n",
      "Step 398 (2130788) @ Episode 1741/2500, loss: 0.0043095638975501065\n",
      "Episode Reward: 3.0\n",
      "Step 373 (2131161) @ Episode 1742/2500, loss: 0.0077680465765297415\n",
      "Episode Reward: 2.0\n",
      "Step 433 (2131594) @ Episode 1743/2500, loss: 0.0073771188035607346\n",
      "Episode Reward: 2.0\n",
      "Step 455 (2132049) @ Episode 1744/2500, loss: 0.0220559611916542055\n",
      "Episode Reward: 4.0\n",
      "Step 373 (2132422) @ Episode 1745/2500, loss: 0.0044348761439323425\n",
      "Episode Reward: 2.0\n",
      "Step 478 (2132900) @ Episode 1746/2500, loss: 0.0074059497565031055\n",
      "Episode Reward: 4.0\n",
      "Step 589 (2133489) @ Episode 1747/2500, loss: 0.0058619701303541665\n",
      "Episode Reward: 6.0\n",
      "Step 490 (2133979) @ Episode 1748/2500, loss: 0.0068852612748742185\n",
      "Episode Reward: 6.0\n",
      "Step 413 (2134392) @ Episode 1749/2500, loss: 0.0071491948328912264\n",
      "Episode Reward: 4.0\n",
      "Step 357 (2134749) @ Episode 1750/2500, loss: 0.0092287231236696248\n",
      "Episode Reward: 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 13:00:09,815] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video001750.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 351 (2135100) @ Episode 1751/2500, loss: 0.0061526228673756125\n",
      "Episode Reward: 3.0\n",
      "Step 441 (2135541) @ Episode 1752/2500, loss: 0.0156829189509153378\n",
      "Episode Reward: 5.0\n",
      "Step 542 (2136083) @ Episode 1753/2500, loss: 0.0084838056936860085\n",
      "Episode Reward: 4.0\n",
      "Step 352 (2136435) @ Episode 1754/2500, loss: 0.0062515400350093845\n",
      "Episode Reward: 3.0\n",
      "Step 343 (2136778) @ Episode 1755/2500, loss: 0.0086747938767075547\n",
      "Episode Reward: 3.0\n",
      "Step 415 (2137193) @ Episode 1756/2500, loss: 0.0058142622001469135\n",
      "Episode Reward: 4.0\n",
      "Step 440 (2137633) @ Episode 1757/2500, loss: 0.0127521958202123647\n",
      "Episode Reward: 5.0\n",
      "Step 375 (2138008) @ Episode 1758/2500, loss: 0.0073568006046116355\n",
      "Episode Reward: 4.0\n",
      "Step 452 (2138460) @ Episode 1759/2500, loss: 0.0157028883695602475\n",
      "Episode Reward: 2.0\n",
      "Step 342 (2138802) @ Episode 1760/2500, loss: 0.0078231152147054675\n",
      "Episode Reward: 3.0\n",
      "Step 280 (2139082) @ Episode 1761/2500, loss: 0.0062458752654492855\n",
      "Episode Reward: 1.0\n",
      "Step 432 (2139514) @ Episode 1762/2500, loss: 0.0748843103647232785\n",
      "Episode Reward: 4.0\n",
      "Step 525 (2140039) @ Episode 1763/2500, loss: 0.0092010768130421643\n",
      "Episode Reward: 6.0\n",
      "Step 422 (2140461) @ Episode 1764/2500, loss: 0.0141273858025670055\n",
      "Episode Reward: 5.0\n",
      "Step 464 (2140925) @ Episode 1765/2500, loss: 0.0415178760886192335\n",
      "Episode Reward: 5.0\n",
      "Step 580 (2141505) @ Episode 1766/2500, loss: 0.0082723498344421395\n",
      "Episode Reward: 7.0\n",
      "Step 573 (2142078) @ Episode 1767/2500, loss: 0.0052770418114960195\n",
      "Episode Reward: 6.0\n",
      "Step 393 (2142471) @ Episode 1768/2500, loss: 0.0372618511319160465\n",
      "Episode Reward: 3.0\n",
      "Step 367 (2142838) @ Episode 1769/2500, loss: 0.0096124215051531836\n",
      "Episode Reward: 4.0\n",
      "Step 497 (2143335) @ Episode 1770/2500, loss: 0.0373817458748817445\n",
      "Episode Reward: 6.0\n",
      "Step 622 (2143957) @ Episode 1771/2500, loss: 0.0079985531046986583\n",
      "Episode Reward: 6.0\n",
      "Step 400 (2144357) @ Episode 1772/2500, loss: 0.0151502341032028234\n",
      "Episode Reward: 4.0\n",
      "Step 590 (2144947) @ Episode 1773/2500, loss: 0.0224621128290891655\n",
      "Episode Reward: 8.0\n",
      "Step 347 (2145294) @ Episode 1774/2500, loss: 0.0182268172502517785\n",
      "Episode Reward: 3.0\n",
      "Step 323 (2145617) @ Episode 1775/2500, loss: 0.0160632953047752385\n",
      "Episode Reward: 2.0\n",
      "Step 473 (2146090) @ Episode 1776/2500, loss: 0.0084890499711036684\n",
      "Episode Reward: 6.0\n",
      "Step 583 (2146673) @ Episode 1777/2500, loss: 0.0115951132029294976\n",
      "Episode Reward: 7.0\n",
      "Step 365 (2147038) @ Episode 1778/2500, loss: 0.0075177932158112536\n",
      "Episode Reward: 3.0\n",
      "Step 548 (2147586) @ Episode 1779/2500, loss: 0.0152047704905271535\n",
      "Episode Reward: 8.0\n",
      "Step 660 (2148246) @ Episode 1780/2500, loss: 0.0158079490065574655\n",
      "Episode Reward: 6.0\n",
      "Step 564 (2148810) @ Episode 1781/2500, loss: 0.0060451505705714234\n",
      "Episode Reward: 5.0\n",
      "Step 254 (2149064) @ Episode 1782/2500, loss: 0.0067967399954795845\n",
      "Episode Reward: 2.0\n",
      "Step 385 (2149449) @ Episode 1783/2500, loss: 0.0126522071659564975\n",
      "Episode Reward: 2.0\n",
      "Step 308 (2149757) @ Episode 1784/2500, loss: 0.0377107858657836963\n",
      "Episode Reward: 2.0\n",
      "Step 391 (2150148) @ Episode 1785/2500, loss: 0.0124730467796325683\n",
      "Episode Reward: 3.0\n",
      "Step 454 (2150602) @ Episode 1786/2500, loss: 0.1765883117914199865\n",
      "Episode Reward: 5.0\n",
      "Step 587 (2151189) @ Episode 1787/2500, loss: 0.0072062676772475245\n",
      "Episode Reward: 7.0\n",
      "Step 504 (2151693) @ Episode 1788/2500, loss: 0.0973261296749115084\n",
      "Episode Reward: 5.0\n",
      "Step 353 (2152046) @ Episode 1789/2500, loss: 0.0122629348188638695\n",
      "Episode Reward: 2.0\n",
      "Step 411 (2152457) @ Episode 1790/2500, loss: 0.0059944242238998413\n",
      "Episode Reward: 3.0\n",
      "Step 527 (2152984) @ Episode 1791/2500, loss: 0.0502078607678413425\n",
      "Episode Reward: 5.0\n",
      "Step 394 (2153378) @ Episode 1792/2500, loss: 0.0322427228093147392\n",
      "Episode Reward: 3.0\n",
      "Step 462 (2153840) @ Episode 1793/2500, loss: 0.0513935647904872933\n",
      "Episode Reward: 2.0\n",
      "Step 668 (2154508) @ Episode 1794/2500, loss: 0.0165606141090393075\n",
      "Episode Reward: 5.0\n",
      "Step 509 (2155017) @ Episode 1795/2500, loss: 0.0150527236983180054\n",
      "Episode Reward: 4.0\n",
      "Step 603 (2155620) @ Episode 1796/2500, loss: 0.0365641862154006965\n",
      "Episode Reward: 6.0\n",
      "Step 446 (2156066) @ Episode 1797/2500, loss: 0.0164177324622869536\n",
      "Episode Reward: 6.0\n",
      "Step 459 (2156525) @ Episode 1798/2500, loss: 0.0039307544939219955\n",
      "Episode Reward: 5.0\n",
      "Step 823 (2157348) @ Episode 1799/2500, loss: 0.0463948696851730356\n",
      "Episode Reward: 9.0\n",
      "Step 735 (2158083) @ Episode 1800/2500, loss: 0.0056460723280906685\n",
      "Episode Reward: 9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 13:44:04,349] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video001800.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 338 (2158421) @ Episode 1801/2500, loss: 0.0216167960315942765\n",
      "Episode Reward: 2.0\n",
      "Step 387 (2158808) @ Episode 1802/2500, loss: 0.0061243614181876185\n",
      "Episode Reward: 4.0\n",
      "Step 424 (2159232) @ Episode 1803/2500, loss: 0.0087853930890560154\n",
      "Episode Reward: 2.0\n",
      "Step 395 (2159627) @ Episode 1804/2500, loss: 0.0251735113561153427\n",
      "Episode Reward: 3.0\n",
      "Step 265 (2159892) @ Episode 1805/2500, loss: 0.0118779214099049575\n",
      "Episode Reward: 1.0\n",
      "Step 393 (2160285) @ Episode 1806/2500, loss: 0.0067676100879907616\n",
      "Episode Reward: 3.0\n",
      "Step 384 (2160669) @ Episode 1807/2500, loss: 0.0063040843233466155\n",
      "Episode Reward: 4.0\n",
      "Step 371 (2161040) @ Episode 1808/2500, loss: 0.0147111872211098675\n",
      "Episode Reward: 2.0\n",
      "Step 489 (2161529) @ Episode 1809/2500, loss: 0.0146638043224811555\n",
      "Episode Reward: 5.0\n",
      "Step 576 (2162105) @ Episode 1810/2500, loss: 0.0030647078529000282\n",
      "Episode Reward: 5.0\n",
      "Step 878 (2162983) @ Episode 1811/2500, loss: 0.0312513411045074465\n",
      "Episode Reward: 10.0\n",
      "Step 539 (2163522) @ Episode 1812/2500, loss: 0.0122264483943581584\n",
      "Episode Reward: 7.0\n",
      "Step 581 (2164103) @ Episode 1813/2500, loss: 0.0212822295725345625\n",
      "Episode Reward: 4.0\n",
      "Step 317 (2164420) @ Episode 1814/2500, loss: 0.2523630261421203647\n",
      "Episode Reward: 1.0\n",
      "Step 408 (2164828) @ Episode 1815/2500, loss: 0.0070756045170128345\n",
      "Episode Reward: 3.0\n",
      "Step 330 (2165158) @ Episode 1816/2500, loss: 0.0078162029385566715\n",
      "Episode Reward: 4.0\n",
      "Step 556 (2165714) @ Episode 1817/2500, loss: 0.0092573286965489395\n",
      "Episode Reward: 7.0\n",
      "Step 631 (2166345) @ Episode 1818/2500, loss: 0.0124696390703320594\n",
      "Episode Reward: 5.0\n",
      "Step 394 (2166739) @ Episode 1819/2500, loss: 0.0094348294660449035\n",
      "Episode Reward: 3.0\n",
      "Step 434 (2167173) @ Episode 1820/2500, loss: 0.0137789752334356325\n",
      "Episode Reward: 3.0\n",
      "Step 571 (2167744) @ Episode 1821/2500, loss: 0.0055179735645651823\n",
      "Episode Reward: 7.0\n",
      "Step 484 (2168228) @ Episode 1822/2500, loss: 0.0103257801383733755\n",
      "Episode Reward: 3.0\n",
      "Step 602 (2168830) @ Episode 1823/2500, loss: 0.0052502420730888845\n",
      "Episode Reward: 8.0\n",
      "Step 587 (2169417) @ Episode 1824/2500, loss: 0.0111123975366353996\n",
      "Episode Reward: 6.0\n",
      "Step 272 (2169689) @ Episode 1825/2500, loss: 0.0086255734786391265\n",
      "Episode Reward: 2.0\n",
      "Step 383 (2170072) @ Episode 1826/2500, loss: 0.0109249390661716465\n",
      "Episode Reward: 4.0\n",
      "Step 543 (2170615) @ Episode 1827/2500, loss: 0.0937460735440254235\n",
      "Episode Reward: 7.0\n",
      "Step 443 (2171058) @ Episode 1828/2500, loss: 0.0062825707718729975\n",
      "Episode Reward: 4.0\n",
      "Step 430 (2171488) @ Episode 1829/2500, loss: 0.0072368378750979986\n",
      "Episode Reward: 3.0\n",
      "Step 301 (2171789) @ Episode 1830/2500, loss: 0.0144650992006063465\n",
      "Episode Reward: 1.0\n",
      "Step 381 (2172170) @ Episode 1831/2500, loss: 0.0054342015646398075\n",
      "Episode Reward: 3.0\n",
      "Step 723 (2172893) @ Episode 1832/2500, loss: 0.0087563851848244677\n",
      "Episode Reward: 5.0\n",
      "Step 339 (2173232) @ Episode 1833/2500, loss: 0.0453539378941059135\n",
      "Episode Reward: 3.0\n",
      "Step 578 (2173810) @ Episode 1834/2500, loss: 0.0170332528650760656\n",
      "Episode Reward: 6.0\n",
      "Step 434 (2174244) @ Episode 1835/2500, loss: 0.0131510086357593545\n",
      "Episode Reward: 4.0\n",
      "Step 507 (2174751) @ Episode 1836/2500, loss: 0.0086138285696506545\n",
      "Episode Reward: 3.0\n",
      "Step 459 (2175210) @ Episode 1837/2500, loss: 0.0117688402533531198\n",
      "Episode Reward: 4.0\n",
      "Step 384 (2175594) @ Episode 1838/2500, loss: 0.1169544458389282282\n",
      "Episode Reward: 3.0\n",
      "Step 351 (2175945) @ Episode 1839/2500, loss: 0.0079671042039990435\n",
      "Episode Reward: 4.0\n",
      "Step 400 (2176345) @ Episode 1840/2500, loss: 0.0119261238723993396\n",
      "Episode Reward: 2.0\n",
      "Step 606 (2176951) @ Episode 1841/2500, loss: 0.0087278373539447783\n",
      "Episode Reward: 8.0\n",
      "Step 392 (2177343) @ Episode 1842/2500, loss: 0.0071463277563452725\n",
      "Episode Reward: 2.0\n",
      "Step 445 (2177788) @ Episode 1843/2500, loss: 0.0047267274931073196\n",
      "Episode Reward: 5.0\n",
      "Step 634 (2178422) @ Episode 1844/2500, loss: 0.0104929842054843965\n",
      "Episode Reward: 7.0\n",
      "Step 313 (2178735) @ Episode 1845/2500, loss: 0.0053604133427143146\n",
      "Episode Reward: 1.0\n",
      "Step 293 (2179028) @ Episode 1846/2500, loss: 0.0077904136851429946\n",
      "Episode Reward: 3.0\n",
      "Step 612 (2179640) @ Episode 1847/2500, loss: 0.0246551111340522775\n",
      "Episode Reward: 7.0\n",
      "Step 517 (2180157) @ Episode 1848/2500, loss: 0.0097076147794723513\n",
      "Episode Reward: 5.0\n",
      "Step 449 (2180606) @ Episode 1849/2500, loss: 0.0053118178620934495\n",
      "Episode Reward: 5.0\n",
      "Step 536 (2181142) @ Episode 1850/2500, loss: 0.0041213035583496095\n",
      "Episode Reward: 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 14:27:22,091] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video001850.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 478 (2181620) @ Episode 1851/2500, loss: 0.0091157369315624243\n",
      "Episode Reward: 4.0\n",
      "Step 295 (2181915) @ Episode 1852/2500, loss: 0.0064338976517319685\n",
      "Episode Reward: 2.0\n",
      "Step 424 (2182339) @ Episode 1853/2500, loss: 0.0091622881591320045\n",
      "Episode Reward: 4.0\n",
      "Step 393 (2182732) @ Episode 1854/2500, loss: 0.0212063305079936985\n",
      "Episode Reward: 4.0\n",
      "Step 249 (2182981) @ Episode 1855/2500, loss: 0.0113874450325965886\n",
      "Episode Reward: 1.0\n",
      "Step 482 (2183463) @ Episode 1856/2500, loss: 0.0127577465027570725\n",
      "Episode Reward: 6.0\n",
      "Step 645 (2184108) @ Episode 1857/2500, loss: 0.0124128283932805065\n",
      "Episode Reward: 7.0\n",
      "Step 394 (2184502) @ Episode 1858/2500, loss: 0.0049145962111651955\n",
      "Episode Reward: 4.0\n",
      "Step 375 (2184877) @ Episode 1859/2500, loss: 0.0062196892686188225\n",
      "Episode Reward: 3.0\n",
      "Step 536 (2185413) @ Episode 1860/2500, loss: 0.0068000410683453085\n",
      "Episode Reward: 4.0\n",
      "Step 385 (2185798) @ Episode 1861/2500, loss: 0.0086654378101229675\n",
      "Episode Reward: 4.0\n",
      "Step 479 (2186277) @ Episode 1862/2500, loss: 0.0062000830657780175\n",
      "Episode Reward: 3.0\n",
      "Step 723 (2187000) @ Episode 1863/2500, loss: 0.0113948397338390355\n",
      "Episode Reward: 7.0\n",
      "Step 628 (2187628) @ Episode 1864/2500, loss: 0.0090614445507526454\n",
      "Episode Reward: 6.0\n",
      "Step 446 (2188074) @ Episode 1865/2500, loss: 0.0036958376877009875\n",
      "Episode Reward: 4.0\n",
      "Step 433 (2188507) @ Episode 1866/2500, loss: 0.0610965937376022345\n",
      "Episode Reward: 4.0\n",
      "Step 612 (2189119) @ Episode 1867/2500, loss: 0.0086843278259038937\n",
      "Episode Reward: 6.0\n",
      "Step 282 (2189401) @ Episode 1868/2500, loss: 0.0080918790772557264\n",
      "Episode Reward: 1.0\n",
      "Step 363 (2189764) @ Episode 1869/2500, loss: 0.0218692328780889515\n",
      "Episode Reward: 1.0\n",
      "Step 554 (2190318) @ Episode 1870/2500, loss: 0.0210972800850868235\n",
      "Episode Reward: 6.0\n",
      "Step 406 (2190724) @ Episode 1871/2500, loss: 0.0095335524529218674\n",
      "Episode Reward: 4.0\n",
      "Step 392 (2191116) @ Episode 1872/2500, loss: 0.0550310350954532685\n",
      "Episode Reward: 3.0\n",
      "Step 540 (2191656) @ Episode 1873/2500, loss: 0.0060707191005349164\n",
      "Episode Reward: 5.0\n",
      "Step 266 (2191922) @ Episode 1874/2500, loss: 0.0048285773955285555\n",
      "Episode Reward: 1.0\n",
      "Step 696 (2192618) @ Episode 1875/2500, loss: 0.0130368219688534744\n",
      "Episode Reward: 9.0\n",
      "Step 569 (2193187) @ Episode 1876/2500, loss: 0.0125898420810699465\n",
      "Episode Reward: 6.0\n",
      "Step 303 (2193490) @ Episode 1877/2500, loss: 0.0066720172762870795\n",
      "Episode Reward: 2.0\n",
      "Step 235 (2193725) @ Episode 1878/2500, loss: 0.0130463996902108224\n",
      "Episode Reward: 1.0\n",
      "Step 493 (2194218) @ Episode 1879/2500, loss: 0.0094147110357880645\n",
      "Episode Reward: 5.0\n",
      "Step 351 (2194569) @ Episode 1880/2500, loss: 0.0091323330998420723\n",
      "Episode Reward: 3.0\n",
      "Step 470 (2195039) @ Episode 1881/2500, loss: 0.0212976932525634776\n",
      "Episode Reward: 5.0\n",
      "Step 291 (2195330) @ Episode 1882/2500, loss: 0.0062038209289312366\n",
      "Episode Reward: 1.0\n",
      "Step 411 (2195741) @ Episode 1883/2500, loss: 0.0067925229668617255\n",
      "Episode Reward: 4.0\n",
      "Step 685 (2196426) @ Episode 1884/2500, loss: 0.0068706311285495765\n",
      "Episode Reward: 8.0\n",
      "Step 440 (2196866) @ Episode 1885/2500, loss: 0.0064005246385931978\n",
      "Episode Reward: 4.0\n",
      "Step 405 (2197271) @ Episode 1886/2500, loss: 0.0084021426737308536\n",
      "Episode Reward: 1.0\n",
      "Step 324 (2197595) @ Episode 1887/2500, loss: 0.0067587834782898435\n",
      "Episode Reward: 3.0\n",
      "Step 445 (2198040) @ Episode 1888/2500, loss: 0.0231791157275438335\n",
      "Episode Reward: 8.0\n",
      "Step 474 (2198514) @ Episode 1889/2500, loss: 0.0102551737800240526\n",
      "Episode Reward: 5.0\n",
      "Step 542 (2199056) @ Episode 1890/2500, loss: 0.0083288094028830535\n",
      "Episode Reward: 4.0\n",
      "Step 404 (2199460) @ Episode 1891/2500, loss: 0.0071171326562762265\n",
      "Episode Reward: 4.0\n",
      "Step 625 (2200085) @ Episode 1892/2500, loss: 0.0039548901841044435\n",
      "Episode Reward: 8.0\n",
      "Step 381 (2200466) @ Episode 1893/2500, loss: 0.0489269867539405845\n",
      "Episode Reward: 4.0\n",
      "Step 410 (2200876) @ Episode 1894/2500, loss: 0.0102045265957713135\n",
      "Episode Reward: 5.0\n",
      "Step 575 (2201451) @ Episode 1895/2500, loss: 0.0177541524171829225\n",
      "Episode Reward: 5.0\n",
      "Step 441 (2201892) @ Episode 1896/2500, loss: 0.0064499727450311185\n",
      "Episode Reward: 5.0\n",
      "Step 381 (2202273) @ Episode 1897/2500, loss: 0.0125800808891654015\n",
      "Episode Reward: 3.0\n",
      "Step 716 (2202989) @ Episode 1898/2500, loss: 0.0086079249158501635\n",
      "Episode Reward: 8.0\n",
      "Step 640 (2203629) @ Episode 1899/2500, loss: 0.0058831418864429375\n",
      "Episode Reward: 6.0\n",
      "Step 342 (2203971) @ Episode 1900/2500, loss: 0.0839112997055053764\n",
      "Episode Reward: 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-13 15:10:59,317] Starting new video recorder writing to /playpen2/chaonan99/course/David_RL/code/reinforcement-learning/DQN/experiments/Breakout-v0/monitor/openaigym.video.0.10190.video001900.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 449 (2204420) @ Episode 1901/2500, loss: 0.0085255876183509837\n",
      "Episode Reward: 5.0\n",
      "Step 553 (2204973) @ Episode 1902/2500, loss: 0.0099837556481361395\n",
      "Episode Reward: 7.0\n",
      "Step 450 (2205423) @ Episode 1903/2500, loss: 0.0140505228191614155\n",
      "Episode Reward: 5.0\n",
      "Step 304 (2205727) @ Episode 1904/2500, loss: 0.0083643347024917655\n",
      "Episode Reward: 2.0\n",
      "Step 424 (2206151) @ Episode 1905/2500, loss: 0.1376144289970398576\n",
      "Episode Reward: 6.0\n",
      "Step 333 (2206484) @ Episode 1906/2500, loss: 0.0225930884480476385\n",
      "Episode Reward: 3.0\n",
      "Step 495 (2206979) @ Episode 1907/2500, loss: 0.0104683227837085725\n",
      "Episode Reward: 5.0\n",
      "Step 248 (2207227) @ Episode 1908/2500, loss: 0.0095704067498445516\n",
      "Episode Reward: 1.0\n",
      "Step 282 (2207509) @ Episode 1909/2500, loss: 0.0036712510045617825\n",
      "Episode Reward: 3.0\n",
      "Step 215 (2207724) @ Episode 1910/2500, loss: 0.0223136395215988165\n",
      "Episode Reward: 1.0\n",
      "Step 266 (2207990) @ Episode 1911/2500, loss: 0.0154102873057127755\n",
      "Episode Reward: 2.0\n",
      "Step 406 (2208396) @ Episode 1912/2500, loss: 0.0197377074509859155\n",
      "Episode Reward: 3.0\n",
      "Step 294 (2208690) @ Episode 1913/2500, loss: 0.0069078146480023865\n",
      "Episode Reward: 2.0\n",
      "Step 322 (2209012) @ Episode 1914/2500, loss: 0.0104325404390692717\n",
      "Episode Reward: 3.0\n",
      "Step 315 (2209327) @ Episode 1915/2500, loss: 0.0099500454962253576\n",
      "Episode Reward: 3.0\n",
      "Step 328 (2209655) @ Episode 1916/2500, loss: 0.0067546982318162927\n",
      "Episode Reward: 3.0\n",
      "Step 395 (2210050) @ Episode 1917/2500, loss: 0.0059122825041413317\n",
      "Episode Reward: 5.0\n",
      "Step 302 (2210352) @ Episode 1918/2500, loss: 0.0052775936201214796\n",
      "Episode Reward: 2.0\n",
      "Step 456 (2210808) @ Episode 1919/2500, loss: 0.0170107111334800724\n",
      "Episode Reward: 5.0\n",
      "Step 292 (2211100) @ Episode 1920/2500, loss: 0.0088189514353871358\n",
      "Episode Reward: 2.0\n",
      "Step 454 (2211554) @ Episode 1921/2500, loss: 0.0070658973418176176\n",
      "Episode Reward: 2.0\n",
      "Step 72 (2211626) @ Episode 1922/2500, loss: 0.0061575397849082955"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-32af60fb7a41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                                     \u001b[0mepsilon_decay_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                     \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                     batch_size=32):\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;31m#     for t, stats in deep_q_learning(sess,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#                                     env,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-e0ad520558a5>\u001b[0m in \u001b[0;36mdeep_q_learning\u001b[0;34m(sess, env, q_estimator, target_estimator, state_processor, num_episodes, experiment_dir, replay_memory_size, replay_memory_init_size, update_target_estimator_every, discount_factor, epsilon_start, epsilon_end, epsilon_decay_steps, batch_size, record_video_every)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;31m# Take a step in the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVALID_ACTIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-80e7ace0c0fb>\u001b[0m in \u001b[0;36mpolicy_fn\u001b[0;34m(sess, observation, epsilon)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpolicy_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d6e4720ffac5>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sess, s)\u001b[0m\n\u001b[1;32m     80\u001b[0m           \u001b[0maction\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \"\"\"\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_pl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1072\u001b[0m                             'strings, lists, numpy ndarrays, or TensorHandles.')\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m           \u001b[0msubfeed_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m           if isinstance(subfeed_val,\n\u001b[1;32m   1076\u001b[0m                         int) and subfeed_dtype(subfeed_val) != subfeed_val:\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36mas_numpy_dtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type_enum\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumpy_incompatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mas_numpy_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;34m\"\"\"Returns a `numpy.dtype` based on this `DType`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=2500,\n",
    "                                    replay_memory_size=500000,\n",
    "                                    replay_memory_init_size=50000,\n",
    "                                    update_target_estimator_every=10000,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=500000,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=32):\n",
    "#     for t, stats in deep_q_learning(sess,\n",
    "#                                     env,\n",
    "#                                     q_estimator=q_estimator,\n",
    "#                                     target_estimator=target_estimator,\n",
    "#                                     state_processor=state_processor,\n",
    "#                                     experiment_dir=experiment_dir,\n",
    "#                                     num_episodes=0,\n",
    "#                                     replay_memory_size=500,\n",
    "#                                     replay_memory_init_size=50,\n",
    "#                                     update_target_estimator_every=10,\n",
    "#                                     epsilon_start=1.0,\n",
    "#                                     epsilon_end=0.1,\n",
    "#                                     epsilon_decay_steps=5,\n",
    "#                                     discount_factor=0.99,\n",
    "#                                     batch_size=32):\n",
    "\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
